{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calvin's Scalable Paraellel Downsampler (CSPD)\n",
    "\n",
    "Written by Calvin W.Y. Chan <calvin.chan@bayer.com>, June 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from siuba import _, select, rename, left_join\n",
    "from pytorch_forecasting.metrics import MAPE\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import HyperBandScheduler\n",
    "from ray.tune.suggest.basic_variant import BasicVariantGenerator\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "from ray.tune.suggest.bohb import TuneBOHB\n",
    "\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "import warnings\n",
    "import filelock\n",
    "\n",
    "import string\n",
    "import time\n",
    "import random\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sys.version_info(major=3, minor=9, micro=5, releaselevel='final', serial=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf /tmp/cpu.lock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environmental Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Data Parameters\n",
    "subgroup_col = \"subgroup\"   # Use \"None\" if no subgrouping is given\n",
    "y_id_col = \"cluster_id\"\n",
    "\n",
    "# Fake Data Creation Parameters\n",
    "N_SAMPLE = 500000\n",
    "N_FEATURE = 4\n",
    "N_CLUSTER = 5000\n",
    "N_SUBGROUP = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Handling Parameters\n",
    "test_split_ratio = 0.2\n",
    "k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "num_epochs = 5\n",
    "\n",
    "# Learning Algorithm Parameters\n",
    "lr_min = 1e-4\n",
    "lr_max = 1e-1\n",
    "# batch_size = [1,16,32,64]\n",
    "batch_size = [64]\n",
    "\n",
    "# Architecture Sampling Parameters\n",
    "h_total_min = 8*N_SUBGROUP\n",
    "h_total_max = 11*N_SUBGROUP\n",
    "h_total_step = N_SUBGROUP\n",
    "\n",
    "h_subgroup_min_neuron_per_sgrp = 7\n",
    "h_subgroup_max_neuron_per_sgrp = 12\n",
    "\n",
    "h_branch_min_neuron_per_layer = 2\n",
    "h_branch_max_neuron_per_layer = 5\n",
    "h_branch_max_layer = None\n",
    "\n",
    "dropout_p_min = 0\n",
    "dropout_p_max = 0.7\n",
    "\n",
    "# Only use for Architecture Table Search\n",
    "h_subgroup_n_samples = 6\n",
    "h_branch_n_samples = 2\n",
    "\n",
    "# Ray Tune Hyperparameter Search\n",
    "num_hp_search_samples = 1\n",
    "chkpt_dir = \"/home/calvin_chan/data/output/checkpoint/testing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup output directories\n",
    "if not os.path.exists(chkpt_dir):\n",
    "    os.makedirs(chkpt_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fake Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_colname = [ 'feature_' + x for x in string.ascii_lowercase[:N_FEATURE] ]\n",
    "\n",
    "clusterid_start = 1 + N_SAMPLE \n",
    "clusterid_end = N_SAMPLE + N_CLUSTER\n",
    "hcp_features = pd.DataFrame(np.random.randn(N_SAMPLE,N_FEATURE),columns=features_colname)\n",
    "hcp_subgroup = pd.DataFrame(np.random.randint(1,N_SUBGROUP+1,size=N_SAMPLE),columns=['subgroup'])\n",
    "hcp_clusterid = pd.DataFrame(np.random.randint(clusterid_start,clusterid_end+1,size=N_SAMPLE+1),columns=['cluster_id'],dtype=int)\n",
    "cluster_y_out = pd.DataFrame(np.random.randint(1000,80000,size=N_CLUSTER+1),columns=['y_out'])\n",
    "cluster_clusterid = pd.DataFrame(np.linspace(clusterid_start,clusterid_end,N_CLUSTER).reshape(N_CLUSTER,1),columns=['cluster_id'],dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_colname = [ 'feature_' + x for x in string.ascii_lowercase[:N_FEATURE] ]\n",
    "\n",
    "hcp_features = pd.DataFrame(np.random.randn(N_SAMPLE,N_FEATURE),columns=features_colname)\n",
    "hcp_subgroup = pd.DataFrame(np.random.randint(1,N_SUBGROUP+1,size=N_SAMPLE),columns=['subgroup'])\n",
    "hcp_clusterid = pd.DataFrame(np.random.randint(1,N_CLUSTER+1,size=N_SAMPLE),columns=['cluster_id'],dtype=int)\n",
    "cluster_y_out = pd.DataFrame(np.random.randint(1000,80000,size=N_CLUSTER),columns=['y_out'])\n",
    "cluster_clusterid = pd.DataFrame(np.linspace(1,N_CLUSTER,N_CLUSTER).reshape(N_CLUSTER,1),columns=['cluster_id'],dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = pd.get_dummies(hcp_subgroup.subgroup, prefix='Subgroup')\n",
    "x = pd.concat([hcp_clusterid.astype(str),\n",
    "               hcp_subgroup,\n",
    "               hcp_features], axis=1)\n",
    "y = pd.concat([cluster_clusterid.astype(str),\n",
    "               cluster_y_out], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calvin's Scalable Paraellel Downsampler (CSPD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"./graphics/DataFlow.jpg\" width=\"1340\" alt=\"subgroup_branch\"  />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalable Layer Block\n",
    "\n",
    "This allow different number of layer blocks to be used in the architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"./graphics/ParameterSharedLayerBlock.jpg\" width=\"947\" alt=\"parameter_shared_layer\"  />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAMPLE_Branch(nn.Module):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, in_feat, layers, dropout_p=None, act_fn=torch.relu):\n",
    "        super(SAMPLE_Branch, self).__init__()\n",
    "        layers = [in_feat] + layers   # Add input layer\n",
    "        self.hidden = nn.ModuleList()\n",
    "        self.out = nn.Linear(layers[-1],1).double()\n",
    "        self.act_fn = act_fn\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "        # --- Scalable Layers ---\n",
    "        for input_size, output_size in zip(layers, layers[1:]):\n",
    "            self.hidden.append(nn.Linear(input_size,output_size).double())\n",
    "            \n",
    "    # Prediction\n",
    "    def forward(self, x):\n",
    "\n",
    "        L = len(self.hidden)\n",
    "        for (l, single_layer) in zip(range(L), self.hidden):\n",
    "            x = single_layer(x)\n",
    "            x = torch.unbind(x,dim=1)                                        # Separate activation sum\n",
    "            x = [ self.dropout(self.act_fn(x_sample)) for x_sample in x ]    # Compute sum activation for each X_hcp separately\n",
    "            x = torch.stack(x, dim=1)                                        # Stack hcp_samples back together for next layer\n",
    "        x = torch.unbind(x,dim=1)\n",
    "        x = [ self.act_fn(x_sample.sum(dim=1)) for x_sample in x ]   # Compute sum activation for each X_hcp separately\n",
    "        x = torch.stack(x,dim=1)                                     # Stack back together for output\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subgroup Parallel Branch Block\n",
    "\n",
    "This allow the achitecture to scale with respect to the total number of subgroups in the input data, as well, to encapsulate each subgroup into a submodel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"./graphics/SubgroupBranch.jpg\" width=\"1340\" alt=\"subgroup_branch\"  />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cspd(nn.Module):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, in_feat, Subgroups, Architecture, dropout_p=None, act_fn=torch.relu):\n",
    "        super(cspd, self).__init__()\n",
    "        self.hidden = nn.ModuleList()\n",
    "        # --- Scalable Subgroup Branch ---\n",
    "        for s_id in range(Subgroups):\n",
    "            self.hidden.append(SAMPLE_Branch(in_feat, Architecture[s_id], dropout_p, act_fn))\n",
    "            \n",
    "    # Prediction\n",
    "    def forward(self, x, s):\n",
    "        z = [ z_s(x) for z_s in self.hidden ]\n",
    "        z = torch.stack(z, dim=2)\n",
    "        branch_avg_factor = s.sum(dim=[2], keepdim=True)             # Cases with unsure subgrouping, normalize them across all subgroups\n",
    "        branch_avg_factor = torch.max(branch_avg_factor,             # Avoid divide by zero\n",
    "                                      torch.ones(branch_avg_factor.shape, \n",
    "                                                 device=branch_avg_factor.device.type))   \n",
    "        z = (z * s).sum(dim=[2], keepdim=True) / branch_avg_factor   # Sum across subgroup branches\n",
    "        z = z.sum(dim=[1], keepdim=True)                             # Sum across samples\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"./graphics/DataArchitecture.jpg\" width=\"1153\" alt=\"architecture\"  />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight.data)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Handling and Hyperparameter Tunning\n",
    "\n",
    "### Overview\n",
    "\n",
    "* Modeling = Train + Validation 80%, Test 20% Data Split\n",
    "* Modeling = Train + Validation 80%, Using K-Fold for Hyperparameters Tunning\n",
    "\n",
    "<ol>\n",
    "    <li>Split the data into 80/20</li>\n",
    "    <li>Use K-Fold cross-validation splitting given k (eg. k=5 would results 80%/5=16% of each fold)</li>\n",
    "    <ol>\n",
    "        <li>For each fold, use the K-Fold training set for building model for each hyperparameters set</li>\n",
    "        <li>For each fold, evaluate all models with different hyperparameters with the K-Fold test set</li>\n",
    "        <li>Summarize the results into a table of (hyperparameter index, K-Fold index)</li>\n",
    "        <li>Find the best hyperparameter set\n",
    "    </ol>\n",
    "    <li>Use the entire \"Modeling = Train + Validation 80%\" dataset to train a model</li>\n",
    "    <li>Evaluate the model using the 20% Test Data\n",
    "</ol>\n",
    "\n",
    "<center>\n",
    "    <img src=\"./graphics/DataHandling.jpg\" width=\"1074\" alt=\"data_splitting\"  />\n",
    "</center>\n",
    "\n",
    "### Implementation\n",
    "\n",
    "Data Splitting\n",
    "* Using `sklearn.model_selection.train_test_split` function to perform the 80/20 split\n",
    "* Generate index for K-fold of the 80% train+validation dataset using `sklearn.model_selection.KFold`\n",
    "* Create a list of pytorch dataloader for each of the K-fold for training\n",
    "* Create a list of pytorch dataloader for each of the K-fold for validation\n",
    "\n",
    "Hyperparameter Tunning\n",
    "* Use K in K-Fold as grid (must run) hyperparameter\n",
    "* Use __custom sampling function__ to describe the hierachical neuron distribution between:\n",
    " * total neuron: $H_{total}$\n",
    " * neuron per subgroup: $H_{subgroup}$\n",
    " * neuron per layer: $H_{branch}$\n",
    "\n",
    "<p style=\"margin-left: 100px\">$H_{total}=15\\quad\\longrightarrow\\quad H_{subgroup}=\\begin{bmatrix}3\\\\4\\\\5\\\\3\\end{bmatrix}\\quad \\longrightarrow\\quad H_{branch}=\\begin{bmatrix}[2,1]\\\\ [2,2]\\\\ [2,2,1]\\\\ [1,2] \\end{bmatrix}$ </p>\n",
    "    \n",
    "* __Custom Sampling Function__\n",
    " * Using the total number of neuron from the last level, create all possible combination given the number of element\n",
    " * Sample an element from the list of combination and returns it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling-Testing 80/20 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_multidimensional_labels(df,col):\n",
    "    '''\n",
    "    Convert Multiple Column Label into Single Column\n",
    "    \n",
    "    Args:\n",
    "        df: A pandas dataframe with row as samples, and column as N-dimensional subgroup to be encoded.\n",
    "        col: Column name of the combined column\n",
    "        \n",
    "    Returns:\n",
    "        df: A pandas dataframe with new label column\n",
    "\n",
    "    Raises:\n",
    "        -\n",
    "\n",
    "    Author:\n",
    "        Dr. Calvin Chan\n",
    "        calvin.chan@bayer.com\n",
    "    '''\n",
    "    if df.shape[1] == 1:\n",
    "        df = pd.concat([df,df],axis=1)\n",
    "        df.columns = [df.columns[0],col]\n",
    "    else:\n",
    "        df[col] = tuple(labels.values.tolist())\n",
    "        df[col] = labels[col].apply(lambda x: ','.join([str(c) for c in x ]))\n",
    "    return(df)\n",
    "\n",
    "def combine_multidimensional_ohe(s):\n",
    "    '''\n",
    "    One-Hot-Encoding (OHE) based on joint label of multiple columns\n",
    "    The default OHE feature of Pandas and sklearn takes each column as independent OHE. \n",
    "    This function uses the 2D unique label combination as a single dimension for OHE.\n",
    "    \n",
    "    Args:\n",
    "        s: A pandas dataframe with row as samples, and column as N-dimensional subgroup to be encoded.\n",
    "\n",
    "    Returns:\n",
    "        s_ohe: A pandas dataframe with N-D OHE\n",
    "        conversion_table: The conversion table for N-D OHE\n",
    "\n",
    "    Raises:\n",
    "        -\n",
    "\n",
    "    Author:\n",
    "        Dr. Calvin Chan\n",
    "        calvin.chan@bayer.com\n",
    "    '''\n",
    "    unique_labels = [ sorted(s[name].unique().tolist()) for name in s.columns.tolist() ]\n",
    "    multidimensional_labels = [*itertools.product(*unique_labels)]\n",
    "    labels = pd.DataFrame(multidimensional_labels, columns=s.columns.tolist())\n",
    "    labels = convert_multidimensional_labels(labels,'sgrp')\n",
    "    conversion_table = pd.get_dummies(labels, columns=['sgrp'])\n",
    "    s_ohe = pd.merge(s,conversion_table,on=s.columns.tolist(),how='left').drop(s.columns.tolist(),axis=1)\n",
    "    return(s_ohe,conversion_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test_split(*args, id_col=None, test_ratio=0.2, random_state=25, report_id=False):\n",
    "    \n",
    "    '''\n",
    "    Split the dataset into modeling and test set\n",
    "    \n",
    "    This function is to encapsulate the variying input feature size given the grouping by id_col,\n",
    "    and this decompose the one-hot-encoding column into a separate feature set to be used in the\n",
    "    deep learning model as separate input.\n",
    "    \n",
    "    Args:\n",
    "        *args:\n",
    "            x: A pandas dataframe with row as samples, and column as ID and feature type\n",
    "            y: A pandas dataframe with row as samples, and column as output\n",
    "        ohe_col: A list of column names indicating the one-hot-encoding columns in x\n",
    "        id_col: Column name of the grouping column to be converted to one-hot-encoding\n",
    "        test_size: The split ratio of the test set\n",
    "        random_state: Random seed use by the `sklearn.model_selection.train_test_split` function\n",
    "\n",
    "    Returns:\n",
    "        x_model, x_test: List of numpy matrix as model/test data split with from commond id_col labels of x and y\n",
    "        s_model, s_test: List of numpy matrix as model/test data split with from commond id_col labels of x and y\n",
    "        y_model, y_test: List of numpy matrix as model/test data split with from commond id_col labels of x and y\n",
    "\n",
    "    Raises:\n",
    "        Warning when the labels in id_col of x and y do not match\n",
    "        \n",
    "    Author:\n",
    "        Dr. Calvin Chan\n",
    "        calvin.chan@bayer.com\n",
    "    '''\n",
    "    \n",
    "    if id_col is not None:\n",
    "\n",
    "        inds = []\n",
    "        data = []\n",
    "        for arg in args:\n",
    "            (ind,dat) = zip(*list(arg.groupby(id_col)))\n",
    "            inds.append(ind)\n",
    "            data.append(dat)\n",
    "\n",
    "        # Determine of ID entry is missing from any of the input dataset\n",
    "        id_not_match_flag = !(len(set.intersection(*[set(ind) for ind in inds])) == len(set.union(*[set(ind) for ind in inds])))\n",
    "        if not id_not_match_flag:\n",
    "            warnings.warn(\"Unmatch ID entries in one or more data inputs (eg. x, y)!\")\n",
    "\n",
    "        # Extract Common ID from x, s, y Samples\n",
    "        select_ids = set.intersection(*[set(ind) for ind in inds])\n",
    "\n",
    "        dataset = []\n",
    "        for i, dat in enumerate(data):\n",
    "            dataset.append([ dat[inds[i].index(single_id)].drop(id_col,axis=1) for single_id in select_ids ])\n",
    "\n",
    "    else:\n",
    "        # Determine number of elements in each input dataset is the same\n",
    "        data_length = list(set([ len(arg) for arg in args ]))\n",
    "        id_not_match_flag = !(len(data_length) == 1)\n",
    "        assert id_not_match_flag, \"Unmatch length in one or more data inputs (eg. x, y)!\"\n",
    "\n",
    "        select_ids = [*range(data_length[0])]\n",
    "        dataset = args\n",
    "\n",
    "    out = train_test_split(*dataset, test_size=test_ratio, random_state=random_state)\n",
    "\n",
    "    if report_id:\n",
    "        return(out, select_ids)\n",
    "    else:\n",
    "        return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test_split(*args, id_col=None, test_ratio=0.2, random_state=25, report_id=False):\n",
    "    \n",
    "    '''\n",
    "    Split the dataset into modeling and test set\n",
    "    \n",
    "    This function is to encapsulate the variying input feature size given the grouping by id_col,\n",
    "    and this decompose the one-hot-encoding column into a separate feature set to be used in the\n",
    "    deep learning model as separate input.\n",
    "    \n",
    "    Args:\n",
    "        *args:\n",
    "            x: A pandas dataframe with row as samples, and column as ID and feature type\n",
    "            y: A pandas dataframe with row as samples, and column as output\n",
    "        ohe_col: A list of column names indicating the one-hot-encoding columns in x\n",
    "        id_col: Column name of the grouping column to be converted to one-hot-encoding\n",
    "        test_size: The split ratio of the test set\n",
    "        random_state: Random seed use by the `sklearn.model_selection.train_test_split` function\n",
    "        retain_df: If this is 'True' and the input 'args' are dataframes, do not convert them to list of single row dataframe\n",
    "\n",
    "    Returns:\n",
    "        x_model, x_test: List of numpy matrix as model/test data split with from commond id_col labels of x and y\n",
    "        s_model, s_test: List of numpy matrix as model/test data split with from commond id_col labels of x and y\n",
    "        y_model, y_test: List of numpy matrix as model/test data split with from commond id_col labels of x and y\n",
    "\n",
    "    Raises:\n",
    "        Warning when the labels in id_col of x and y do not match\n",
    "        \n",
    "    Author:\n",
    "        Dr. Calvin Chan\n",
    "        calvin.chan@bayer.com\n",
    "    '''\n",
    "\n",
    "    if id_col is not None:\n",
    "\n",
    "        inds = []\n",
    "        data = []\n",
    "        for arg in args:\n",
    "            (ind,dat) = zip(*list(arg.groupby(id_col)))\n",
    "            inds.append(ind)\n",
    "            data.append(dat)\n",
    "\n",
    "        # Determine of ID entry is missing from any of the input dataset\n",
    "        id_not_match_flag = !(len(set.intersection(*[set(ind) for ind in inds])) == len(set.union(*[set(ind) for ind in inds])))\n",
    "        if not id_not_match_flag:\n",
    "            warnings.warn(\"Unmatch ID entries in one or more data inputs (eg. x, y)!\")\n",
    "\n",
    "        # Extract Common ID from x, s, y Samples\n",
    "        select_ids = list(set.intersection(*[set(ind) for ind in inds]))\n",
    "\n",
    "        # Split dataframes into sample list\n",
    "        # (multi-resolution: each list element contains multiple x and single y based on id_col)\n",
    "        dataset = []\n",
    "        for i, dat in enumerate(data):\n",
    "            dataset.append([ dat[inds[i].index(single_id)].drop(id_col,axis=1) for single_id in select_ids ])\n",
    "\n",
    "    else:\n",
    "        # Determine index labels in each input dataset is the same\n",
    "        dataset_indices = [ list(dataset.index) for dataset in args ]\n",
    "        select_ids = unique_list(dataset_indices)\n",
    "        \n",
    "        id_not_match_flag = !(len(data_length) == 1)\n",
    "        assert id_not_match_flag, \"Unmatch length in one or more data inputs (eg. x, y)!\"\n",
    "        select_ids = select_ids[0]\n",
    "        \n",
    "        # Split dataframes into sample list\n",
    "        # (equal resolution: each list element contains one row in both x and y)\n",
    "        dataset = []\n",
    "        for i, dat in enumerate(args):\n",
    "            dataset.append([ dat.loc[[single_id]] for single_id in select_ids ])\n",
    "\n",
    "    # Including index as one of the splitting dataset\n",
    "    dataset = dataset + [select_ids]\n",
    "    out = train_test_split(*dataset, test_size=test_ratio, random_state=random_state)\n",
    "    split_ids = out[-2:]\n",
    "    out = out[0:-2]\n",
    "\n",
    "    if report_id:\n",
    "        return(out, split_ids)\n",
    "    else:\n",
    "        return(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SgrpData(Dataset):\n",
    "#     def __init__(self, x, s, y, transform=None, id_col=\"cluster_id\"):\n",
    "        \n",
    "#         self.x_col = x[0].columns\n",
    "#         self.s_col = s[0].columns\n",
    "#         self.len = len(y)\n",
    "#         self.transform = transform\n",
    "        \n",
    "#         # === Preconvert Pandas Dataframe to Torch Tensor and move to GPU for Performance ===\n",
    "#         # Remove \"id_col\" and convert to matrix\n",
    "#         # (Common samples are extracted, therefore row ID of x,s,y must be aligned)\n",
    "#         self.x = [ torch.tensor(x_ele.values).type(torch.double) for x_ele in x ]\n",
    "#         self.s = [ torch.tensor(s_ele.values).type(torch.double) for s_ele in s ]\n",
    "#         self.y = [ torch.tensor(y_ele.values).type(torch.double) for y_ele in y ]\n",
    "        \n",
    "#     def __getitem__(self, index):\n",
    "        \n",
    "#         sample = [self.x[index],\n",
    "#                   self.s[index],\n",
    "#                   self.y[index]]\n",
    "#         if self.transform:\n",
    "#             sample = self.transform(sample)\n",
    "#         return sample\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SgrpDataBatch(Dataset):\n",
    "    '''\n",
    "    Data object for precomputing zero-patched dataset\n",
    "    \n",
    "    This is the data class for pytorch dataloader for input dataset with subgrouping/sgrpregation \n",
    "    input designed specifically for input x with higher resolution than output y.  Due to the\n",
    "    difference between the resolution between each input sample, zero patching is required to\n",
    "    perform mini-batch training. \n",
    "    \n",
    "    This class precompute the zero-patched samples and provides an internal flag to decide whether\n",
    "    a zero-patched sample is outputted during loading.\n",
    "\n",
    "    The initializing input data x, s, y are all converted to list of dataframe already by the \n",
    "    data spliting function `sgrp_split_parser`.  This is neede because the x and y have different\n",
    "    resolution, in order to match the resolution, splitting must be done prior creating the dataset\n",
    "    object.\n",
    "\n",
    "    Author:\n",
    "        Dr. Calvin Chan\n",
    "        calvin.chan@bayer.com\n",
    "    '''\n",
    "\n",
    "    def __init__(self, x, s, y, transform=None, zero_patch=False, dtype=torch.double, samples_id=None):\n",
    "        \n",
    "        input_equal_length_flag = (len(set([len(y), len(x), len(s)])) == 1)\n",
    "        assert input_equal_length_flag, \"Number of x and y samples do not match!\"\n",
    "        \n",
    "        self.x_col = x[0].columns\n",
    "        self.s_col = s[0].columns\n",
    "        self.len = len(y)\n",
    "        self.transform = transform\n",
    "        self.zero_patched = zero_patch\n",
    "        self.samples_id = samples_id\n",
    "        \n",
    "        # === Convert sample data type ===\n",
    "        # (Common samples are extracted, therefore row ID of x,s,y must be aligned)\n",
    "        self.x = self._sample_type_convert(x,dtype)\n",
    "        self.s = self._sample_type_convert(s,dtype)\n",
    "        self.y = self._sample_type_convert(y,dtype)\n",
    "        \n",
    "        # === Zero Patching x and s for Batch Training ===\n",
    "        max_sample = max([ x.size(0) for x in self.x ])\n",
    "        self.x_zero_patched = [ torch.cat([x.to(device), torch.zeros(max_sample - x.size(0), len(self.x_col)).to(device)], dim=0) for x in self.x ]\n",
    "        self.s_zero_patched = [ torch.cat([s.to(device), torch.zeros(max_sample - s.size(0), len(self.s_col)).to(device)], dim=0) for s in self.s ]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        if self.zero_patched:\n",
    "            sample = [self.x_zero_patched[index],\n",
    "                      self.s_zero_patched[index],\n",
    "                      self.y[index]]\n",
    "            \n",
    "        else:\n",
    "            sample = [self.x[index],\n",
    "                      self.s[index],\n",
    "                      self.y[index]]\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def _sample_type_convert(self, samples, dtype):\n",
    "        samples_out = [ torch.tensor(sample_ele.values).type(dtype) for sample_ele in samples ]\n",
    "        return samples_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Fold Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_k_fold_indices(n_samples, k=5, shuffle=False):\n",
    "    '''\n",
    "    Drawing sample indices for K-Fold\n",
    "    \n",
    "    Args:\n",
    "        samples: Number of samples in the dataset\n",
    "        shuffle: Shuffling of samples\n",
    "\n",
    "    Returns:\n",
    "        kfold_train_ind: Indices for training set\n",
    "        kfold_valid_ind: Indices for validation set\n",
    "\n",
    "    Raises:\n",
    "        -\n",
    "\n",
    "    Author:\n",
    "        Dr. Calvin Chan\n",
    "        calvin.chan@bayer.com\n",
    "    '''\n",
    "    kfold = KFold(n_splits=k, shuffle=shuffle).split([*range(n_samples)])\n",
    "    i, kfold_ind = zip(*[*enumerate(kfold)])   # Expand the index obtained by the K-Fold function\n",
    "    kfold_train_ind, kfold_valid_ind = zip(*kfold_ind)\n",
    "    return(kfold_train_ind, kfold_valid_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Creator\n",
    "* generalized for data with and without subgroup\n",
    "* x and y with same or different resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgrp_split_parser(x, y, test_ratio=0.2, sgrp_col=None, y_id_col=None, report_id=False):\n",
    "    '''\n",
    "    Parsing Subgroup and Dualscaled Data\n",
    "    Split the dualscaled x and y dataframe into a list of dataframe and extract the subgrouping information for Pytorch data object\n",
    "    \n",
    "    Args:\n",
    "        x: Pandas dataframe at same or higher resoultion than y\n",
    "        y: Pandas dataframe at same or lower resolution than x\n",
    "        test_ratio: The split ratio of the test set\n",
    "        sgrp_col: Column name of the column(s) indicating the branch/subgroup/sgrpregation in x\n",
    "        y_id_col: Common column for dualscaled x and y, the column id name which joins the 2 dataframes\n",
    "\n",
    "    Returns:\n",
    "        out: Tuple of 6 variables - x_model, x_test, s_model, s_test, y_model, y_test indicates the splited dataset\n",
    "        s_ohe_table: Subgroup one-hot-encoding conversion table\n",
    "            \n",
    "    Author:\n",
    "        Dr. Calvin Chan\n",
    "        calvin.chan@bayer.com\n",
    "    '''\n",
    "    if sgrp_col is None:\n",
    "        s_ohe = pd.DataFrame(np.ones([x.shape[0],1]), columns=['no_subgroup'])\n",
    "        s_ohe.index = x.index\n",
    "        s_ohe_table = None\n",
    "    else:\n",
    "        (s_ohe, s_ohe_table) = combine_multidimensional_ohe(x[[sgrp_col]])\n",
    "        x = x.drop(sgrp_col,axis=1)\n",
    "        \n",
    "    if y_id_col is not None:\n",
    "        s_ohe[y_id_col] = x[y_id_col]\n",
    "        \n",
    "    out, sample_ids = model_test_split(x, s_ohe, y, id_col=y_id_col, test_ratio=test_ratio, random_state=25, report_id=True)\n",
    "    \n",
    "    if report_id:\n",
    "        return (out, s_ohe_table, sample_ids)\n",
    "    else:\n",
    "        return (out, s_ohe_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_ind(ls,ind):\n",
    "    return [ ls[i] for i in ind.tolist() ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process and Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patitioned_data_object_cspd(x, y, sgrp_col, y_id_col, test_split_ratio, k):\n",
    "    # Model/Test Splitting\n",
    "    ((x_model, x_test, \n",
    "      s_model, s_test, \n",
    "      y_model, y_test), \n",
    "      s_ohe_table, \n",
    "     (samples_id_model, \n",
    "      samples_id_test)) = sgrp_split_parser(x, y, \n",
    "                                           test_ratio=test_split_ratio, \n",
    "                                           sgrp_col=subgroup_col, \n",
    "                                           y_id_col=y_id_col,\n",
    "                                           report_id=True)    \n",
    "    # K-Fold Index Sampling\n",
    "    [kfold_train_ind, kfold_valid_ind] = get_k_fold_indices(n_samples=len(y_model), k=k, shuffle=False)   # Shuffle is NOT needed, since the samples were shuffled in the model/test split\n",
    "\n",
    "    # Create K-set of datasets for Pytorch data loader\n",
    "    dataset_train_kfold = [ SgrpDataBatch(select_ind(x_model,fold_ind),\n",
    "                                         select_ind(s_model,fold_ind),\n",
    "                                         select_ind(y_model,fold_ind),\n",
    "                                         zero_patch = False,\n",
    "                                         samples_id = select_ind(samples_id_model, fold_ind) )\n",
    "                                               for fold_ind in kfold_train_ind ]\n",
    "    dataset_valid_kfold = [ SgrpDataBatch(select_ind(x_model,fold_ind),\n",
    "                                         select_ind(s_model,fold_ind),\n",
    "                                         select_ind(y_model,fold_ind),\n",
    "                                         zero_patch = False,\n",
    "                                         samples_id = select_ind(samples_id_model, fold_ind) )\n",
    "                                               for fold_ind in kfold_valid_ind ]\n",
    "\n",
    "    dataset_model = SgrpDataBatch(x_model, s_model, y_model, zero_patch = False, samples_id = samples_id_model)\n",
    "    dataset_test = SgrpDataBatch(x_test, s_test, y_test, zero_patch = False, samples_id = samples_id_test)\n",
    "    \n",
    "    return dataset_model, dataset_test, dataset_train_kfold, dataset_valid_kfold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neuron Custom Sampling Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integer_partitions(n_ele, n_min=1, max_dim=None, recursion_level=1):\n",
    "    '''\n",
    "    Fast Integer Partitioning\n",
    "    Dividing a single integer into a list of integer that sums up to the given number\n",
    "    \n",
    "    Args:\n",
    "        num_ele: Total number of elements to be distributed\n",
    "        n_min: Minimum number of elements per output dimension\n",
    "\n",
    "    Returns:\n",
    "        Iterator as list of elements splitted into multiple dimensions\n",
    "        \n",
    "    Original Source :\n",
    "    (Modification made to speed up by skpping recurrsion exceed max_dim)\n",
    "        https://stackoverflow.com/questions/10035752/elegant-python-code-for-integer-partitioning\n",
    "    \n",
    "    Author:\n",
    "        Dr. Calvin Chan\n",
    "        calvin.chan@bayer.com\n",
    "    '''\n",
    "    if (max_dim is not None) and (recursion_level > max_dim):\n",
    "        yield None\n",
    "    else:\n",
    "        yield (n_ele,)\n",
    "        for i in range(n_min, n_ele//2 + 1):\n",
    "            for p in integer_partitions(n_ele-i, i, max_dim, recursion_level+1):\n",
    "                if p is not None:\n",
    "                    yield (i,) + p\n",
    "                elif recursion_level != 1:\n",
    "                    yield None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sampling(num_ele, num_layers=None, n_min=1, n_max=None, n_samples=1, prepend=[], postpend=[], single_sample=False):\n",
    "    '''\n",
    "    Randomly split the elements into multiple dimensions\n",
    "    This is use for neuron sampling the number of elements and layer for multibranch neural network\n",
    "    \n",
    "    Args:\n",
    "        num_ele: Total number of elements to be distributed\n",
    "        n_min: Minimum number of elements per output dimension\n",
    "        n_max: Maximum number of elements per output dimension\n",
    "        num_layers: Number of layers to distribute the element, random dimensions will be given with None given\n",
    "\n",
    "    Returns:\n",
    "        sample: List of elements splitted into multiple dimensions\n",
    "        \n",
    "    Raises:\n",
    "        -\n",
    "        \n",
    "    Example:\n",
    "        >>> split_sampling(14, n_min=2, num_layers=4)\n",
    "        [2, 5, 4, 3]\n",
    "        \n",
    "    Author:\n",
    "        Dr. Calvin Chan\n",
    "        calvin.chan@bayer.com\n",
    "    '''\n",
    "    # !!! DEBUG !!!\n",
    "    # print(f\"num_ele: {num_ele}; n_min: {n_min}; num_layers: {num_layers}\")\n",
    "    \n",
    "    # Generate the Integer Partitions\n",
    "    splits = integer_partitions(num_ele, n_min=n_min, max_dim=num_layers)\n",
    "    if n_max is not None:\n",
    "        splits = [ split for split in splits if max(split) <= n_max ]\n",
    "    if num_layers is not None:\n",
    "        splits = [ split for split in splits if len(list(split)) == num_layers ]\n",
    "    else:\n",
    "        splits = [ split for split in splits ]\n",
    "    \n",
    "    # Filter with Number of Output Dimension\n",
    "    splits_perm = [list(set(itertools.permutations(split))) for split in splits ]\n",
    "    unique_splits_perm = list(itertools.chain.from_iterable(splits_perm))\n",
    "        \n",
    "    # Randomly Sample one of the permutation\n",
    "    if n_samples <= len(unique_splits_perm):\n",
    "        sample = list([ prepend+list(sample)+postpend for sample in random.sample(unique_splits_perm, k=n_samples)])\n",
    "    else:\n",
    "        sample = list([ prepend+list(sample)+postpend for sample in random.choices(unique_splits_perm, k=n_samples)])\n",
    "    if single_sample:\n",
    "        sample = sample[0]\n",
    "    \n",
    "    return(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __Remark__: Random Architecture Configuration Table could be useful when training needs to be done with repeated samples at different level for comparison.  Ray Tune grid search only run each given hyperparameter for once and mix with random choices of hyperparameters, therefore, for fair K-fold comparison the Radom Architecture Configuration Table method is requred!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature request had been make to Ray Tune repeated sampling on grid search to fix the above problem: https://discuss.ray.io/t/tune-feature-request-using-ray-tune-for-k-fold-with-repeated-grid-sampling/2541/2\n",
    "\n",
    "Once Ray Tune has this feature, Random Architecture Configuration Table will not be needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Neuron/Layer Architecture as Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_architecture_table(h_total_min, h_total_max, h_total_step,\n",
    "                               h_subgroup_dim, h_subgroup_min_neuron_per_sgrp, h_subgroup_max_neuron_per_sgrp, h_subgroup_n_samples,\n",
    "                               h_branch_min_neuron_per_layer, h_branch_max_neuron_per_layer, h_branch_n_samples):\n",
    "    '''\n",
    "    Generate Architecture Table for the Neural Network Architecture\n",
    "    \n",
    "    Args:\n",
    "        h_total_min, h_total_max, h_total_step: Equally spaced ampling criteria for total number of neuron\n",
    "        subgroup_dim: Subgroup output dimension (# subgroups)\n",
    "        h_subgroup_min_neuron_per_sgrp: Minimum total number of neuron per subgroup branch\n",
    "        h_subgroup_max_neuron_per_sgrp: Maximum total number of neuron per subgroup branch\n",
    "        h_subgroup_n_samples: Total number of sample for subgroup distribution (from splitted neuron distribution)\n",
    "        h_branch_min_neuron_per_layer: Minimum total number of neuron per layer\n",
    "        h_branch_max_neuron_per_layer: Maximum total number of neuron per layer\n",
    "        h_branch_n_samples: Total number of sample for branch distriubtion (from splitted neuron distribution)\n",
    "\n",
    "    Returns:\n",
    "        Pandas Dataframe with each row containing one random neural network architecture sample with the corresponding architecture information in each column.\n",
    "            \n",
    "    Author:\n",
    "        Dr. Calvin Chan\n",
    "        calvin.chan@bayer.com\n",
    "    '''\n",
    "    # Sampling the Architecture given the criteria\n",
    "    h_total = [*range(h_total_min, h_total_max+1, h_total_step)]\n",
    "    h_subgroup = [ split_sampling(num_ele = n_neuron, \n",
    "                             n_min = h_subgroup_min_neuron_per_sgrp, \n",
    "                             n_max = h_subgroup_max_neuron_per_sgrp,\n",
    "                             n_samples = h_subgroup_n_samples, \n",
    "                             num_layers = h_subgroup_dim) for n_neuron in h_total ]\n",
    "    h_branch = [[[ split_sampling(num_ele = n_subgroup, \n",
    "                              n_min = h_branch_min_neuron_per_layer,\n",
    "                              n_max = h_branch_max_neuron_per_layer,\n",
    "                              n_samples = h_branch_n_samples,\n",
    "                              num_layers = None) \n",
    "                for n_subgroup in sample ] for sample in total ] for total in h_subgroup ]\n",
    "    \n",
    "    # Compile Random Architecture Configuration Table (Table index is used as grid search for hyperparameter tunning)\n",
    "    total_table = pd.DataFrame({'total_id': [*range(len(h_total))], 'total': h_total})\n",
    "    \n",
    "    subgroup_table = pd.DataFrame(columns=['total_id', 'subgroup_id', 'subgroup'])\n",
    "    for total_id, total in enumerate(h_subgroup):\n",
    "        for subgroup_id, subgroup in enumerate(total):\n",
    "                subgroup_table.loc[len(subgroup_table)] = [total_id, subgroup_id, subgroup]\n",
    "                \n",
    "    sample_table = pd.DataFrame(columns=['total_id', 'subgroup_id', 'branch_id', 'sample_id', 'h_branch'])\n",
    "    for total_id, total in enumerate(h_branch):\n",
    "        for subgroup_id, subgroup in enumerate(total):\n",
    "            for branch_id, branch in enumerate(subgroup):\n",
    "                for sample_id, sample in enumerate(branch):\n",
    "                    sample_table.loc[len(sample_table)] = [total_id, subgroup_id, branch_id, sample_id, sample]\n",
    "                    \n",
    "    architecture_table = sample_table.groupby([\"total_id\",\"subgroup_id\",\"sample_id\"])[\"h_branch\"].apply(list).reset_index()\n",
    "    architecture_table = pd.merge(architecture_table, subgroup_table, how=\"left\", on=[\"total_id\", \"subgroup_id\"])\n",
    "    architecture_table = pd.merge(architecture_table, total_table, how=\"left\", on=[\"total_id\"])\n",
    "    \n",
    "    return(architecture_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_id</th>\n",
       "      <th>subgroup_id</th>\n",
       "      <th>total</th>\n",
       "      <th>subgroup</th>\n",
       "      <th>h_branch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>[7, 7, 7, 7, 12]</td>\n",
       "      <td>[[4, 3], [2, 2, 3], [2, 2, 3], [4, 3], [2, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>[7, 7, 7, 7, 12]</td>\n",
       "      <td>[[2, 2, 3], [4, 3], [2, 5], [2, 2, 3], [5, 2, 5]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>[7, 7, 8, 11, 7]</td>\n",
       "      <td>[[2, 3, 2], [4, 3], [3, 5], [5, 2, 2, 2], [2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>[7, 7, 8, 11, 7]</td>\n",
       "      <td>[[4, 3], [3, 4], [2, 4, 2], [4, 3, 4], [2, 5]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>[9, 7, 7, 8, 9]</td>\n",
       "      <td>[[2, 5, 2], [4, 3], [3, 2, 2], [2, 2, 4], [2, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  total_id subgroup_id  total          subgroup  \\\n",
       "0        0           0     40  [7, 7, 7, 7, 12]   \n",
       "1        0           0     40  [7, 7, 7, 7, 12]   \n",
       "2        0           1     40  [7, 7, 8, 11, 7]   \n",
       "3        0           1     40  [7, 7, 8, 11, 7]   \n",
       "4        0           2     40   [9, 7, 7, 8, 9]   \n",
       "\n",
       "                                            h_branch  \n",
       "0  [[4, 3], [2, 2, 3], [2, 2, 3], [4, 3], [2, 2, ...  \n",
       "1  [[2, 2, 3], [4, 3], [2, 5], [2, 2, 3], [5, 2, 5]]  \n",
       "2  [[2, 3, 2], [4, 3], [3, 5], [5, 2, 2, 2], [2, ...  \n",
       "3     [[4, 3], [3, 4], [2, 4, 2], [4, 3, 4], [2, 5]]  \n",
       "4  [[2, 5, 2], [4, 3], [3, 2, 2], [2, 2, 4], [2, ...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "architecture_table = compile_architecture_table(h_total_min,\n",
    "                                                h_total_max, \n",
    "                                                h_total_step,\n",
    "                                                N_SUBGROUP,\n",
    "                                                h_subgroup_min_neuron_per_sgrp,\n",
    "                                                h_subgroup_max_neuron_per_sgrp,\n",
    "                                                h_subgroup_n_samples,\n",
    "                                                h_branch_min_neuron_per_layer,\n",
    "                                                h_branch_max_neuron_per_layer,\n",
    "                                                h_branch_n_samples,\n",
    "                                                )\n",
    "\n",
    "architecture_table[[\"total_id\",\"subgroup_id\",\"total\",\"subgroup\",\"h_branch\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reporting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_zero_padded(y_est, y, sgrp, return_length=False):\n",
    "    '''\n",
    "    Remove Zero Padding Data for Batch Data\n",
    "    Due to the variable input resolution, zero padding is required for batch gradient decent for cspd algorithm.  Therefore, the zero padded batches could introduce a bias in the loss metric computation.  To avoid this problem, the zero padded data with all zeros for the subgroup indicator is used to remove these entries during error computation.\n",
    "    \n",
    "    Args:\n",
    "        y_est: Model prediction output\n",
    "        y: Training data output ground truth\n",
    "        sgrp: Subgrouping one-hot-encoded matrix for the batch data (B x O x S matrix, where B is batch size, O is output dimensions, S is number of subgroups)\n",
    "\n",
    "    Returns:\n",
    "        out: Loss metric variable\n",
    "\n",
    "    Raises:\n",
    "        -\n",
    "\n",
    "    Example:\n",
    "        \n",
    "\n",
    "    Author:\n",
    "        Dr. Calvin Chan\n",
    "        calvin.chan@bayer.com\n",
    "    '''\n",
    "    # identify non-zero padded data (not all subgroup flags are zero)\n",
    "    # :: first `torch.any` perform non-zero padded sample checking across subgroups\n",
    "    # :: second `torch.any` perform non-zero padded sample checking across input sample space within output sample\n",
    "    flag = torch.any(torch.any(sgrp,dim=2,keepdim=False),dim=1,keepdim=False)   \n",
    "    # select non-zero padded entries\n",
    "    y_est = y_est[flag]\n",
    "    y = y[flag]\n",
    "    \n",
    "    if not return_length:\n",
    "        return(y_est, y)\n",
    "    else:\n",
    "        return(y_est, y, sum(flag).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fifo(y_est, y, sgrp=None, history=None, queue_len=1000):\n",
    "    '''\n",
    "    Record Loss of Output Data\n",
    "    Due to the variable input resolution, zero padding is required for batch gradient decent for cspd algorithm.  Therefore, the zero padded batches could introduce a bias in the loss metric computation.  To avoid this problem, the zero padded data with all zeros for the subgroup indicator is used to remove these entries during error computation.\n",
    "    \n",
    "    Args:\n",
    "        y_est: Model prediction output\n",
    "        y: Training data output ground truth\n",
    "        sgrp: Subgrouping one-hot-encoded matrix for the batch data (B x O x S matrix, where B is batch size, O is output dimensions, S is number of subgroups)\n",
    "        queue_len: Maximum records to be stored in the history queue\n",
    "\n",
    "    Returns:\n",
    "        history: Dictionary of output to be reported, each dictionary element is a numpy array as a queue containing the history of past results.\n",
    "\n",
    "    Raises:\n",
    "        -\n",
    "\n",
    "    Example:\n",
    "        \n",
    "\n",
    "    Author:\n",
    "        Dr. Calvin Chan\n",
    "        calvin.chan@bayer.com\n",
    "    '''\n",
    "       \n",
    "    # remove zero-padded cases\n",
    "    if sgrp is not None:\n",
    "        y_est, y, num_pts = remove_zero_padded(y_est, y, sgrp, return_length=True)\n",
    "    else:\n",
    "        assert y_est.shape[0] == y.shape[0], \"y and y_est has different shape\"\n",
    "        num_pts = y_est.shape[0]\n",
    "    \n",
    "    num_pts = min(num_pts, queue_len)   # if queue is smaller than the number of results, truncate the front\n",
    "    y_est = y_est[-num_pts:]\n",
    "    y = y[-num_pts:]\n",
    "        \n",
    "    # managing the results FIFO queue\n",
    "    # :: push new sample and remove older samples\n",
    "    # :: keep the y, y_est in a FIFO for computing statistics\n",
    "    if history is None or len(history) == 0:\n",
    "        # initialize for the queue\n",
    "        history = {'y': y, 'y_est': y_est}\n",
    "    elif len(history['y']) < queue_len:\n",
    "        # insert y into non-empty queue and trim data extended beyond queue size\n",
    "        history['y'] = torch.cat( (history['y'], y), dim=0)[-queue_len:]\n",
    "        history['y_est'] = torch.cat( (history['y_est'], y_est), dim=0)[-queue_len:]\n",
    "    else:\n",
    "        # shift element and replace (push on FIFO)\n",
    "        history['y'] = torch.roll(history['y'], -num_pts, dims=0)\n",
    "        history['y'][-num_pts:] = y\n",
    "        history['y_est'] = torch.roll(history['y_est'], -num_pts, dims=0)\n",
    "        history['y_est'][-num_pts:] = y_est\n",
    "    \n",
    "    return(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iqr(e):\n",
    "    '''\n",
    "    Compute Loss IQR\n",
    "    \n",
    "    Args:\n",
    "        e: Error/Loss\n",
    "\n",
    "    Returns:\n",
    "        iqr: Interquartile range of the error\n",
    "\n",
    "    Raises:\n",
    "        -\n",
    "\n",
    "    Example:\n",
    "        \n",
    "\n",
    "    Author:\n",
    "        Dr. Calvin Chan\n",
    "        calvin.chan@bayer.com\n",
    "    '''\n",
    "    q75 = torch.quantile(e, 0.75)\n",
    "    q25 = torch.quantile(e, 0.25)\n",
    "    iqr = q75 - q25\n",
    "    return iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_l1_iqr(y_est, y):\n",
    "    '''\n",
    "    Compute L1 Loss IQR\n",
    "    \n",
    "    Args:\n",
    "        y_est: Model prediction output\n",
    "        y: Training data output ground truth\n",
    "\n",
    "    Returns:\n",
    "        iqr: Interquartile range of the error\n",
    "\n",
    "    Raises:\n",
    "        -\n",
    "\n",
    "    Example:\n",
    "        \n",
    "\n",
    "    Author:\n",
    "        Dr. Calvin Chan\n",
    "        calvin.chan@bayer.com\n",
    "    '''\n",
    "    e = torch.abs(y_est - y)\n",
    "    q75 = torch.quantile(e, 0.75)\n",
    "    q25 = torch.quantile(e, 0.25)\n",
    "    iqr = q75 - q25\n",
    "    return iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mape_iqr(y_est, y):\n",
    "    '''\n",
    "    Compute Error IQR\n",
    "    \n",
    "    Args:\n",
    "        y_est: Model prediction output\n",
    "        y: Training data output ground truth\n",
    "\n",
    "    Returns:\n",
    "        iqr: Interquartile range of the error\n",
    "\n",
    "    Raises:\n",
    "        -\n",
    "\n",
    "    Example:\n",
    "        \n",
    "\n",
    "    Author:\n",
    "        Dr. Calvin Chan\n",
    "        calvin.chan@bayer.com\n",
    "    '''\n",
    "    e = torch.abs(y_est - y)\n",
    "    q75 = torch.quantile(e, 0.75)\n",
    "    q25 = torch.quantile(e, 0.25)\n",
    "    iqr = q75 - q25\n",
    "    return iqr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ray Tune Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training procedure\n",
    "def train_cspd_raytune(config, \n",
    "                       num_in_feat,\n",
    "                       num_branch,\n",
    "                       criterion=nn.MSELoss(),\n",
    "                       checkpoint_dir=None, \n",
    "                       num_epochs=100, \n",
    "                       train_dataset=None, \n",
    "                       valid_dataset=None,\n",
    "                       metric_dict={'rmse':     lambda y_est,y: torch.sqrt(nn.MSELoss(reduction=\"mean\")(y_est,y)),\n",
    "                                    'mean_l1':  lambda y_est,y: nn.L1Loss(reduction=\"mean\")(y_est,y),\n",
    "                                    'l1_iqr':   lambda y_est,y: compute_iqr(nn.L1Loss(reduction=\"none\")(y_est,y)),\n",
    "                                    'med-ape':  lambda y_est,y: torch.median((y-y_est).abs()/y.abs()),\n",
    "                                    'mape':     lambda y_est,y: torch.mean((y-y_est).abs()/y.abs()),\n",
    "                                    'mape_iqr': lambda y_est,y: compute_iqr((y-y_est).abs()/y.abs())},\n",
    "                       train_metric_samples=None,\n",
    "                       force_cpu=False\n",
    "                      ):\n",
    "    '''\n",
    "    Training procedure for cspd regression with Ray Tune hyperparameter tuning\n",
    "    This function is to be used for training with hyperparameter tuning based on Ray Tune. A cspd architecture table is given and the following hyperparameters are sampled by Ray Tune:\n",
    "        lr: learning rate\n",
    "        h_branch: neural network architecture definition\n",
    "        dropout_p: dropout probability of all the neurons in the network\n",
    "        k: k-fold index k for the dataset\n",
    "        batch_size: the batch size use for the mini-batch use for batch gradient descent\n",
    "\n",
    "    Args:\n",
    "    (Note: This function is not meant to run directly by user, these arguemnts are passed indirectly by tune.run.)\n",
    "        config: Ray Tune hyperparameter sampling configuration (for details, please refer to: https://docs.ray.io/en/master/tune/user-guide.html)\n",
    "        checkpoint_dir: Output directory of training log, including the tensorboard output\n",
    "        num_epochs: Number of training epochs\n",
    "        num_in_feat: Number of input features for the network\n",
    "        num_branch: Number of parallel branches in the network (subgroups)\n",
    "        train_dataset: List of K element SgrpDataBatch class Pytorch dataloader object\n",
    "        valid_dataset: List of K element SgrpDataBatch class Pytorch dataloader object\n",
    "        metric_dict: Dictionary of loss function to be use for metric reporting (Attention: These are only used for reporting, not as training loss function!)\n",
    "\n",
    "    Returns:\n",
    "        result is return indirectly with tune.run\n",
    "\n",
    "    Raises:\n",
    "        -\n",
    "\n",
    "    Example:\n",
    "        -\n",
    "    '''\n",
    "\n",
    "    #====================== Ray Tune Parameters Setup ======================#\n",
    "\n",
    "    if 'dropout_p' in config.keys():\n",
    "        _dropout_p = config['dropout_p']\n",
    "    else:\n",
    "        _dropout_p = 0\n",
    "\n",
    "    if 'batch_size' in config.keys():\n",
    "        _batch_size = config['batch_size']\n",
    "    else:\n",
    "        _batch_size = 1\n",
    "        \n",
    "    # determine if input is k-fold dataset or single dataset\n",
    "    if (type(train_dataset) is list) and (type(valid_dataset) is list) and ('k' in config.keys()):\n",
    "        _train_dataset = train_dataset[config['k']]\n",
    "        _valid_dataset = valid_dataset[config['k']]\n",
    "    else:\n",
    "        _train_dataset = train_dataset\n",
    "        _valid_dataset = valid_dataset\n",
    "\n",
    "    # zero patch high-res dimension dataset if we are doing mini-batch\n",
    "    if _batch_size > 1:\n",
    "        _train_dataset.zero_patched = True\n",
    "        _valid_dataset.zero_patched = True\n",
    "    else:\n",
    "        _train_dataset.zero_patched = False\n",
    "        _valid_dataset.zero_patched = False\n",
    "\n",
    "    # measure error metric across whole epoch if no sample length is given\n",
    "    # (the latest progress might not be shown properly and error could be overestimated by earlier samples)\n",
    "    if train_metric_samples is None:\n",
    "        train_metric_samples = len(_train_dataset)\n",
    "    \n",
    "    # gpu usage\n",
    "    if not force_cpu:\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=_train_dataset, batch_size=_batch_size, shuffle=True, \n",
    "                                               collate_fn=lambda x: [ x_ele.to(device) for x_ele in default_collate(x) ] )\n",
    "    valid_loader = torch.utils.data.DataLoader(dataset=_valid_dataset, batch_size=_batch_size, shuffle=True,\n",
    "                                               collate_fn=lambda x: [ x_ele.to(device) for x_ele in default_collate(x) ] )\n",
    "\n",
    "    #====================== Model Setup ======================#\n",
    "\n",
    "    # initialize ANN architecture\n",
    "    model = cspd(in_feat = num_in_feat, \n",
    "                 Subgroups = num_branch, \n",
    "                 Architecture = config['h_branch'], \n",
    "                 dropout_p = _dropout_p,\n",
    "                 act_fn = torch.relu)\n",
    "    model.apply(initialize_weights)\n",
    "\n",
    "    # gpu usage\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)   # for multiple GPUs\n",
    "    model.to(device)\n",
    "\n",
    "    \n",
    "    # optimizer is controlled by ray tune hyperparameter\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = config[\"lr\"])\n",
    "    \n",
    "    # The `checkpoint_dir` parameter gets passed by Ray Tune when a checkpoint\n",
    "    # should be restored.\n",
    "    if checkpoint_dir:\n",
    "        checkpoint = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "        model_state, optimizer_state = torch.load(checkpoint)\n",
    "        model.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "    \n",
    "\n",
    "    # create loss metric dictionary to store results\n",
    "    history = {'train': {}, 'valid': {}}\n",
    "    metric_output = {}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        #====================== Training ======================#\n",
    "\n",
    "        # training using all training samples\n",
    "        for i, (x, s, y) in enumerate(train_loader):\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # set the model to training mode\n",
    "            model.train()\n",
    "            \n",
    "            # forward + backward + optimize\n",
    "            y_est = model(x, s)\n",
    "            y_est, y = remove_zero_padded(y_est, y, s)\n",
    "            loss = criterion(y_est, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # record the prediction results\n",
    "            # :: the following function is use to remove zero-padded samples in batch training\n",
    "            # :: loss metrics are kept in a FIFO queue per latest samples in order to compute statistics\n",
    "            history['train'] = loss_fifo(y_est, y, sgrp=s, history=history['train'], queue_len=train_metric_samples)\n",
    "            \n",
    "        #====================== Validation ======================#    \n",
    "        \n",
    "        # set the model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # training using all validation samples\n",
    "        with torch.no_grad():\n",
    "            for  i, (x, s, y) in enumerate(valid_loader):\n",
    "                y_est = model(x, s)\n",
    "                # record the prediction results\n",
    "                # :: the following function is use to remove zero-padded samples in batch training\n",
    "                # :: loss metrics are kept in a FIFO queue per latest samples in order to compute statistics\n",
    "                history['valid'] = loss_fifo(y_est, y, sgrp=s, history=history['valid'], queue_len=len(_valid_dataset))\n",
    "                \n",
    "        for metric in metric_dict.keys():\n",
    "            for dataset in history.keys():\n",
    "                metric_label = '_'.join([dataset,metric])\n",
    "                metric_output[metric_label] = metric_dict[metric](history[dataset]['y_est'],history[dataset]['y']).item()\n",
    "                \n",
    "        # Here we save a checkpoint. It is automatically registered with\n",
    "        # Ray Tune and will potentially be passed as the `checkpoint_dir`\n",
    "        # parameter in future iterations.\n",
    "        with tune.checkpoint_dir(step=epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save( (model.state_dict(), optimizer.state_dict()), path )\n",
    "\n",
    "        tune.report(**metric_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training routine to be use for manual training with __No Hyperparameter Tuning__ with Ray Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training procedure\n",
    "def train_cspd(model, train_dataset, valid_dataset, criterion, optimizer, \n",
    "               epochs=100, \n",
    "               batch_size=1, \n",
    "               metric_dict={'rmse':     lambda y_est,y: torch.sqrt(nn.MSELoss(reduction=\"mean\")(y_est,y)),\n",
    "                            'mean_l1':  lambda y_est,y: nn.L1Loss(reduction=\"mean\")(y_est,y),\n",
    "                            'l1_iqr':   lambda y_est,y: compute_iqr(nn.L1Loss(reduction=\"none\")(y_est,y)),\n",
    "                            'med-ape':  lambda y_est,y: torch.median((y-y_est).abs()/y.abs()),\n",
    "                            'mape':     lambda y_est,y: torch.mean((y-y_est).abs()/y.abs()),\n",
    "                            'mape_iqr': lambda y_est,y: compute_iqr((y-y_est).abs()/y.abs())},\n",
    "               train_metric_samples=None,\n",
    "               ):\n",
    "    '''\n",
    "    Training procedure for cspd regression\n",
    "    This function is to be used for training without hyperparameter optimization, this function is usually use for test run to make sure all modification on the cspd architecture is working before submitting a list of models for hyperparameter search. To use hyperparameter optimization, please use either `train_cspd_raytune` or `train_cspd_raytune_auto_architecture`.\n",
    "    \n",
    "    Args:\n",
    "        model: Pytorch model object of cspd\n",
    "        train_dataset: List of K element SgrpDataBatch class Pytorch dataloader object\n",
    "        valid_dataset: List of K element SgrpDataBatch class Pytorch dataloader object\n",
    "        criterion: Training criterion to be used (eg. criterion = nn.MSELoss())\n",
    "        optimizer: Training optimizer to be used (eg. optimizer = torch.optim.Adam(model.parameters(), lr = 0.1))\n",
    "        epochs: Number of training epochs to be used\n",
    "        batch_size: The batch size to use for batch gradient descent of the output dimension, the input dimension will be setted to zero patching within the SgrpDataBatch object for comparable input size to perform the stacked computation\n",
    "        metric_dict: Dictionary of loss function to be use for metric reporting (Attention: These are only used for reporting, not as training loss function!)\n",
    "        history_queue_len: The number of loss result samples to keep for statistical reporting\n",
    "\n",
    "    Returns:\n",
    "        history: Training and validation results summary\n",
    "        model: Implicitly updated in the model object\n",
    "\n",
    "    Raises:\n",
    "        -\n",
    "\n",
    "    Example:\n",
    "        # Example of cspd training with no subgroupings\n",
    "        # (Remark: s_model and s_test are all generated with all 1's by model_test_split function with ohe_cols=None)\n",
    "        dataset_model, dataset_test, dataset_train_kfold, dataset_valid_kfold = patitioned_data_object_cspd(x, y, subgroup_col, y_id_col, test_split_ratio, k)\n",
    "        dataset_train = dataset_train_kfold\n",
    "        dataset_valid = dataset_valid_kfold\n",
    "        architecture = [ [2,2,3,2,2] ]   # single branch with 5 layers\n",
    "        model = cspd(in_feat=10, Subgroups=1, Architecture=architecture, dropout_p=0.3)\n",
    "        model.apply(initialize_weights)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = 0.1)\n",
    "        criterion = nn.MSELoss()\n",
    "        metric_dict = {'rmse': lambda y_est,y: torch.sqrt(nn.MSELoss(reduction=\"none\")(y_est,y)), \n",
    "                       'mape': lambda y_est,y: (y-y_est).abs()/y.abs()}\n",
    "        training_results = train_cspd(model=model, \n",
    "                                      train_dataset=dataset_model, \n",
    "                                      valid_dataset=dataset_test, \n",
    "                                      criterion=criterion,\n",
    "                                      optimizer=optimizer,\n",
    "                                      metric_dict=metric_dict,\n",
    "                                      epochs=num_epochs, \n",
    "                                      batch_size=64)        \n",
    "\n",
    "    Author:\n",
    "        Dr. Calvin Chan\n",
    "        calvin.chan@bayer.com\n",
    "    '''\n",
    "    history = {'train': {}, 'valid': {}}\n",
    "    metric_output = {}\n",
    "\n",
    "    if train_metric_samples is None:\n",
    "        train_metric_samples = len(train_dataset)\n",
    "    \n",
    "    if batch_size > 1:\n",
    "        train_dataset.zero_patched = True\n",
    "        valid_dataset.zero_patched = True\n",
    "    else:\n",
    "        train_dataset.zero_patched = False\n",
    "        valid_dataset.zero_patched = False\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        #====================== Training ======================#\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "\n",
    "        # training using all training samples\n",
    "        for i, (x, sgrp, y) in enumerate(train_loader):\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # set the model to training mode\n",
    "            model.train()\n",
    "            \n",
    "            # forward + backward + optimize\n",
    "            y_est = model(x, sgrp)\n",
    "            y_est, y = remove_zero_padded(y_est, y, sgrp)\n",
    "            loss = criterion(y_est, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            history['train'] = loss_fifo(y_est, y, sgrp=sgrp, history=history['train'], queue_len=train_metric_samples)\n",
    "\n",
    "        #====================== Validation ======================#\n",
    "        \n",
    "        # set the model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # training using all validation samples\n",
    "        with torch.no_grad():\n",
    "            for  i, (x, sgrp, y) in enumerate(valid_loader):\n",
    "                y_est = model(x, sgrp)\n",
    "                history['valid'] = loss_fifo(y_est, y, sgrp=sgrp, history=history['valid'], queue_len=len(valid_dataset))\n",
    "                \n",
    "    \n",
    "        metric_labels = []\n",
    "        for metric in metric_dict.keys():\n",
    "            for dataset in history.keys():\n",
    "                metric_label = '_'.join([dataset,metric])\n",
    "                metric_output[metric_label] = metric_dict[metric](history[dataset]['y_est'],history[dataset]['y']).item()\n",
    "                metric_labels.append(metric_label)\n",
    "                        \n",
    "        print(f\"[Epoch: { epoch+1 }]\", end=\" \" )\n",
    "        for metric_label in metric_labels:\n",
    "            print(f\"{metric_label}: {metric_output[metric_label]:.3f},\", end=\" \")\n",
    "        print(f\"\")\n",
    "\n",
    "    return (history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cspd_raytune_cpu_gpu_distributed(config, \n",
    "                                           num_in_feat,\n",
    "                                           num_branch,\n",
    "                                           criterion=nn.MSELoss(),\n",
    "                                           checkpoint_dir=None, \n",
    "                                           num_epochs=100, \n",
    "                                           train_dataset=None, \n",
    "                                           valid_dataset=None,\n",
    "                                           metric_dict={'rmse':     lambda y_est,y: torch.sqrt(nn.MSELoss(reduction=\"mean\")(y_est,y)),\n",
    "                                                        'mean_l1':  lambda y_est,y: nn.L1Loss(reduction=\"mean\")(y_est,y),\n",
    "                                                        'l1_iqr':   lambda y_est,y: compute_iqr(nn.L1Loss(reduction=\"none\")(y_est,y)),\n",
    "                                                        'med-ape':  lambda y_est,y: torch.median((y-y_est).abs()/y.abs()),\n",
    "                                                        'mape':     lambda y_est,y: torch.mean((y-y_est).abs()/y.abs()),\n",
    "                                                        'mape_iqr': lambda y_est,y: compute_iqr((y-y_est).abs()/y.abs())},\n",
    "                                           train_metric_samples=None,\n",
    "                                           ):\n",
    "    '''\n",
    "    CPU/GPU Distributed Wrapper Function for Training procedure for cspd regression\n",
    "    This function is written to allow training done on both CPU and GPU of a single machine at the same time.\n",
    "    \n",
    "    Args:\n",
    "\n",
    "    Returns:\n",
    "        result: Training metric results\n",
    "\n",
    "    Source:\n",
    "        This code is modified from the following: https://discuss.ray.io/t/different-trial-on-cpu-and-gpu-separately/2883\n",
    "\n",
    "    Author:\n",
    "        Dr. Calvin Chan\n",
    "        calvin.chan@bayer.com\n",
    "    '''\n",
    "    \n",
    "    a = filelock.FileLock(\"/tmp/gpu.lock\")\n",
    "    try:\n",
    "        # Makes it so that 1 trial will use the GPU at once.\n",
    "        a.acquire(timeout=1)\n",
    "        result = train_cspd_raytune(config, \n",
    "                                    num_in_feat,\n",
    "                                    num_branch,\n",
    "                                    criterion,\n",
    "                                    checkpoint_dir, \n",
    "                                    num_epochs, \n",
    "                                    train_dataset, \n",
    "                                    valid_dataset,\n",
    "                                    metric_dict,\n",
    "                                    train_metric_samples,\n",
    "                                    force_cpu=False\n",
    "                                    )\n",
    "    except filelock.Timeout:\n",
    "        # If the lock is acquired, you can just use CPU, and disable GPU access.\n",
    "        result = train_cspd_raytune(config, \n",
    "                                    num_in_feat,\n",
    "                                    num_branch,\n",
    "                                    criterion,\n",
    "                                    checkpoint_dir, \n",
    "                                    num_epochs, \n",
    "                                    train_dataset, \n",
    "                                    valid_dataset,\n",
    "                                    metric_dict,\n",
    "                                    train_metric_samples,\n",
    "                                    force_cpu=True\n",
    "                                    )\n",
    "    finally:\n",
    "        # Release the lock after training is done.\n",
    "        a.release()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_model, dataset_test, dataset_train_kfold, dataset_valid_kfold = patitioned_data_object_cspd(x, y, subgroup_col, y_id_col, test_split_ratio, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for Joining Results and Architecture Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_nested_numeric_to_string(in_list):\n",
    "    return(' ; '.join([' '.join([str(c) for c in lst]) for lst in in_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_nested(left, right, on):\n",
    "    left['key'] = left[on].apply(convert_nested_numeric_to_string)\n",
    "    right['key'] = right[on].apply(convert_nested_numeric_to_string)\n",
    "    out = pd.merge(left.drop(columns=[on]), right, on='key', how='left')\n",
    "    out = out.drop(columns=['key'])\n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning (Ray Tune Using Network Architecture Table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training for Ray Tune with __Ramdom Sampled Network Architecture Hyperparameters__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "architectures = architecture_table['h_branch'][0:5].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_metrics = [\"training_iteration\",\n",
    "                  \"train_rmse\", \n",
    "                  \"valid_rmse\",\n",
    "                  \"train_mean_l1\", \n",
    "                  \"valid_mean_l1\",\n",
    "                  \"train_l1_iqr\",\n",
    "                  \"valid_l1_iqr\",\n",
    "                  \"train_med-ape\",\n",
    "                  \"valid_med-ape\",\n",
    "                  \"train_mape\", \n",
    "                  \"valid_mape\",\n",
    "                  \"train_mape_iqr\", \n",
    "                  \"valid_mape_iqr\",\n",
    "                 ]\n",
    "\n",
    "# reporter = tune.CLIReporter(max_progress_rows=35, metric_columns=report_metrics)\n",
    "reporter = tune.JupyterNotebookReporter(overwrite=True, max_progress_rows=35,  metric_columns= report_metrics)\n",
    "scheduler = HyperBandScheduler(metric=\"valid_mape\", mode=\"min\", max_t=num_epochs)\n",
    "searchopt = BasicVariantGenerator(max_concurrent=15)\n",
    "\n",
    "config = {'lr': tune.loguniform(lr_min, lr_max),                     # Learning Rate\n",
    "          \"dropout_p\": tune.uniform(dropout_p_min, dropout_p_max),   # Dropout On/Off\n",
    "          'k': tune.grid_search([*range(k)]),                        # K-Fold Index\n",
    "          'batch_size': tune.choice(batch_size),                     # 1: SGD; 2+: Zero-Filled BGD\n",
    "          'h_branch': tune.grid_search(architectures),\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 4.1/59.9 GiB<br>Using HyperBand: num_stopped=10 total_brackets=10\n",
       "Round #0:\n",
       "  Bracket(Max Size (n)=2, Milestone (r)=5, completed=100.0%): {TERMINATED: 2} \n",
       "  Bracket(Max Size (n)=1, Milestone (r)=3, completed=100.0%): {TERMINATED: 3} \n",
       "Round #1:\n",
       "  Bracket(Max Size (n)=2, Milestone (r)=5, completed=100.0%): {TERMINATED: 2} \n",
       "  Bracket(Max Size (n)=1, Milestone (r)=3, completed=100.0%): {TERMINATED: 3} \n",
       "Round #2:\n",
       "  Bracket(Max Size (n)=2, Milestone (r)=5, completed=100.0%): {TERMINATED: 2} \n",
       "  Bracket(Max Size (n)=1, Milestone (r)=3, completed=100.0%): {TERMINATED: 3} \n",
       "Round #3:\n",
       "  Bracket(Max Size (n)=2, Milestone (r)=5, completed=100.0%): {TERMINATED: 2} \n",
       "  Bracket(Max Size (n)=1, Milestone (r)=3, completed=100.0%): {TERMINATED: 3} \n",
       "Round #4:\n",
       "  Bracket(Max Size (n)=2, Milestone (r)=5, completed=100.0%): {TERMINATED: 2} \n",
       "  Bracket(Max Size (n)=1, Milestone (r)=3, completed=100.0%): {TERMINATED: 3} <br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/34.68 GiB heap, 0.0/17.34 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /home/calvin_chan/data/output/checkpoint/testing/train_cspd_raytune_cpu_gpu_distributed_2022-03-25_16-45-01<br>Number of trials: 25/25 (25 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                                        </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  dropout_p</th><th>h_branch                                               </th><th style=\"text-align: right;\">  k</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  training_iteration</th><th style=\"text-align: right;\">  train_rmse</th><th style=\"text-align: right;\">  valid_rmse</th><th style=\"text-align: right;\">  train_mean_l1</th><th style=\"text-align: right;\">  valid_mean_l1</th><th style=\"text-align: right;\">  train_l1_iqr</th><th style=\"text-align: right;\">  valid_l1_iqr</th><th style=\"text-align: right;\">  train_med-ape</th><th style=\"text-align: right;\">  valid_med-ape</th><th style=\"text-align: right;\">  train_mape</th><th style=\"text-align: right;\">  valid_mape</th><th style=\"text-align: right;\">  train_mape_iqr</th><th style=\"text-align: right;\">  valid_mape_iqr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_cspd_raytune_cpu_gpu_distributed_e4721_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">  0.0853914</td><td>[[4, 3], [2, 2, 3], [2, 2, 3], [4, 3], [2, 2, 3, 3, 2]]</td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">0.00176027 </td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">     44517.7</td><td style=\"text-align: right;\">     45598.1</td><td style=\"text-align: right;\">        38371.2</td><td style=\"text-align: right;\">        39433.7</td><td style=\"text-align: right;\">       37274.8</td><td style=\"text-align: right;\">       38981.5</td><td style=\"text-align: right;\">       0.963136</td><td style=\"text-align: right;\">       0.962158</td><td style=\"text-align: right;\">    0.916497</td><td style=\"text-align: right;\">    0.91754 </td><td style=\"text-align: right;\">      0.0527474 </td><td style=\"text-align: right;\">      0.0454284 </td></tr>\n",
       "<tr><td>train_cspd_raytune_cpu_gpu_distributed_e4721_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">  0.697423 </td><td>[[2, 2, 3], [4, 3], [2, 5], [2, 2, 3], [5, 2, 5]]      </td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">0.0033187  </td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">     42677.2</td><td style=\"text-align: right;\">     43943.8</td><td style=\"text-align: right;\">        36553.9</td><td style=\"text-align: right;\">        37580.6</td><td style=\"text-align: right;\">       38286.4</td><td style=\"text-align: right;\">       39306.7</td><td style=\"text-align: right;\">       0.914488</td><td style=\"text-align: right;\">       0.918149</td><td style=\"text-align: right;\">    0.862635</td><td style=\"text-align: right;\">    0.858257</td><td style=\"text-align: right;\">      0.103201  </td><td style=\"text-align: right;\">      0.0915404 </td></tr>\n",
       "<tr><td>train_cspd_raytune_cpu_gpu_distributed_e4721_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">  0.453953 </td><td>[[2, 3, 2], [4, 3], [3, 5], [5, 2, 2, 2], [2, 3, 2]]   </td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">0.000382137</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">     46084.4</td><td style=\"text-align: right;\">     46850  </td><td style=\"text-align: right;\">        40217.4</td><td style=\"text-align: right;\">        40871.4</td><td style=\"text-align: right;\">       38416.5</td><td style=\"text-align: right;\">       39236.2</td><td style=\"text-align: right;\">       0.996796</td><td style=\"text-align: right;\">       0.99705 </td><td style=\"text-align: right;\">    0.992111</td><td style=\"text-align: right;\">    0.993479</td><td style=\"text-align: right;\">      0.00485981</td><td style=\"text-align: right;\">      0.00352699</td></tr>\n",
       "<tr><td>train_cspd_raytune_cpu_gpu_distributed_e4721_00003</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">  0.403146 </td><td>[[4, 3], [3, 4], [2, 4, 2], [4, 3, 4], [2, 5]]         </td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">0.0346138  </td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">     29202  </td><td style=\"text-align: right;\">     28360.4</td><td style=\"text-align: right;\">        23474.6</td><td style=\"text-align: right;\">        23395.6</td><td style=\"text-align: right;\">       29565.5</td><td style=\"text-align: right;\">       26093.8</td><td style=\"text-align: right;\">       0.602103</td><td style=\"text-align: right;\">       0.556996</td><td style=\"text-align: right;\">    0.852773</td><td style=\"text-align: right;\">    1.05081 </td><td style=\"text-align: right;\">      0.364137  </td><td style=\"text-align: right;\">      0.325352  </td></tr>\n",
       "<tr><td>train_cspd_raytune_cpu_gpu_distributed_e4721_00004</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">  0.612245 </td><td>[[2, 5, 2], [4, 3], [3, 2, 2], [2, 2, 4], [2, 2, 5]]   </td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">0.00632615 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">     43081.1</td><td style=\"text-align: right;\">     42682.3</td><td style=\"text-align: right;\">        37001.7</td><td style=\"text-align: right;\">        36225.2</td><td style=\"text-align: right;\">       35847.7</td><td style=\"text-align: right;\">       38796.3</td><td style=\"text-align: right;\">       0.895171</td><td style=\"text-align: right;\">       0.884017</td><td style=\"text-align: right;\">    0.835857</td><td style=\"text-align: right;\">    0.830839</td><td style=\"text-align: right;\">      0.120791  </td><td style=\"text-align: right;\">      0.125901  </td></tr>\n",
       "<tr><td>train_cspd_raytune_cpu_gpu_distributed_e4721_00005</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">  0.388295 </td><td>[[4, 3], [2, 2, 3], [2, 2, 3], [4, 3], [2, 2, 3, 3, 2]]</td><td style=\"text-align: right;\">  1</td><td style=\"text-align: right;\">0.0508286  </td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">     23754.4</td><td style=\"text-align: right;\">     23089.2</td><td style=\"text-align: right;\">        20412.6</td><td style=\"text-align: right;\">        19869.4</td><td style=\"text-align: right;\">       19479.5</td><td style=\"text-align: right;\">       19547.6</td><td style=\"text-align: right;\">       0.429355</td><td style=\"text-align: right;\">       0.420032</td><td style=\"text-align: right;\">    1.47403 </td><td style=\"text-align: right;\">    1.37029 </td><td style=\"text-align: right;\">      0.605378  </td><td style=\"text-align: right;\">      0.584318  </td></tr>\n",
       "<tr><td>train_cspd_raytune_cpu_gpu_distributed_e4721_00006</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">  0.328781 </td><td>[[2, 2, 3], [4, 3], [2, 5], [2, 2, 3], [5, 2, 5]]      </td><td style=\"text-align: right;\">  1</td><td style=\"text-align: right;\">0.0016617  </td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">     43369.5</td><td style=\"text-align: right;\">     45609.2</td><td style=\"text-align: right;\">        36738  </td><td style=\"text-align: right;\">        39547.6</td><td style=\"text-align: right;\">       42050.7</td><td style=\"text-align: right;\">       39837.9</td><td style=\"text-align: right;\">       0.958085</td><td style=\"text-align: right;\">       0.961793</td><td style=\"text-align: right;\">    0.916795</td><td style=\"text-align: right;\">    0.922185</td><td style=\"text-align: right;\">      0.0647695 </td><td style=\"text-align: right;\">      0.0442157 </td></tr>\n",
       "<tr><td>train_cspd_raytune_cpu_gpu_distributed_e4721_00007</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">  0.376204 </td><td>[[2, 3, 2], [4, 3], [3, 5], [5, 2, 2, 2], [2, 3, 2]]   </td><td style=\"text-align: right;\">  1</td><td style=\"text-align: right;\">0.0384199  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">     24629.3</td><td style=\"text-align: right;\">     24030.4</td><td style=\"text-align: right;\">        20722.6</td><td style=\"text-align: right;\">        20356.9</td><td style=\"text-align: right;\">       21638.8</td><td style=\"text-align: right;\">       20620.4</td><td style=\"text-align: right;\">       0.452192</td><td style=\"text-align: right;\">       0.431487</td><td style=\"text-align: right;\">    1.38091 </td><td style=\"text-align: right;\">    1.30625 </td><td style=\"text-align: right;\">      0.415867  </td><td style=\"text-align: right;\">      0.461189  </td></tr>\n",
       "<tr><td>train_cspd_raytune_cpu_gpu_distributed_e4721_00008</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">  0.459848 </td><td>[[4, 3], [3, 4], [2, 4, 2], [4, 3, 4], [2, 5]]         </td><td style=\"text-align: right;\">  1</td><td style=\"text-align: right;\">0.0141262  </td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">     47510.9</td><td style=\"text-align: right;\">     45282.2</td><td style=\"text-align: right;\">        42019  </td><td style=\"text-align: right;\">        39176  </td><td style=\"text-align: right;\">       37301.9</td><td style=\"text-align: right;\">       39709.3</td><td style=\"text-align: right;\">       0.960628</td><td style=\"text-align: right;\">       0.952846</td><td style=\"text-align: right;\">    0.922977</td><td style=\"text-align: right;\">    0.907597</td><td style=\"text-align: right;\">      0.0378298 </td><td style=\"text-align: right;\">      0.0582972 </td></tr>\n",
       "<tr><td>train_cspd_raytune_cpu_gpu_distributed_e4721_00009</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">  0.259113 </td><td>[[2, 5, 2], [4, 3], [3, 2, 2], [2, 2, 4], [2, 2, 5]]   </td><td style=\"text-align: right;\">  1</td><td style=\"text-align: right;\">0.000479242</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">     45112.6</td><td style=\"text-align: right;\">     46853.7</td><td style=\"text-align: right;\">        39517.6</td><td style=\"text-align: right;\">        40970.7</td><td style=\"text-align: right;\">       35672.1</td><td style=\"text-align: right;\">       39696.5</td><td style=\"text-align: right;\">       0.996909</td><td style=\"text-align: right;\">       0.997088</td><td style=\"text-align: right;\">    0.992551</td><td style=\"text-align: right;\">    0.993888</td><td style=\"text-align: right;\">      0.00365959</td><td style=\"text-align: right;\">      0.00334135</td></tr>\n",
       "<tr><td>train_cspd_raytune_cpu_gpu_distributed_e4721_00010</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">  0.0804786</td><td>[[4, 3], [2, 2, 3], [2, 2, 3], [4, 3], [2, 2, 3, 3, 2]]</td><td style=\"text-align: right;\">  2</td><td style=\"text-align: right;\">0.000155851</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">     48279  </td><td style=\"text-align: right;\">     45803  </td><td style=\"text-align: right;\">        42215.8</td><td style=\"text-align: right;\">        39858.5</td><td style=\"text-align: right;\">       42142.8</td><td style=\"text-align: right;\">       39974.9</td><td style=\"text-align: right;\">       0.996582</td><td style=\"text-align: right;\">       0.996423</td><td style=\"text-align: right;\">    0.992628</td><td style=\"text-align: right;\">    0.992841</td><td style=\"text-align: right;\">      0.00376995</td><td style=\"text-align: right;\">      0.00434385</td></tr>\n",
       "<tr><td>train_cspd_raytune_cpu_gpu_distributed_e4721_00011</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">  0.443408 </td><td>[[2, 2, 3], [4, 3], [2, 5], [2, 2, 3], [5, 2, 5]]      </td><td style=\"text-align: right;\">  2</td><td style=\"text-align: right;\">0.00679288 </td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">     29373.7</td><td style=\"text-align: right;\">     28302.6</td><td style=\"text-align: right;\">        24175.2</td><td style=\"text-align: right;\">        23217.6</td><td style=\"text-align: right;\">       27924.3</td><td style=\"text-align: right;\">       27653.4</td><td style=\"text-align: right;\">       0.575617</td><td style=\"text-align: right;\">       0.571426</td><td style=\"text-align: right;\">    0.987243</td><td style=\"text-align: right;\">    0.891843</td><td style=\"text-align: right;\">      0.339845  </td><td style=\"text-align: right;\">      0.320691  </td></tr>\n",
       "<tr><td>train_cspd_raytune_cpu_gpu_distributed_e4721_00012</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">  0.398188 </td><td>[[2, 3, 2], [4, 3], [3, 5], [5, 2, 2, 2], [2, 3, 2]]   </td><td style=\"text-align: right;\">  2</td><td style=\"text-align: right;\">0.0616605  </td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">     25422.2</td><td style=\"text-align: right;\">     23611.5</td><td style=\"text-align: right;\">        20839.6</td><td style=\"text-align: right;\">        20048.8</td><td style=\"text-align: right;\">       21931.8</td><td style=\"text-align: right;\">       19454.1</td><td style=\"text-align: right;\">       0.425907</td><td style=\"text-align: right;\">       0.45309 </td><td style=\"text-align: right;\">    1.35526 </td><td style=\"text-align: right;\">    1.19454 </td><td style=\"text-align: right;\">      0.438925  </td><td style=\"text-align: right;\">      0.408764  </td></tr>\n",
       "<tr><td>train_cspd_raytune_cpu_gpu_distributed_e4721_00013</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">  0.671978 </td><td>[[4, 3], [3, 4], [2, 4, 2], [4, 3, 4], [2, 5]]         </td><td style=\"text-align: right;\">  2</td><td style=\"text-align: right;\">0.000240075</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">     45551.3</td><td style=\"text-align: right;\">     45825.4</td><td style=\"text-align: right;\">        39528.1</td><td style=\"text-align: right;\">        39884.8</td><td style=\"text-align: right;\">       38791.9</td><td style=\"text-align: right;\">       39941.8</td><td style=\"text-align: right;\">       0.99632 </td><td style=\"text-align: right;\">       0.99718 </td><td style=\"text-align: right;\">    0.991792</td><td style=\"text-align: right;\">    0.994206</td><td style=\"text-align: right;\">      0.00486241</td><td style=\"text-align: right;\">      0.00361723</td></tr>\n",
       "<tr><td>train_cspd_raytune_cpu_gpu_distributed_e4721_00014</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">  0.67715  </td><td>[[2, 5, 2], [4, 3], [3, 2, 2], [2, 2, 4], [2, 2, 5]]   </td><td style=\"text-align: right;\">  2</td><td style=\"text-align: right;\">0.0357766  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">     25623.5</td><td style=\"text-align: right;\">     22944.7</td><td style=\"text-align: right;\">        21301.7</td><td style=\"text-align: right;\">        19561.5</td><td style=\"text-align: right;\">       20243.3</td><td style=\"text-align: right;\">       19380  </td><td style=\"text-align: right;\">       0.444155</td><td style=\"text-align: right;\">       0.434774</td><td style=\"text-align: right;\">    1.37844 </td><td style=\"text-align: right;\">    1.25858 </td><td style=\"text-align: right;\">      0.479303  </td><td style=\"text-align: right;\">      0.581273  </td></tr>\n",
       "<tr><td>train_cspd_raytune_cpu_gpu_distributed_e4721_00015</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">  0.22347  </td><td>[[4, 3], [2, 2, 3], [2, 2, 3], [4, 3], [2, 2, 3, 3, 2]]</td><td style=\"text-align: right;\">  3</td><td style=\"text-align: right;\">0.000580691</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">     46744.1</td><td style=\"text-align: right;\">     46117.9</td><td style=\"text-align: right;\">        40386.9</td><td style=\"text-align: right;\">        40403.7</td><td style=\"text-align: right;\">       41344.3</td><td style=\"text-align: right;\">       37306.8</td><td style=\"text-align: right;\">       0.993954</td><td style=\"text-align: right;\">       0.994345</td><td style=\"text-align: right;\">    0.986558</td><td style=\"text-align: right;\">    0.988556</td><td style=\"text-align: right;\">      0.00776691</td><td style=\"text-align: right;\">      0.00635076</td></tr>\n",
       "<tr><td>train_cspd_raytune_cpu_gpu_distributed_e4721_00016</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">  0.509723 </td><td>[[2, 2, 3], [4, 3], [2, 5], [2, 2, 3], [5, 2, 5]]      </td><td style=\"text-align: right;\">  3</td><td style=\"text-align: right;\">0.000477558</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">     46922  </td><td style=\"text-align: right;\">     45928.7</td><td style=\"text-align: right;\">        41086.6</td><td style=\"text-align: right;\">        40186.8</td><td style=\"text-align: right;\">       38356.4</td><td style=\"text-align: right;\">       37318  </td><td style=\"text-align: right;\">       0.988433</td><td style=\"text-align: right;\">       0.988966</td><td style=\"text-align: right;\">    0.976052</td><td style=\"text-align: right;\">    0.977064</td><td style=\"text-align: right;\">      0.0152479 </td><td style=\"text-align: right;\">      0.012891  </td></tr>\n",
       "<tr><td>train_cspd_raytune_cpu_gpu_distributed_e4721_00017</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">  0.428759 </td><td>[[2, 3, 2], [4, 3], [3, 5], [5, 2, 2, 2], [2, 3, 2]]   </td><td style=\"text-align: right;\">  3</td><td style=\"text-align: right;\">0.0730267  </td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">     26537.3</td><td style=\"text-align: right;\">     23689.4</td><td style=\"text-align: right;\">        21804.2</td><td style=\"text-align: right;\">        19906.1</td><td style=\"text-align: right;\">       24312.4</td><td style=\"text-align: right;\">       20262.6</td><td style=\"text-align: right;\">       0.466801</td><td style=\"text-align: right;\">       0.434986</td><td style=\"text-align: right;\">    1.58235 </td><td style=\"text-align: right;\">    1.19399 </td><td style=\"text-align: right;\">      0.882215  </td><td style=\"text-align: right;\">      0.378114  </td></tr>\n",
       "<tr><td>train_cspd_raytune_cpu_gpu_distributed_e4721_00018</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">  0.503177 </td><td>[[4, 3], [3, 4], [2, 4, 2], [4, 3, 4], [2, 5]]         </td><td style=\"text-align: right;\">  3</td><td style=\"text-align: right;\">0.000261208</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">     47179  </td><td style=\"text-align: right;\">     46174  </td><td style=\"text-align: right;\">        41657.8</td><td style=\"text-align: right;\">        40467  </td><td style=\"text-align: right;\">       34658.1</td><td style=\"text-align: right;\">       37310.2</td><td style=\"text-align: right;\">       0.995434</td><td style=\"text-align: right;\">       0.995968</td><td style=\"text-align: right;\">    0.99092 </td><td style=\"text-align: right;\">    0.991605</td><td style=\"text-align: right;\">      0.00496981</td><td style=\"text-align: right;\">      0.00464465</td></tr>\n",
       "<tr><td>train_cspd_raytune_cpu_gpu_distributed_e4721_00019</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">  0.0393316</td><td>[[2, 5, 2], [4, 3], [3, 2, 2], [2, 2, 4], [2, 2, 5]]   </td><td style=\"text-align: right;\">  3</td><td style=\"text-align: right;\">0.0019249  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">     46502.4</td><td style=\"text-align: right;\">     45586.1</td><td style=\"text-align: right;\">        40263  </td><td style=\"text-align: right;\">        39793.2</td><td style=\"text-align: right;\">       40085.5</td><td style=\"text-align: right;\">       37215.2</td><td style=\"text-align: right;\">       0.979658</td><td style=\"text-align: right;\">       0.979016</td><td style=\"text-align: right;\">    0.952343</td><td style=\"text-align: right;\">    0.956714</td><td style=\"text-align: right;\">      0.0258352 </td><td style=\"text-align: right;\">      0.0239415 </td></tr>\n",
       "<tr><td>train_cspd_raytune_cpu_gpu_distributed_e4721_00020</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">  0.426606 </td><td>[[4, 3], [2, 2, 3], [2, 2, 3], [4, 3], [2, 2, 3, 3, 2]]</td><td style=\"text-align: right;\">  4</td><td style=\"text-align: right;\">0.0318479  </td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">     23074.9</td><td style=\"text-align: right;\">     22942.1</td><td style=\"text-align: right;\">        19542.1</td><td style=\"text-align: right;\">        19692.7</td><td style=\"text-align: right;\">       20117.1</td><td style=\"text-align: right;\">       19559.4</td><td style=\"text-align: right;\">       0.415139</td><td style=\"text-align: right;\">       0.412584</td><td style=\"text-align: right;\">    1.2634  </td><td style=\"text-align: right;\">    1.35278 </td><td style=\"text-align: right;\">      0.553806  </td><td style=\"text-align: right;\">      0.519547  </td></tr>\n",
       "<tr><td>train_cspd_raytune_cpu_gpu_distributed_e4721_00021</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">  0.695532 </td><td>[[2, 2, 3], [4, 3], [2, 5], [2, 2, 3], [5, 2, 5]]      </td><td style=\"text-align: right;\">  4</td><td style=\"text-align: right;\">0.000146376</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">     46073.3</td><td style=\"text-align: right;\">     46189.9</td><td style=\"text-align: right;\">        40236.8</td><td style=\"text-align: right;\">        40452.9</td><td style=\"text-align: right;\">       37520.6</td><td style=\"text-align: right;\">       37813  </td><td style=\"text-align: right;\">       0.994185</td><td style=\"text-align: right;\">       0.995353</td><td style=\"text-align: right;\">    0.987294</td><td style=\"text-align: right;\">    0.990155</td><td style=\"text-align: right;\">      0.00704164</td><td style=\"text-align: right;\">      0.00567803</td></tr>\n",
       "<tr><td>train_cspd_raytune_cpu_gpu_distributed_e4721_00022</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">  0.118732 </td><td>[[2, 3, 2], [4, 3], [3, 5], [5, 2, 2, 2], [2, 3, 2]]   </td><td style=\"text-align: right;\">  4</td><td style=\"text-align: right;\">0.0551902  </td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">     29978.9</td><td style=\"text-align: right;\">     28683.7</td><td style=\"text-align: right;\">        23894.8</td><td style=\"text-align: right;\">        23688.9</td><td style=\"text-align: right;\">       32342.4</td><td style=\"text-align: right;\">       26007.9</td><td style=\"text-align: right;\">       0.607478</td><td style=\"text-align: right;\">       0.575244</td><td style=\"text-align: right;\">    1.04145 </td><td style=\"text-align: right;\">    0.919419</td><td style=\"text-align: right;\">      0.389396  </td><td style=\"text-align: right;\">      0.325833  </td></tr>\n",
       "<tr><td>train_cspd_raytune_cpu_gpu_distributed_e4721_00023</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">  0.250955 </td><td>[[4, 3], [3, 4], [2, 4, 2], [4, 3, 4], [2, 5]]         </td><td style=\"text-align: right;\">  4</td><td style=\"text-align: right;\">0.0126533  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">     27633.2</td><td style=\"text-align: right;\">     26028.4</td><td style=\"text-align: right;\">        23115.5</td><td style=\"text-align: right;\">        21789.7</td><td style=\"text-align: right;\">       24485.7</td><td style=\"text-align: right;\">       22138.9</td><td style=\"text-align: right;\">       0.544919</td><td style=\"text-align: right;\">       0.509293</td><td style=\"text-align: right;\">    1.04607 </td><td style=\"text-align: right;\">    1.07319 </td><td style=\"text-align: right;\">      0.340165  </td><td style=\"text-align: right;\">      0.318525  </td></tr>\n",
       "<tr><td>train_cspd_raytune_cpu_gpu_distributed_e4721_00024</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">  0.311023 </td><td>[[2, 5, 2], [4, 3], [3, 2, 2], [2, 2, 4], [2, 2, 5]]   </td><td style=\"text-align: right;\">  4</td><td style=\"text-align: right;\">0.000156316</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">     44407.7</td><td style=\"text-align: right;\">     46202  </td><td style=\"text-align: right;\">        38355.6</td><td style=\"text-align: right;\">        40466  </td><td style=\"text-align: right;\">       36746  </td><td style=\"text-align: right;\">       37816  </td><td style=\"text-align: right;\">       0.994921</td><td style=\"text-align: right;\">       0.995643</td><td style=\"text-align: right;\">    0.987989</td><td style=\"text-align: right;\">    0.990671</td><td style=\"text-align: right;\">      0.00657214</td><td style=\"text-align: right;\">      0.00522846</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-25 16:49:21,401\tINFO tune.py:561 -- Total run time: 259.66 seconds (258.95 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 269.33325386047363s\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "result = tune.run(\n",
    "    tune.with_parameters(train_cspd_raytune_cpu_gpu_distributed, \n",
    "                         num_in_feat   = N_FEATURE,\n",
    "                         num_branch    = N_SUBGROUP,\n",
    "                         num_epochs    = num_epochs, \n",
    "                         train_dataset = dataset_train_kfold, \n",
    "                         valid_dataset = dataset_valid_kfold,\n",
    "                         train_metric_samples = round(len(dataset_train_kfold[0])/10),\n",
    "                         ),\n",
    "    config = config,\n",
    "    resources_per_trial = {\"cpu\": 2 ,\"gpu\": 0.1},\n",
    "    num_samples = num_hp_search_samples,\n",
    "    local_dir = chkpt_dir,\n",
    "    max_failures = 5,\n",
    "    progress_reporter = reporter,\n",
    "    scheduler = scheduler,\n",
    "    search_alg = searchopt,\n",
    ")\n",
    "\n",
    "t1 = time.time()\n",
    "print(f\"Time elapsed: {t1-t0}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['train_rmse', 'valid_rmse', 'train_mean_l1', 'valid_mean_l1',\n",
       "       'train_l1_iqr', 'valid_l1_iqr', 'train_med-ape', 'valid_med-ape',\n",
       "       'train_mape', 'valid_mape', 'train_mape_iqr', 'valid_mape_iqr',\n",
       "       'time_this_iter_s', 'should_checkpoint', 'done', 'timesteps_total',\n",
       "       'episodes_total', 'training_iteration', 'experiment_id', 'date',\n",
       "       'timestamp', 'time_total_s', 'pid', 'hostname', 'node_ip',\n",
       "       'time_since_restore', 'timesteps_since_restore',\n",
       "       'iterations_since_restore', 'experiment_tag', 'lr', 'dropout_p', 'k',\n",
       "       'batch_size', 'h_branch', 'key'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_trials.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>h_branch</th>\n",
       "      <th>dropout_p</th>\n",
       "      <th>lr</th>\n",
       "      <th>valid_med-ape</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trial_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>e4721_00000</th>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>[[4, 3], [2, 2, 3], [2, 2, 3], [4, 3], [2, 2, ...</td>\n",
       "      <td>0.085391</td>\n",
       "      <td>0.001760</td>\n",
       "      <td>0.962158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e4721_00001</th>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>[[2, 2, 3], [4, 3], [2, 5], [2, 2, 3], [5, 2, 5]]</td>\n",
       "      <td>0.697423</td>\n",
       "      <td>0.003319</td>\n",
       "      <td>0.918149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e4721_00002</th>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>[[2, 3, 2], [4, 3], [3, 5], [5, 2, 2, 2], [2, ...</td>\n",
       "      <td>0.453953</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>0.997050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e4721_00003</th>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>[[4, 3], [3, 4], [2, 4, 2], [4, 3, 4], [2, 5]]</td>\n",
       "      <td>0.403146</td>\n",
       "      <td>0.034614</td>\n",
       "      <td>0.556996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e4721_00004</th>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>[[2, 5, 2], [4, 3], [3, 2, 2], [2, 2, 4], [2, ...</td>\n",
       "      <td>0.612245</td>\n",
       "      <td>0.006326</td>\n",
       "      <td>0.884017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e4721_00005</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>[[4, 3], [2, 2, 3], [2, 2, 3], [4, 3], [2, 2, ...</td>\n",
       "      <td>0.388295</td>\n",
       "      <td>0.050829</td>\n",
       "      <td>0.420032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e4721_00006</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>[[2, 2, 3], [4, 3], [2, 5], [2, 2, 3], [5, 2, 5]]</td>\n",
       "      <td>0.328781</td>\n",
       "      <td>0.001662</td>\n",
       "      <td>0.961793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e4721_00007</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>[[2, 3, 2], [4, 3], [3, 5], [5, 2, 2, 2], [2, ...</td>\n",
       "      <td>0.376204</td>\n",
       "      <td>0.038420</td>\n",
       "      <td>0.431487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e4721_00008</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>[[4, 3], [3, 4], [2, 4, 2], [4, 3, 4], [2, 5]]</td>\n",
       "      <td>0.459848</td>\n",
       "      <td>0.014126</td>\n",
       "      <td>0.952846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e4721_00009</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>[[2, 5, 2], [4, 3], [3, 2, 2], [2, 2, 4], [2, ...</td>\n",
       "      <td>0.259113</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>0.997088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e4721_00010</th>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>[[4, 3], [2, 2, 3], [2, 2, 3], [4, 3], [2, 2, ...</td>\n",
       "      <td>0.080479</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.996423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e4721_00011</th>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>[[2, 2, 3], [4, 3], [2, 5], [2, 2, 3], [5, 2, 5]]</td>\n",
       "      <td>0.443408</td>\n",
       "      <td>0.006793</td>\n",
       "      <td>0.571426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e4721_00012</th>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>[[2, 3, 2], [4, 3], [3, 5], [5, 2, 2, 2], [2, ...</td>\n",
       "      <td>0.398188</td>\n",
       "      <td>0.061660</td>\n",
       "      <td>0.453090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e4721_00013</th>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>[[4, 3], [3, 4], [2, 4, 2], [4, 3, 4], [2, 5]]</td>\n",
       "      <td>0.671978</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.997180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e4721_00014</th>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>[[2, 5, 2], [4, 3], [3, 2, 2], [2, 2, 4], [2, ...</td>\n",
       "      <td>0.677150</td>\n",
       "      <td>0.035777</td>\n",
       "      <td>0.434774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e4721_00015</th>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>[[4, 3], [2, 2, 3], [2, 2, 3], [4, 3], [2, 2, ...</td>\n",
       "      <td>0.223470</td>\n",
       "      <td>0.000581</td>\n",
       "      <td>0.994345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e4721_00016</th>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>[[2, 2, 3], [4, 3], [2, 5], [2, 2, 3], [5, 2, 5]]</td>\n",
       "      <td>0.509723</td>\n",
       "      <td>0.000478</td>\n",
       "      <td>0.988966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e4721_00017</th>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>[[2, 3, 2], [4, 3], [3, 5], [5, 2, 2, 2], [2, ...</td>\n",
       "      <td>0.428759</td>\n",
       "      <td>0.073027</td>\n",
       "      <td>0.434986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e4721_00018</th>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>[[4, 3], [3, 4], [2, 4, 2], [4, 3, 4], [2, 5]]</td>\n",
       "      <td>0.503177</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.995968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e4721_00019</th>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>[[2, 5, 2], [4, 3], [3, 2, 2], [2, 2, 4], [2, ...</td>\n",
       "      <td>0.039332</td>\n",
       "      <td>0.001925</td>\n",
       "      <td>0.979016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e4721_00020</th>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>[[4, 3], [2, 2, 3], [2, 2, 3], [4, 3], [2, 2, ...</td>\n",
       "      <td>0.426606</td>\n",
       "      <td>0.031848</td>\n",
       "      <td>0.412584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e4721_00021</th>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>[[2, 2, 3], [4, 3], [2, 5], [2, 2, 3], [5, 2, 5]]</td>\n",
       "      <td>0.695532</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.995353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e4721_00022</th>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>[[2, 3, 2], [4, 3], [3, 5], [5, 2, 2, 2], [2, ...</td>\n",
       "      <td>0.118732</td>\n",
       "      <td>0.055190</td>\n",
       "      <td>0.575244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e4721_00023</th>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>[[4, 3], [3, 4], [2, 4, 2], [4, 3, 4], [2, 5]]</td>\n",
       "      <td>0.250955</td>\n",
       "      <td>0.012653</td>\n",
       "      <td>0.509293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e4721_00024</th>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>[[2, 5, 2], [4, 3], [3, 2, 2], [2, 2, 4], [2, ...</td>\n",
       "      <td>0.311023</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.995643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             k  batch_size                                           h_branch  \\\n",
       "trial_id                                                                        \n",
       "e4721_00000  0          64  [[4, 3], [2, 2, 3], [2, 2, 3], [4, 3], [2, 2, ...   \n",
       "e4721_00001  0          64  [[2, 2, 3], [4, 3], [2, 5], [2, 2, 3], [5, 2, 5]]   \n",
       "e4721_00002  0          64  [[2, 3, 2], [4, 3], [3, 5], [5, 2, 2, 2], [2, ...   \n",
       "e4721_00003  0          64     [[4, 3], [3, 4], [2, 4, 2], [4, 3, 4], [2, 5]]   \n",
       "e4721_00004  0          64  [[2, 5, 2], [4, 3], [3, 2, 2], [2, 2, 4], [2, ...   \n",
       "e4721_00005  1          64  [[4, 3], [2, 2, 3], [2, 2, 3], [4, 3], [2, 2, ...   \n",
       "e4721_00006  1          64  [[2, 2, 3], [4, 3], [2, 5], [2, 2, 3], [5, 2, 5]]   \n",
       "e4721_00007  1          64  [[2, 3, 2], [4, 3], [3, 5], [5, 2, 2, 2], [2, ...   \n",
       "e4721_00008  1          64     [[4, 3], [3, 4], [2, 4, 2], [4, 3, 4], [2, 5]]   \n",
       "e4721_00009  1          64  [[2, 5, 2], [4, 3], [3, 2, 2], [2, 2, 4], [2, ...   \n",
       "e4721_00010  2          64  [[4, 3], [2, 2, 3], [2, 2, 3], [4, 3], [2, 2, ...   \n",
       "e4721_00011  2          64  [[2, 2, 3], [4, 3], [2, 5], [2, 2, 3], [5, 2, 5]]   \n",
       "e4721_00012  2          64  [[2, 3, 2], [4, 3], [3, 5], [5, 2, 2, 2], [2, ...   \n",
       "e4721_00013  2          64     [[4, 3], [3, 4], [2, 4, 2], [4, 3, 4], [2, 5]]   \n",
       "e4721_00014  2          64  [[2, 5, 2], [4, 3], [3, 2, 2], [2, 2, 4], [2, ...   \n",
       "e4721_00015  3          64  [[4, 3], [2, 2, 3], [2, 2, 3], [4, 3], [2, 2, ...   \n",
       "e4721_00016  3          64  [[2, 2, 3], [4, 3], [2, 5], [2, 2, 3], [5, 2, 5]]   \n",
       "e4721_00017  3          64  [[2, 3, 2], [4, 3], [3, 5], [5, 2, 2, 2], [2, ...   \n",
       "e4721_00018  3          64     [[4, 3], [3, 4], [2, 4, 2], [4, 3, 4], [2, 5]]   \n",
       "e4721_00019  3          64  [[2, 5, 2], [4, 3], [3, 2, 2], [2, 2, 4], [2, ...   \n",
       "e4721_00020  4          64  [[4, 3], [2, 2, 3], [2, 2, 3], [4, 3], [2, 2, ...   \n",
       "e4721_00021  4          64  [[2, 2, 3], [4, 3], [2, 5], [2, 2, 3], [5, 2, 5]]   \n",
       "e4721_00022  4          64  [[2, 3, 2], [4, 3], [3, 5], [5, 2, 2, 2], [2, ...   \n",
       "e4721_00023  4          64     [[4, 3], [3, 4], [2, 4, 2], [4, 3, 4], [2, 5]]   \n",
       "e4721_00024  4          64  [[2, 5, 2], [4, 3], [3, 2, 2], [2, 2, 4], [2, ...   \n",
       "\n",
       "             dropout_p        lr  valid_med-ape  \n",
       "trial_id                                         \n",
       "e4721_00000   0.085391  0.001760       0.962158  \n",
       "e4721_00001   0.697423  0.003319       0.918149  \n",
       "e4721_00002   0.453953  0.000382       0.997050  \n",
       "e4721_00003   0.403146  0.034614       0.556996  \n",
       "e4721_00004   0.612245  0.006326       0.884017  \n",
       "e4721_00005   0.388295  0.050829       0.420032  \n",
       "e4721_00006   0.328781  0.001662       0.961793  \n",
       "e4721_00007   0.376204  0.038420       0.431487  \n",
       "e4721_00008   0.459848  0.014126       0.952846  \n",
       "e4721_00009   0.259113  0.000479       0.997088  \n",
       "e4721_00010   0.080479  0.000156       0.996423  \n",
       "e4721_00011   0.443408  0.006793       0.571426  \n",
       "e4721_00012   0.398188  0.061660       0.453090  \n",
       "e4721_00013   0.671978  0.000240       0.997180  \n",
       "e4721_00014   0.677150  0.035777       0.434774  \n",
       "e4721_00015   0.223470  0.000581       0.994345  \n",
       "e4721_00016   0.509723  0.000478       0.988966  \n",
       "e4721_00017   0.428759  0.073027       0.434986  \n",
       "e4721_00018   0.503177  0.000261       0.995968  \n",
       "e4721_00019   0.039332  0.001925       0.979016  \n",
       "e4721_00020   0.426606  0.031848       0.412584  \n",
       "e4721_00021   0.695532  0.000146       0.995353  \n",
       "e4721_00022   0.118732  0.055190       0.575244  \n",
       "e4721_00023   0.250955  0.012653       0.509293  \n",
       "e4721_00024   0.311023  0.000156       0.995643  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_trial = result.get_best_trial(\"valid_med-ape\", \"min\", \"last\")\n",
    "\n",
    "all_trials = result.results_df\n",
    "all_trials.columns = all_trials.columns.str.replace('config.', '', regex=False).tolist()\n",
    "join_nested(all_trials, architecture_table, on='h_branch')\n",
    "all_trials[['k','batch_size','h_branch','dropout_p','lr','valid_med-ape']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial config: {'lr': 0.03184793123042965, 'dropout_p': 0.4266056926273974, 'k': 4, 'batch_size': 64, 'h_branch': [[4, 3], [2, 2, 3], [2, 2, 3], [4, 3], [2, 2, 3, 3, 2]]}\n",
      "Best trial final validation loss: 0.4125843968327914\n"
     ]
    }
   ],
   "source": [
    "print(\"Best trial config: {}\".format(best_trial.config))\n",
    "print(\"Best trial final validation loss: {}\".format(best_trial.last_result[\"valid_med-ape\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning (Ray Tune using Auto Network Architecture Tunning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: Ray Tune does not yet support same sampling for all set of Random+Grid Random Search with same set of Grid Parameters.  Therefore, this is not suitable for K-Fold fair comparision yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_metrics = [\"training_iteration\",\n",
    "                  \"train_rmse\", \n",
    "                  \"valid_rmse\",\n",
    "                  \"train_mean_l1\", \n",
    "                  \"valid_mean_l1\",\n",
    "                  \"train_l1_iqr\",\n",
    "                  \"valid_l1_iqr\",\n",
    "                  \"train_med-ape\",\n",
    "                  \"valid_med-ape\",\n",
    "                  \"train_mape\", \n",
    "                  \"valid_mape\",\n",
    "                  \"train_mape_iqr\", \n",
    "                  \"valid_mape_iqr\",\n",
    "                 ]\n",
    "\n",
    "reporter = tune.JupyterNotebookReporter(overwrite=True, max_progress_rows=35, metric_columns= report_metrics)\n",
    "scheduler = HyperBandScheduler(metric=\"valid_mape\", mode=\"min\", max_t=num_epochs)\n",
    "searchopt = BasicVariantGenerator(max_concurrent=15)\n",
    "\n",
    "config = {\"lr\": tune.loguniform(lr_min, lr_max),                       # Learning Rate\n",
    "          \"dropout_p\": tune.uniform(dropout_p_min, dropout_p_max),     # Dropout On/Off\n",
    "          \"k\": tune.grid_search([*range(k)]),                          # K-Fold Index\n",
    "          \"batch_size\": tune.choice(batch_size),                       # 1: SGD; 2+: Zero-Filled BGD\n",
    "          \"h_total\": tune.choice([*range(h_total_min, h_total_max, h_total_step)]),\n",
    "          \"h_subgroup\": tune.sample_from(lambda spec: split_sampling(num_ele = spec.config.h_total, \n",
    "                                                                    num_layers = N_SUBGROUP,\n",
    "                                                                    n_min = h_subgroup_min_neuron_per_sgrp,\n",
    "                                                                    n_max = h_subgroup_max_neuron_per_sgrp,\n",
    "                                                                    single_sample = True)),\n",
    "          \"h_branch\": tune.sample_from(lambda spec: [ split_sampling(num_ele = h_sgrp_ele, \n",
    "                                                                     num_layers = h_branch_max_layer,\n",
    "                                                                     n_min = h_branch_min_neuron_per_layer,\n",
    "                                                                     n_max = h_branch_max_neuron_per_layer,\n",
    "                                                                     single_sample = True) for h_sgrp_ele in spec.config.h_subgroup ]),\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 5.2/59.9 GiB<br>Using HyperBand: num_stopped=2 total_brackets=2\n",
       "Round #0:\n",
       "  Bracket(Max Size (n)=2, Milestone (r)=5, completed=100.0%): {TERMINATED: 2} \n",
       "  Bracket(Max Size (n)=1, Milestone (r)=3, completed=100.0%): {TERMINATED: 3} <br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/34.68 GiB heap, 0.0/17.34 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /home/calvin_chan/data/output/checkpoint/testing/train_cspd_raytune_cpu_gpu_distributed_2022-03-25_16-57-18<br>Number of trials: 5/5 (5 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                                        </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  dropout_p</th><th>h_branch                                                  </th><th>h_subgroup       </th><th style=\"text-align: right;\">  h_total</th><th style=\"text-align: right;\">  k</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">  training_iteration</th><th style=\"text-align: right;\">  train_rmse</th><th style=\"text-align: right;\">  valid_rmse</th><th style=\"text-align: right;\">  train_mean_l1</th><th style=\"text-align: right;\">  valid_mean_l1</th><th style=\"text-align: right;\">  train_l1_iqr</th><th style=\"text-align: right;\">  valid_l1_iqr</th><th style=\"text-align: right;\">  train_med-ape</th><th style=\"text-align: right;\">  valid_med-ape</th><th style=\"text-align: right;\">  train_mape</th><th style=\"text-align: right;\">  valid_mape</th><th style=\"text-align: right;\">  train_mape_iqr</th><th style=\"text-align: right;\">  valid_mape_iqr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_cspd_raytune_cpu_gpu_distributed_9d03e_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">   0.201241</td><td>[[2, 3, 3, 2], [2, 5], [5, 3], [4, 2, 2, 2, 2], [2, 3, 3]]</td><td>[10, 7, 8, 12, 8]</td><td style=\"text-align: right;\">       45</td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">0.0208903 </td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">     23584.4</td><td style=\"text-align: right;\">     23914.7</td><td style=\"text-align: right;\">        19694.2</td><td style=\"text-align: right;\">        20299  </td><td style=\"text-align: right;\">       21419  </td><td style=\"text-align: right;\">       20809.1</td><td style=\"text-align: right;\">       0.434326</td><td style=\"text-align: right;\">       0.415087</td><td style=\"text-align: right;\">    1.59344 </td><td style=\"text-align: right;\">    1.55312 </td><td style=\"text-align: right;\">      0.980396  </td><td style=\"text-align: right;\">      0.524093  </td></tr>\n",
       "<tr><td>train_cspd_raytune_cpu_gpu_distributed_9d03e_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">   0.486102</td><td>[[5, 3], [3, 2, 3], [3, 2, 3], [3, 4, 2], [2, 5]]         </td><td>[8, 8, 8, 9, 7]  </td><td style=\"text-align: right;\">       40</td><td style=\"text-align: right;\">  1</td><td style=\"text-align: right;\">0.00200156</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">     43392.3</td><td style=\"text-align: right;\">     45486  </td><td style=\"text-align: right;\">        37211.7</td><td style=\"text-align: right;\">        39404.7</td><td style=\"text-align: right;\">       37562.6</td><td style=\"text-align: right;\">       39649.8</td><td style=\"text-align: right;\">       0.955967</td><td style=\"text-align: right;\">       0.9574  </td><td style=\"text-align: right;\">    0.904384</td><td style=\"text-align: right;\">    0.915648</td><td style=\"text-align: right;\">      0.0507016 </td><td style=\"text-align: right;\">      0.0492652 </td></tr>\n",
       "<tr><td>train_cspd_raytune_cpu_gpu_distributed_9d03e_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">   0.302968</td><td>[[5, 3, 2, 2], [3, 5], [3, 2, 2, 2], [4, 3], [2, 2, 5]]   </td><td>[12, 8, 9, 7, 9] </td><td style=\"text-align: right;\">       45</td><td style=\"text-align: right;\">  2</td><td style=\"text-align: right;\">0.00607891</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">     45329  </td><td style=\"text-align: right;\">     45358.5</td><td style=\"text-align: right;\">        39151.5</td><td style=\"text-align: right;\">        39348.1</td><td style=\"text-align: right;\">       39064.5</td><td style=\"text-align: right;\">       39932.4</td><td style=\"text-align: right;\">       0.984127</td><td style=\"text-align: right;\">       0.983779</td><td style=\"text-align: right;\">    0.96545 </td><td style=\"text-align: right;\">    0.966027</td><td style=\"text-align: right;\">      0.0234205 </td><td style=\"text-align: right;\">      0.0211702 </td></tr>\n",
       "<tr><td>train_cspd_raytune_cpu_gpu_distributed_9d03e_00003</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">   0.190956</td><td>[[4, 4], [2, 3, 2, 2], [3, 2, 2], [2, 3, 2, 2], [2, 5]]   </td><td>[8, 9, 7, 9, 7]  </td><td style=\"text-align: right;\">       40</td><td style=\"text-align: right;\">  3</td><td style=\"text-align: right;\">0.00906328</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">     40002.8</td><td style=\"text-align: right;\">     39058.6</td><td style=\"text-align: right;\">        33800  </td><td style=\"text-align: right;\">        32678.8</td><td style=\"text-align: right;\">       37461.2</td><td style=\"text-align: right;\">       37264.9</td><td style=\"text-align: right;\">       0.816422</td><td style=\"text-align: right;\">       0.795858</td><td style=\"text-align: right;\">    0.820393</td><td style=\"text-align: right;\">    0.800123</td><td style=\"text-align: right;\">      0.183966  </td><td style=\"text-align: right;\">      0.190475  </td></tr>\n",
       "<tr><td>train_cspd_raytune_cpu_gpu_distributed_9d03e_00004</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">   0.431493</td><td>[[2, 4, 2], [2, 2, 3], [2, 2, 3, 3], [3, 4, 2], [3, 5, 3]]</td><td>[8, 7, 10, 9, 11]</td><td style=\"text-align: right;\">       45</td><td style=\"text-align: right;\">  4</td><td style=\"text-align: right;\">0.00295892</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">     47423.3</td><td style=\"text-align: right;\">     46102.1</td><td style=\"text-align: right;\">        41672.5</td><td style=\"text-align: right;\">        40352.4</td><td style=\"text-align: right;\">       38152.6</td><td style=\"text-align: right;\">       37861.1</td><td style=\"text-align: right;\">       0.992448</td><td style=\"text-align: right;\">       0.992815</td><td style=\"text-align: right;\">    0.983752</td><td style=\"text-align: right;\">    0.984764</td><td style=\"text-align: right;\">      0.00956165</td><td style=\"text-align: right;\">      0.00853514</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-25 16:58:20,983\tINFO tune.py:561 -- Total run time: 62.04 seconds (60.75 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 69.64378261566162s\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "result = tune.run(\n",
    "    tune.with_parameters(train_cspd_raytune_cpu_gpu_distributed, \n",
    "                         num_in_feat   = N_FEATURE,\n",
    "                         num_branch    = N_SUBGROUP,\n",
    "                         num_epochs    = num_epochs, \n",
    "                         train_dataset = dataset_train_kfold, \n",
    "                         valid_dataset = dataset_valid_kfold,\n",
    "                         train_metric_samples = round(len(dataset_train_kfold[0])/10),\n",
    "                         ),\n",
    "    config = config,\n",
    "    resources_per_trial={\"cpu\": 1},\n",
    "    num_samples = num_hp_search_samples,\n",
    "    local_dir = chkpt_dir,\n",
    "    progress_reporter = reporter,\n",
    "    scheduler = scheduler,\n",
    "    search_alg = searchopt,\n",
    ")\n",
    "\n",
    "t1 = time.time()\n",
    "print(f\"Time elapsed: {t1-t0}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>h_branch</th>\n",
       "      <th>h_subgroup</th>\n",
       "      <th>h_total</th>\n",
       "      <th>dropout_p</th>\n",
       "      <th>lr</th>\n",
       "      <th>valid_med-ape</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trial_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9d03e_00000</th>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>[[2, 3, 3, 2], [2, 5], [5, 3], [4, 2, 2, 2, 2]...</td>\n",
       "      <td>[10, 7, 8, 12, 8]</td>\n",
       "      <td>45</td>\n",
       "      <td>0.201241</td>\n",
       "      <td>0.020890</td>\n",
       "      <td>0.415087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9d03e_00001</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>[[5, 3], [3, 2, 3], [3, 2, 3], [3, 4, 2], [2, 5]]</td>\n",
       "      <td>[8, 8, 8, 9, 7]</td>\n",
       "      <td>40</td>\n",
       "      <td>0.486102</td>\n",
       "      <td>0.002002</td>\n",
       "      <td>0.957400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9d03e_00002</th>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>[[5, 3, 2, 2], [3, 5], [3, 2, 2, 2], [4, 3], [...</td>\n",
       "      <td>[12, 8, 9, 7, 9]</td>\n",
       "      <td>45</td>\n",
       "      <td>0.302968</td>\n",
       "      <td>0.006079</td>\n",
       "      <td>0.983779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9d03e_00003</th>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>[[4, 4], [2, 3, 2, 2], [3, 2, 2], [2, 3, 2, 2]...</td>\n",
       "      <td>[8, 9, 7, 9, 7]</td>\n",
       "      <td>40</td>\n",
       "      <td>0.190956</td>\n",
       "      <td>0.009063</td>\n",
       "      <td>0.795858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9d03e_00004</th>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>[[2, 4, 2], [2, 2, 3], [2, 2, 3, 3], [3, 4, 2]...</td>\n",
       "      <td>[8, 7, 10, 9, 11]</td>\n",
       "      <td>45</td>\n",
       "      <td>0.431493</td>\n",
       "      <td>0.002959</td>\n",
       "      <td>0.992815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             k  batch_size                                           h_branch  \\\n",
       "trial_id                                                                        \n",
       "9d03e_00000  0          64  [[2, 3, 3, 2], [2, 5], [5, 3], [4, 2, 2, 2, 2]...   \n",
       "9d03e_00001  1          64  [[5, 3], [3, 2, 3], [3, 2, 3], [3, 4, 2], [2, 5]]   \n",
       "9d03e_00002  2          64  [[5, 3, 2, 2], [3, 5], [3, 2, 2, 2], [4, 3], [...   \n",
       "9d03e_00003  3          64  [[4, 4], [2, 3, 2, 2], [3, 2, 2], [2, 3, 2, 2]...   \n",
       "9d03e_00004  4          64  [[2, 4, 2], [2, 2, 3], [2, 2, 3, 3], [3, 4, 2]...   \n",
       "\n",
       "                    h_subgroup  h_total  dropout_p        lr  valid_med-ape  \n",
       "trial_id                                                                     \n",
       "9d03e_00000  [10, 7, 8, 12, 8]       45   0.201241  0.020890       0.415087  \n",
       "9d03e_00001    [8, 8, 8, 9, 7]       40   0.486102  0.002002       0.957400  \n",
       "9d03e_00002   [12, 8, 9, 7, 9]       45   0.302968  0.006079       0.983779  \n",
       "9d03e_00003    [8, 9, 7, 9, 7]       40   0.190956  0.009063       0.795858  \n",
       "9d03e_00004  [8, 7, 10, 9, 11]       45   0.431493  0.002959       0.992815  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_trial = result.get_best_trial(\"valid_med-ape\", \"min\", \"last\")\n",
    "\n",
    "all_trials = result.results_df\n",
    "all_trials.columns = all_trials.columns.str.replace('config.', '', regex=False).tolist()\n",
    "all_trials[['k','batch_size','h_branch','h_subgroup','h_total','dropout_p','lr','valid_med-ape']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial config: {'lr': 0.020890300100544615, 'dropout_p': 0.20124093472181864, 'k': 0, 'batch_size': 64, 'h_total': 45, 'h_subgroup': [10, 7, 8, 12, 8], 'h_branch': [[2, 3, 3, 2], [2, 5], [5, 3], [4, 2, 2, 2, 2], [2, 3, 3]]}\n",
      "Best trial final validation loss: 0.4150868202638076\n"
     ]
    }
   ],
   "source": [
    "print(\"Best trial config: {}\".format(best_trial.config))\n",
    "print(\"Best trial final validation loss: {}\".format(best_trial.last_result[\"valid_med-ape\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling (No Ray Tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "arch_ind = 2\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "architecture = architecture_table['h_branch'][arch_ind]\n",
    "model = cspd(N_FEATURE,N_SUBGROUP,architecture,dropout_p=0.5).to(device)\n",
    "model.apply(initialize_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.1)\n",
    "criterion = nn.MSELoss()\n",
    "metric_dict = {'rmse': lambda y_est,y: torch.sqrt(nn.MSELoss(reduction=\"mean\")(y_est,y)), \n",
    "               'mape': lambda y_est,y: torch.mean((y-y_est).abs()/y.abs()),\n",
    "               'l1_iqr': lambda y_est,y: compute_iqr(nn.L1Loss(reduction=\"none\")(y_est,y))}\n",
    "\n",
    "\n",
    "training_results = train_cspd(model=model, \n",
    "                              train_dataset=dataset_model, \n",
    "                              valid_dataset=dataset_test, \n",
    "                              criterion=criterion,\n",
    "                              optimizer=optimizer,\n",
    "                              metric_dict=metric_dict,\n",
    "                              epochs=num_epochs, \n",
    "                              batch_size=64)\n",
    "\n",
    "t1 = time.time()\n",
    "print(f\"Time elapsed: {t1-t0}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_p39]",
   "language": "python",
   "name": "conda-env-pytorch_p39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordinal Hyperplane Loss Classifier - OHPL-All\n",
    "- Re-implementation in Pytorch based Tensorflow version on https://github.com/ohpl/ohpl\n",
    "- Algorithm: Bob Vanderheyden, Ying Xie, Mohan Rachumallu 2019 IEEE International Conference on Big Data\n",
    "\n",
    "Copyright (C) Bayer Pharmaceutical - All Rights Reserved\n",
    "\n",
    "Unauthorized copying of this file, via any medium is strictly prohibited\n",
    "Proprietary and confidential\n",
    "Written by Calvin W.Y. Chan calvin.chan@bayer.com, August 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "import torch.nn.functional as F\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import HyperBandScheduler\n",
    "from ray.tune.suggest.basic_variant import BasicVariantGenerator\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "from ray.tune.suggest.bohb import TuneBOHB\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import warnings\n",
    "import filelock\n",
    "\n",
    "import string\n",
    "import time\n",
    "import random\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environmental Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sys.version_info(major=3, minor=9, micro=5, releaselevel='final', serial=0)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.version_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environmental Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Handling Parameters\n",
    "test_split_ratio = 0.2\n",
    "k = 5\n",
    "random_state = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "num_epochs = 100\n",
    "\n",
    "# Learning Algorithm Parameters\n",
    "lr_min = 1e-4\n",
    "lr_max = 1e-1\n",
    "# batch_size = [1,16,32,64]\n",
    "batch_size = [5]\n",
    "\n",
    "# Architecture Sampling Parameters\n",
    "h_total_min = 4\n",
    "h_total_max = 10\n",
    "h_total_step = 2\n",
    "\n",
    "h_fc_min_neuron_per_layer = 2\n",
    "h_fc_max_neuron_per_layer = 5\n",
    "h_fc_max_layer = None\n",
    "\n",
    "dropout_p_min = 0\n",
    "dropout_p_max = 0.7\n",
    "\n",
    "# OHPL Parameters\n",
    "margin_min = 0.5\n",
    "margin_max = 1.0\n",
    "alpha_min  = 0.5\n",
    "alpha_max  = 1.0\n",
    "\n",
    "# Ray Tune Hyperparameter Search\n",
    "num_hp_search_samples = 1\n",
    "chkpt_dir = \"/home/calvin_chan/data/output/testing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup output directories\n",
    "if not os.path.exists(chkpt_dir):\n",
    "    os.makedirs(chkpt_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/home/calvin_chan/package/ohpl/design/ohpl-master/world_happiness_2015_2019.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(filepath)\n",
    "data.Score = data.Score.astype('int32')\n",
    "data.drop(['Year'], axis=1, inplace=True)\n",
    "data = data.dropna()\n",
    "data.columns = data.columns.str.replace(' ','_').str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_col = ['gdp_per_capita', \n",
    "         'social_support', \n",
    "         'healthy_life_expectancy',\n",
    "         'freedom_to_make_life_choices', \n",
    "         'generosity',\n",
    "         'perceptions_of_corruption']\n",
    "y_col = ['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[x_col]\n",
    "y = data[y_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_label = min(y.values.tolist())[0]\n",
    "max_label = max(y.values.tolist())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FEATURE = len(x_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerFC(nn.Module):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, in_feat, layers, dropout_p=None, act_fn=torch.relu, dtype=torch.double):\n",
    "        super(MultiLayerFC, self).__init__()\n",
    "        layers = [in_feat] + layers   # Add input layer\n",
    "        self.hidden = nn.ModuleList()\n",
    "        self.out = nn.Linear(layers[-1], 1).type(dtype)\n",
    "        self.act_fn = act_fn\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "        # --- Scalable Layers ---\n",
    "        for input_size, output_size in zip(layers, layers[1:]):\n",
    "            self.hidden.append(nn.Linear(input_size,output_size).type(dtype))\n",
    "            \n",
    "    # Prediction\n",
    "    def forward(self, x):\n",
    "        L = len(self.hidden)\n",
    "        for (l, single_layer) in zip(range(L), self.hidden):\n",
    "            x = single_layer(x)\n",
    "            x = self.dropout(self.act_fn(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight.data)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Handling and Hyperparameter Tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling-Testing 80/20 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_multidimensional_labels(df,col):\n",
    "    '''\n",
    "    Convert Multiple Column Label into Single Column\n",
    "    \n",
    "    Args:\n",
    "        df: A pandas dataframe with row as samples, and column as N-dimensional segment to be encoded.\n",
    "        col: Column name of the combined column\n",
    "        \n",
    "    Returns:\n",
    "        df: A pandas dataframe with new label column\n",
    "\n",
    "    Raises:\n",
    "        -\n",
    "\n",
    "    Author:\n",
    "        Dr. Calvin Chan\n",
    "        calvin.chan@bayer.com\n",
    "    '''\n",
    "    if df.shape[1] == 1:\n",
    "        df = pd.concat([df,df],axis=1)\n",
    "        df.columns = [df.columns[0],col]\n",
    "    else:\n",
    "        df[col] = tuple(labels.values.tolist())\n",
    "        df[col] = labels[col].apply(lambda x: ','.join([str(c) for c in x ]))\n",
    "    return(df)\n",
    "\n",
    "def combine_multidimensional_ohe(s):\n",
    "    '''\n",
    "    One-Hot-Encoding (OHE) based on joint label of multiple columns\n",
    "    The default OHE feature of Pandas and sklearn takes each column as independent OHE. \n",
    "    This function uses the 2D unique label combination as a single dimension for OHE.\n",
    "    \n",
    "    Args:\n",
    "        s: A pandas dataframe with row as samples, and column as N-dimensional segment to be encoded.\n",
    "\n",
    "    Returns:\n",
    "        s_ohe: A pandas dataframe with N-D OHE\n",
    "        conversion_table: The conversion table for N-D OHE\n",
    "\n",
    "    Raises:\n",
    "        -\n",
    "\n",
    "    Author:\n",
    "        Dr. Calvin Chan\n",
    "        calvin.chan@bayer.com\n",
    "    '''\n",
    "    unique_labels = [ sorted(s[name].unique().tolist()) for name in s.columns.tolist() ]\n",
    "    multidimensional_labels = [*itertools.product(*unique_labels)]\n",
    "    labels = pd.DataFrame(multidimensional_labels, columns=s.columns.tolist())\n",
    "    labels = convert_multidimensional_labels(labels,'seg')\n",
    "    conversion_table = pd.get_dummies(labels, columns=['seg'])\n",
    "    s_ohe = pd.merge(s,conversion_table,on=s.columns.tolist(),how='left').drop(s.columns.tolist(),axis=1)\n",
    "    return(s_ohe,conversion_table)\n",
    "\n",
    "\n",
    "def unique_list(ls_of_ls):\n",
    "    '''\n",
    "    Return the unique list in a list of list\n",
    "    \n",
    "    Args:\n",
    "        ls_of_ls: List of list (eg. [[1,2,3],[1,3,2],[1,2,3]])\n",
    "\n",
    "    Returns:\n",
    "        unique_ls: Unique list within the input list (eg. [[1,3,2],[1,2,3]])\n",
    "\n",
    "    Author:\n",
    "        Dr. Calvin Chan\n",
    "        calvin.chan@bayer.com\n",
    "    '''\n",
    "    unique_ls = [list(ls_out) for ls_out in set(tuple(ls) for ls in ls_of_ls)]\n",
    "    return unique_ls\n",
    "\n",
    "\n",
    "def model_test_split(*args, id_col=None, test_ratio=0.2, random_state=25, report_id=False):\n",
    "    \n",
    "    '''\n",
    "    Split the dataset into modeling and test set\n",
    "    \n",
    "    This function is to encapsulate the variying input feature size given the grouping by id_col,\n",
    "    and this decompose the one-hot-encoding column into a separate feature set to be used in the\n",
    "    deep learning model as separate input.\n",
    "    \n",
    "    Args:\n",
    "        *args:\n",
    "            x: A pandas dataframe with row as samples, and column as ID and feature type\n",
    "            y: A pandas dataframe with row as samples, and column as output\n",
    "        ohe_col: A list of column names indicating the one-hot-encoding columns in x\n",
    "        id_col: Column name of the grouping column to be converted to one-hot-encoding\n",
    "        test_size: The split ratio of the test set\n",
    "        random_state: Random seed use by the `sklearn.model_selection.train_test_split` function\n",
    "        retain_df: If this is 'True' and the input 'args' are dataframes, do not convert them to list of single row dataframe\n",
    "\n",
    "    Returns:\n",
    "        x_model, x_test: List of numpy matrix as model/test data split with from commond id_col labels of x and y\n",
    "        s_model, s_test: List of numpy matrix as model/test data split with from commond id_col labels of x and y\n",
    "        y_model, y_test: List of numpy matrix as model/test data split with from commond id_col labels of x and y\n",
    "\n",
    "    Raises:\n",
    "        Warning when the labels in id_col of x and y do not match\n",
    "        \n",
    "    Author:\n",
    "        Dr. Calvin Chan\n",
    "        calvin.chan@bayer.com\n",
    "    '''\n",
    "\n",
    "    if id_col is not None:\n",
    "\n",
    "        inds = []\n",
    "        data = []\n",
    "        for arg in args:\n",
    "            (ind,dat) = zip(*list(arg.groupby(id_col)))\n",
    "            inds.append(ind)\n",
    "            data.append(dat)\n",
    "\n",
    "        # Determine of ID entry is missing from any of the input dataset\n",
    "        id_match_flag = (len(set.intersection(*[set(ind) for ind in inds])) == len(set.union(*[set(ind) for ind in inds])))\n",
    "        if not id_match_flag:\n",
    "            warnings.warn(\"Unmatch ID entries in one or more data inputs (eg. x, y)!\")\n",
    "\n",
    "        # Extract Common ID from x, s, y Samples\n",
    "        select_ids = list(set.intersection(*[set(ind) for ind in inds]))\n",
    "\n",
    "        # Split dataframes into sample list\n",
    "        # (multi-resolution: each list element contains multiple x and single y based on id_col)\n",
    "        dataset = []\n",
    "        for i, dat in enumerate(data):\n",
    "            dataset.append([ dat[inds[i].index(single_id)].drop(id_col,axis=1) for single_id in select_ids ])\n",
    "\n",
    "    else:\n",
    "        # Determine index labels in each input dataset is the same\n",
    "        dataset_indices = [ list(dataset.index) for dataset in args ]\n",
    "        select_ids = unique_list(dataset_indices)\n",
    "        id_match_flag = (len(select_ids) == 1)\n",
    "        assert id_match_flag, \"Unmatch length in one or more data inputs (eg. x, y)!\"\n",
    "        select_ids = select_ids[0]\n",
    "        \n",
    "        # Split dataframes into sample list\n",
    "        # (equal resolution: each list element contains one row in both x and y)\n",
    "        dataset = []\n",
    "        for i, dat in enumerate(args):\n",
    "            dataset.append([ dat.loc[[single_id]] for single_id in select_ids ])\n",
    "\n",
    "    # Including index as one of the splitting dataset\n",
    "    assert all(test_ratio*len(data) >= 1 for data in dataset), \"Number of samples resulting from ratio must be larger than 1 sample!\"\n",
    "    dataset = dataset + [select_ids]\n",
    "    out = train_test_split(*dataset, test_size=test_ratio, random_state=random_state)\n",
    "    split_ids = out[-2:]\n",
    "    out = out[0:-2]\n",
    "\n",
    "    if report_id:\n",
    "        return(out, split_ids)\n",
    "    else:\n",
    "        return(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericData(Dataset):\n",
    "    def __init__(self, x, y, transform=None, dtype=torch.double, sample_ids=None):\n",
    "        assert (len(y) == len(x)), \"Number of x and y samples do not match!\"\n",
    "        self.len = len(y)\n",
    "        self.transform = transform\n",
    "        self.sample_ids = sample_ids\n",
    "\n",
    "        self.x, self.x_col, self.x_min, self.x_max = self._format_dataset(x, dtype)\n",
    "        self.y, self.y_col, self.y_min, self.y_max = self._format_dataset(y, dtype)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        sample = [self.x[index],\n",
    "                  self.y[index]]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def _format_dataset(self, d, dtype):\n",
    "        if type(d) == pd.core.frame.DataFrame:\n",
    "            # check to make sure that the sample_ids are the same as dataframe row index if sample_ids exist\n",
    "            if self.sample_ids is not None:\n",
    "                assert (len(unique_list([list(d.index),self.sample_ids])) == 1), \"Input data rowname/index not equal to sample_ids!\"\n",
    "            else:\n",
    "                self.sample_ids = list(d.index)\n",
    "            \n",
    "            # extract column names\n",
    "            colname = d.columns\n",
    "            \n",
    "            # get y-min/max (required for OHPL)\n",
    "            d_min = d.min().tolist()\n",
    "            d_min = d.max().tolist()          \n",
    "        else:\n",
    "            # extract column names\n",
    "            colname = d[0].columns\n",
    "\n",
    "            # get y-min/max (required for OHPL)\n",
    "            d_max = pd.concat(d).max().tolist()\n",
    "            d_min = pd.concat(d).min().tolist()\n",
    "            \n",
    "        # convert dataframe to list of a single row tensor\n",
    "        out = self._sample_type_convert(d, dtype)\n",
    "\n",
    "        return out, colname, d_min, d_max\n",
    "        \n",
    "    def _sample_type_convert(self, samples, dtype):\n",
    "        # since the input samples are list of single-row-dataframe, with dimension of 1 x Features\n",
    "        # to convert them into tensors, the row dimension is removed.\n",
    "        samples_out = [ torch.tensor(sample_ele.iloc[0]).type(dtype) for sample_ele in samples ]\n",
    "        return samples_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Fold Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_fold_indices(n_samples, k=5, shuffle=False):\n",
    "    '''\n",
    "    Drawing sample indices for K-Fold\n",
    "    \n",
    "    Args:\n",
    "        samples: Number of samples in the dataset\n",
    "        shuffle: Shuffling of samples\n",
    "\n",
    "    Returns:\n",
    "        kfold_train_ind: Indices for training set\n",
    "        kfold_valid_ind: Indices for validation set\n",
    "\n",
    "    Raises:\n",
    "        -\n",
    "\n",
    "    Author:\n",
    "        Dr. Calvin Chan\n",
    "        calvin.chan@bayer.com\n",
    "    '''\n",
    "    kfold = KFold(n_splits=k, shuffle=shuffle).split([*range(n_samples)])\n",
    "    i, kfold_ind = zip(*[*enumerate(kfold)])   # Expand the index obtained by the K-Fold function\n",
    "    kfold_train_ind, kfold_valid_ind = zip(*kfold_ind)\n",
    "    return(kfold_train_ind, kfold_valid_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_ind(ls,ind):\n",
    "    return [ ls[i] for i in ind.tolist() ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process and Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patitioned_data_object_numeric(x, y, test_split_ratio, k, random_state=25):\n",
    "    # Model/Test Splitting\n",
    "    (x_model, x_test, \n",
    "     y_model, y_test), (samples_id_model, samples_id_test) = model_test_split(x, y, \n",
    "                                                             test_ratio=test_split_ratio, \n",
    "                                                             report_id=True,\n",
    "                                                             random_state=random_state)\n",
    "    \n",
    "    # K-Fold Index Sampling\n",
    "    [kfold_train_ind, kfold_valid_ind] = get_k_fold_indices(n_samples=len(y_model), k=k, shuffle=False)   # Shuffle is NOT needed, since the samples were shuffled in the model/test split\n",
    "\n",
    "    # Create K-set of datasets for Pytorch data loader\n",
    "    dataset_train_kfold = [ NumericData(select_ind(x_model,fold_ind), \n",
    "                                        select_ind(y_model,fold_ind),\n",
    "                                        sample_ids = select_ind(samples_id_model, fold_ind)) \n",
    "                                           for fold_ind in kfold_train_ind ]\n",
    "    dataset_valid_kfold = [ NumericData(select_ind(x_model,fold_ind), \n",
    "                                        select_ind(y_model,fold_ind),\n",
    "                                        sample_ids = select_ind(samples_id_model, fold_ind)) \n",
    "                                           for fold_ind in kfold_valid_ind ]\n",
    "\n",
    "    # Create dataset for modeling and testing\n",
    "    dataset_model = NumericData(x_model, y_model, sample_ids = samples_id_model)\n",
    "    dataset_test = NumericData(x_test, y_test, sample_ids = samples_id_test)\n",
    "    \n",
    "    return dataset_model, dataset_test, dataset_train_kfold, dataset_valid_kfold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integer_partitions(n_ele, n_min=1, max_dim=None, recursion_level=1):\n",
    "    '''\n",
    "    Fast Integer Partitioning\n",
    "    Dividing a single integer into a list of integer that sums up to the given number\n",
    "    \n",
    "    Args:\n",
    "        num_ele: Total number of elements to be distributed\n",
    "        n_min: Minimum number of elements per output dimension\n",
    "\n",
    "    Returns:\n",
    "        Iterator as list of elements splitted into multiple dimensions\n",
    "        \n",
    "    Original Source :\n",
    "    (Modification made to speed up by skpping recurrsion exceed max_dim)\n",
    "        https://stackoverflow.com/questions/10035752/elegant-python-code-for-integer-partitioning\n",
    "    \n",
    "    Author:\n",
    "        Dr. Calvin Chan\n",
    "        calvin.chan@bayer.com\n",
    "    '''\n",
    "    if (max_dim is not None) and (recursion_level > max_dim):\n",
    "        yield None\n",
    "    else:\n",
    "        yield (n_ele,)\n",
    "        for i in range(n_min, n_ele//2 + 1):\n",
    "            for p in integer_partitions(n_ele-i, i, max_dim, recursion_level+1):\n",
    "                if p is not None:\n",
    "                    yield (i,) + p\n",
    "                elif recursion_level != 1:\n",
    "                    yield None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sampling(num_ele, n_min=1, n_max=None, out_dim=None, n_samples=1, prepend=[], postpend=[], single_sample=False):\n",
    "    '''\n",
    "    Randomly split the elements into multiple dimensions\n",
    "    This is use for neuron sampling the number of elements and layer for multibranch neural network\n",
    "    \n",
    "    Args:\n",
    "        num_ele: Total number of elements to be distributed\n",
    "        n_min: Minimum number of elements per output dimension\n",
    "        n_max: Maximum number of elements per output dimension\n",
    "        out_dim: Number of output dimensions to distribute the element, random dimensions will be given with None given\n",
    "\n",
    "    Returns:\n",
    "        sample: List of elements splitted into multiple dimensions\n",
    "        \n",
    "    Raises:\n",
    "        -\n",
    "        \n",
    "    Example:\n",
    "        >>> split_sampling(14, n_min=2, out_dim=4)\n",
    "        [2, 5, 4, 3]\n",
    "        \n",
    "    Author:\n",
    "        Dr. Calvin Chan\n",
    "        calvin.chan@bayer.com\n",
    "    '''\n",
    "    # !!! DEBUG !!!\n",
    "    # print(f\"num_ele: {num_ele}; n_min: {n_min}; out_dim: {out_dim}\")\n",
    "    \n",
    "    # Generate the Integer Partitions\n",
    "    splits = integer_partitions(num_ele, n_min=n_min, max_dim=out_dim)\n",
    "    if n_max is not None:\n",
    "        splits = [ split for split in splits if max(split) <= n_max ]\n",
    "    if out_dim is not None:\n",
    "        splits = [ split for split in splits if len(list(split)) == out_dim ]\n",
    "    else:\n",
    "        splits = [ split for split in splits ]\n",
    "    \n",
    "    # Filter with Number of Output Dimension\n",
    "    splits_perm = [list(set(itertools.permutations(split))) for split in splits ]\n",
    "    unique_splits_perm = list(itertools.chain.from_iterable(splits_perm))\n",
    "        \n",
    "    # Randomly Sample one of the permutation\n",
    "    if n_samples <= len(unique_splits_perm):\n",
    "        sample = list([ prepend+list(sample)+postpend for sample in random.sample(unique_splits_perm, k=n_samples)])\n",
    "    else:\n",
    "        sample = list([ prepend+list(sample)+postpend for sample in random.choices(unique_splits_perm, k=n_samples)])\n",
    "    if single_sample:\n",
    "        sample = sample[0]\n",
    "    \n",
    "    return(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OHPL-All Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohpl(y_true, pred, min_label, max_label, margin=1, ordering_loss_weight=1, loss_bound=1e9, ohpl_norm_order=2, dtype=torch.double):\n",
    "    '''\n",
    "    OHPL Hyperplane Loss Function\n",
    "    (Modified from: https://github.com/ohpl/ohpl/blob/master/OHPLall.ipynb)\n",
    "    \n",
    "    Args:\n",
    "        y_true: Ground Truth of output\n",
    "        y_pred: Network output (w^T Phi(x)) - NOT CATEGORICAL OUTPUT!!!\n",
    "        minlabel: Minimum ordinal categorical label of y\n",
    "        maxlabel: Maximum ordinal categorical label of y\n",
    "        margin:\n",
    "        ordering_loss_weight: \n",
    "        \n",
    "    Returns:\n",
    "        mean_loss: Loss measure\n",
    "\n",
    "    Raises:\n",
    "        -\n",
    "\n",
    "    Author:\n",
    "        Dr. Calvin Chan\n",
    "        calvin.chan@bayer.com\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # === HCL: Hyperplane Centroid Loss ===\n",
    "    # (To ensure hyperplane are ordered by rank)\n",
    "\n",
    "    pred = pred.type(dtype)\n",
    "    y_true = y_true.type(dtype)\n",
    "\n",
    "    ords, idx = torch.unique(y_true, return_inverse=True)\n",
    "    num_label = ords.shape[0]\n",
    "    y_true_ohe = F.one_hot(idx,num_classes=num_label)\n",
    "\n",
    "    # hyperplane intercept term\n",
    "    yO = pred.type(dtype) @ y_true_ohe.type(dtype)\n",
    "    yc = torch.sum(y_true_ohe, dim=0)\n",
    "    class_mean = torch.div(yO,yc).type(dtype)\n",
    "\n",
    "    # relative rank distance between centroids\n",
    "    min_distance = torch.reshape(ords,(-1,1)) - torch.reshape(ords,(1,-1))\n",
    "    min_distance = torch.relu(min_distance)\n",
    "\n",
    "    # keeps min. distance (???)\n",
    "    keep = torch.minimum(min_distance,torch.ones(min_distance.shape))\n",
    "    \n",
    "    # positive mean sample distance between centroids\n",
    "    centroid_distance = torch.reshape(class_mean,(-1,1)) - torch.reshape(class_mean,(1,-1))\n",
    "    centroid_distance = torch.relu(centroid_distance)   # zero loss for correct ordering\n",
    "    centroid_distance = torch.multiply(keep, centroid_distance)\n",
    "\n",
    "    hp_ordering_loss = torch.sum(torch.relu(min_distance - centroid_distance))\n",
    "\n",
    "    # === HPL/HPPL: Hyperplane Point Loss ===\n",
    "    # (To ensure transformation place the point near the correct centroid)\n",
    "    mean_centroid_of_sample = y_true_ohe.type(dtype) @ torch.reshape(class_mean,(-1,1))\n",
    "\n",
    "    # --- Limit Edge Case Loss ---\n",
    "    # No reason to limit distance from edge cases:\n",
    "    # 1. Positive edge case (max_label) for upper loss\n",
    "    # 2. Negative edge case (min_label) for lower loss\n",
    "    upper_bound = (y_true - max_label + 1) * loss_bound   # Select edge case and give a large loss_bound (we want to pull it back in case if it gets too big)\n",
    "    upper_bound = torch.relu(upper_bound) + margin        # Add margin to non-edge cases\n",
    "    lower_bound = (-(y_true - min_label) + 1) * loss_bound\n",
    "    lower_bound = torch.relu(lower_bound) + margin   \n",
    "\n",
    "    # -- Compute Loss ---\n",
    "    upper_loss = pred[:,None] - mean_centroid_of_sample\n",
    "    upper_loss_bounded = torch.relu(upper_loss - upper_bound[:,None])\n",
    "    lower_loss = -(pred[:,None] - mean_centroid_of_sample)\n",
    "    lower_loss_bounded = torch.relu(lower_loss - lower_bound[:,None])\n",
    "\n",
    "    hp_point_loss = torch.mean(upper_loss_bounded + lower_loss_bounded)\n",
    "\n",
    "        \n",
    "    # === OHPL ===\n",
    "    loss = torch.norm(torch.cat([hp_point_loss[None], (ordering_loss_weight * hp_ordering_loss)[None]]), p=ohpl_norm_order)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohpl_y_class_mean(y):\n",
    "    '''\n",
    "    Sample class mean calculation for computing centroid\n",
    "    The training sample class mean matrix was previously part of the dataloader object.  However, to allow maximum flexibility of \n",
    "    random sampling, it was separated out.\n",
    "    \n",
    "    Args:\n",
    "        y: Class label of training samples\n",
    "    \n",
    "    Return:\n",
    "        class_mean: The mean value of each class label for each sample according to their class\n",
    "    '''\n",
    "    ohe_encoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "    y_ohe = ohe_encoder.fit_transform(y)\n",
    "    y_ohe_inverse = 1/np.sum((y_ohe), axis=0)\n",
    "    class_mean = (y_ohe * y_ohe_inverse).T\n",
    "    return class_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohpl_predict(pred, centroid, min_label, delta=1e-9):\n",
    "    '''\n",
    "    OHPL Class Label prediction using training centroid\n",
    "    The OHPL train the transformation function, but the class label prediction requires using the function as well as the\n",
    "    centroid computed during training.  The output model only provide the sample projected dimension and distance between\n",
    "    the model transformed output and the centroid is required to determine class.\n",
    "    \n",
    "    Args:\n",
    "        pred: Model tranformed output at the ordinal hyperplane space\n",
    "        centroid: Class associated centroid in the ordinal hyperplane space\n",
    "        min_label: Lowest rank class label\n",
    "    \n",
    "    Return:\n",
    "        y_pred: Predicted class label\n",
    "        \n",
    "    Raises:\n",
    "        -\n",
    "\n",
    "    Author:\n",
    "        Dr. Calvin Chan\n",
    "        calvin.chan@bayer.com\n",
    "\n",
    "    '''\n",
    "    y_dist = torch.abs(pred - centroid)\n",
    "    y_prob = (1/(y_dist+delta))/torch.sum(1/(y_dist+delta),axis=1)[:,None]   # convert distance to probability for cross-entropy computation\n",
    "    y_pred = torch.argmin(y_dist, axis=1) + min_label\n",
    "    return y_pred, y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ohpl_raytune(config, \n",
    "                          num_in_feat,\n",
    "                          train_dataset, \n",
    "                          valid_dataset,\n",
    "                          criterion=ohpl,\n",
    "                          checkpoint_dir=None, \n",
    "                          num_epochs=100, \n",
    "                          metric_dict = {'mae': lambda y_est,y: torch.mean(abs(y_est-y)), \n",
    "                                         'mze': lambda y_est,y: torch.mean((torch.abs(y_est-y) > 0).type(torch.double)),\n",
    "                                         'f1-micro':  lambda y_est,y: f1_score(y,y_est,average='micro'),\n",
    "                                         'f1-macro':  lambda y_est,y: f1_score(y,y_est,average='macro'),},\n",
    "                          ohpl_norm_order=1,\n",
    "                          dtype=torch.double,\n",
    "                          force_cpu=False\n",
    "                          ):\n",
    "    '''\n",
    "    '''\n",
    "    \n",
    "    # gpu usage\n",
    "    if not force_cpu:\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "\n",
    "    #====================== Ray Tune Parameters Setup ======================#\n",
    "\n",
    "    if 'dropout_p' in config.keys():\n",
    "        _dropout_p = config['dropout_p']\n",
    "    else:\n",
    "        _dropout_p = 0\n",
    "\n",
    "    if 'batch_size' in config.keys():\n",
    "        _batch_size = config['batch_size']\n",
    "    else:\n",
    "        _batch_size = 1\n",
    "        \n",
    "    if 'margin' in config.keys():\n",
    "        _margin = config['margin']\n",
    "    else:\n",
    "        _margin = 1\n",
    "        \n",
    "    if 'ordering_loss_weight' in config.keys():\n",
    "        _ordering_loss_weight = config['ordering_loss_weight']\n",
    "    else:\n",
    "        _ordering_loss_weight = 1\n",
    "        \n",
    "    if 'loss_bound' in config.keys():\n",
    "        _loss_bound = config['loss_bound']\n",
    "    else:\n",
    "        _loss_bound = 1e9\n",
    "        \n",
    "    if (type(train_dataset) is list) and (type(valid_dataset) is list) and ('k' in config.keys()):\n",
    "        _train_dataset = train_dataset[config['k']]\n",
    "        _valid_dataset = valid_dataset[config['k']]\n",
    "    else:\n",
    "        _train_dataset = train_dataset\n",
    "        _valid_dataset = valid_dataset\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(dataset=_train_dataset, batch_size=_batch_size, shuffle=True, \n",
    "                                               collate_fn=lambda x: [ x_ele.to(device) for x_ele in default_collate(x) ] )\n",
    "    valid_loader = torch.utils.data.DataLoader(dataset=_valid_dataset, batch_size=_batch_size, shuffle=True,\n",
    "                                               collate_fn=lambda x: [ x_ele.to(device) for x_ele in default_collate(x) ] )\n",
    "\n",
    "    y_col_index = 0\n",
    "    min_label = _train_dataset.y_min[y_col_index]\n",
    "    max_label = _train_dataset.y_max[y_col_index]\n",
    "    \n",
    "    # initialize ANN architecture\n",
    "    model = MultiLayerFC(in_feat = num_in_feat,\n",
    "                         layers = config['h_fc'],\n",
    "                         dropout_p = _dropout_p,\n",
    "                         act_fn = torch.relu).to(device)\n",
    "    model.apply(initialize_weights)\n",
    "    \n",
    "    # optimizer is controlled by ray tune hyperparameter\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = config[\"lr\"])\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        #====================== Training ======================#\n",
    "\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "\n",
    "        # training using all training samples\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # set the model to training mode\n",
    "            model.train()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            pred = model(x)\n",
    "\n",
    "            ohpl_loss = criterion(y.squeeze(dim=1), \n",
    "                                  pred, \n",
    "                                  min_label, \n",
    "                                  max_label, \n",
    "                                  _margin, \n",
    "                                  _ordering_loss_weight, \n",
    "                                  _loss_bound,\n",
    "                                  ohpl_norm_order,\n",
    "                                  dtype)\n",
    "            ohpl_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # set the model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        #====================== Compute Metrics ======================#\n",
    "        # model can only be evaluated after finishing the complete dataset for OHPL\n",
    "\n",
    "        train_pred = torch.tensor([])\n",
    "        valid_pred = torch.tensor([])\n",
    "        history = {'train': {}, 'valid': {}}\n",
    "        history['train']['y'] = torch.tensor([])\n",
    "        history['valid']['y'] = torch.tensor([])\n",
    "        metric_output = {}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for  i, (x, y) in enumerate(train_loader):\n",
    "                pred = model(x)\n",
    "                train_pred = torch.cat([train_pred,pred])\n",
    "                history['train']['y'] = torch.cat( [history['train']['y'], y.squeeze()], dim=0 )\n",
    "\n",
    "            y_class_mean = ohpl_y_class_mean(history['train']['y'].reshape(-1,1))\n",
    "            centroid = torch.reshape( torch.tensor(y_class_mean @ train_pred.numpy()), [1,-1] )\n",
    "            history['train']['y_est'], train_prob = ohpl_predict(train_pred, centroid, min_label)\n",
    "\n",
    "            for  i, (x, y) in enumerate(valid_loader):\n",
    "                pred = model(x)\n",
    "                valid_pred = torch.cat([valid_pred,pred])\n",
    "                history['valid']['y'] = torch.cat( (history['valid']['y'], y.squeeze()), dim=0 )\n",
    "            history['valid']['y_est'], valid_prob = ohpl_predict(valid_pred, centroid, min_label)\n",
    "\n",
    "        for metric in metric_dict.keys():\n",
    "            for dataset in history.keys():\n",
    "                metric_label = '_'.join([dataset,metric])\n",
    "                metric_output[metric_label] = metric_dict[metric](history[dataset]['y_est'],history[dataset]['y']).item()\n",
    "\n",
    "        # ohpl metric append to output\n",
    "        metric_output['ohpl'] = ohpl_loss.item()\n",
    "                \n",
    "        # cross-entropy-loss metric requires probability matrix\n",
    "        # (not using y_pred & y_true for computation, therefore need to separate out)\n",
    "        cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "        cel_train = cross_entropy_loss(train_prob, history['train']['y'].type(torch.int64) - min_label)\n",
    "        cel_valid = cross_entropy_loss(valid_prob, history['valid']['y'].type(torch.int64) - min_label)\n",
    "        metric_output['train_cross-entropy'] = cel_train.item()\n",
    "        metric_output['valid_cross-entropy'] = cel_valid.item()\n",
    "        \n",
    "        # Here we save a checkpoint. It is automatically registered with\n",
    "        # Ray Tune and will potentially be passed as the `checkpoint_dir`\n",
    "        # parameter in future iterations.\n",
    "        with tune.checkpoint_dir(step=epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save( (model.state_dict(), optimizer.state_dict(), centroid), path )\n",
    "\n",
    "        tune.report(**metric_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training procedure\n",
    "def train_ohpl(model, train_dataset, valid_dataset, min_label, max_label, criterion, optimizer, \n",
    "                  num_epochs=100, \n",
    "                  batch_size=2, \n",
    "                  metric_dict = {'mae': lambda y_est,y: torch.mean(abs(y_est-y)), \n",
    "                                 'mze': lambda y_est,y: torch.mean((torch.abs(y_est-y) > 0).type(torch.double)),\n",
    "                                 'f1-micro':  lambda y_est,y: f1_score(y,y_est,average='micro'),\n",
    "                                 'f1-macro':  lambda y_est,y: f1_score(y,y_est,average='macro'),},\n",
    "                  margin=1,\n",
    "                  ordering_loss_weight=1, \n",
    "                  loss_bound=1e9,\n",
    "                  ohpl_norm_order=1,\n",
    "                  show_progress=True,\n",
    "                  dtype=torch.double,\n",
    "                  ):\n",
    "    '''\n",
    "    Training procedure for OHPL classifier\n",
    "    \n",
    "    Author:\n",
    "        Dr. Calvin Chan\n",
    "        calvin.chan@bayer.com\n",
    "    '''\n",
    "    history = {'train': {}, 'valid': {}}\n",
    "    metric_output = {}\n",
    "\n",
    "    # gpu usage\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # parallel gpu usage\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)   # for multiple GPUs\n",
    "    model.to(device)\n",
    "\n",
    "    # initialize dataloader\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                               collate_fn=lambda x: [ x_ele.to(device) for x_ele in default_collate(x) ])\n",
    "    valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                               collate_fn=lambda x: [ x_ele.to(device) for x_ele in default_collate(x) ])\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        #====================== Training ======================#\n",
    "\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "\n",
    "        # training using all training samples\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # set the model to training mode\n",
    "            model.train()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            pred = model(x)\n",
    "\n",
    "            ohpl_loss = criterion(y.squeeze(), \n",
    "                                  pred.squeeze(), \n",
    "                                  min_label, \n",
    "                                  max_label, \n",
    "                                  margin, \n",
    "                                  ordering_loss_weight, \n",
    "                                  loss_bound,\n",
    "                                  ohpl_norm_order,\n",
    "                                  dtype)\n",
    "            ohpl_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # set the model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        #====================== Compute Metrics ======================#\n",
    "        # model can only be evaluated after finishing the complete dataset for OHPL\n",
    "\n",
    "        train_pred = torch.tensor([])\n",
    "        valid_pred = torch.tensor([])\n",
    "        history['train']['y'] = torch.tensor([])\n",
    "        history['valid']['y'] = torch.tensor([])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for  i, (x, y) in enumerate(train_loader):\n",
    "                pred = model(x)\n",
    "                train_pred = torch.cat([train_pred,pred])\n",
    "                history['train']['y'] = torch.cat( [history['train']['y'], y.squeeze()], dim=0 )\n",
    "\n",
    "            y_class_mean = ohpl_y_class_mean(history['train']['y'].reshape(-1,1))\n",
    "            centroid = torch.reshape( torch.tensor(y_class_mean @ train_pred.numpy()), [1,-1] )\n",
    "            history['train']['y_est'], train_prob = ohpl_predict(train_pred, centroid, min_label)\n",
    "\n",
    "            for  i, (x, y) in enumerate(valid_loader):\n",
    "                pred = model(x)\n",
    "                valid_pred = torch.cat([valid_pred,pred])\n",
    "                history['valid']['y'] = torch.cat( (history['valid']['y'], y.squeeze()), dim=0 )\n",
    "            history['valid']['y_est'], valid_prob = ohpl_predict(valid_pred, centroid, min_label)\n",
    "\n",
    "        # compute loss metrics based on y_pred & y_true\n",
    "        metric_labels = []\n",
    "        for metric in metric_dict.keys():\n",
    "            for dataset in history.keys():\n",
    "                metric_label = '_'.join([dataset,metric])\n",
    "                metric_output[metric_label] = metric_dict[metric](history[dataset]['y_est'],history[dataset]['y'])\n",
    "                metric_labels.append(metric_label)\n",
    "\n",
    "        # cross-entropy-loss metric requires probability matrix\n",
    "        # (not using y_pred & y_true for computation, therefore need to separate out)\n",
    "        cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "        metric_labels.append(\"train_cross-entropy\")\n",
    "        metric_labels.append(\"valid_cross-entropy\")\n",
    "        cel_train = cross_entropy_loss(train_prob, history['train']['y'].type(torch.int64) - min_label)\n",
    "        cel_valid = cross_entropy_loss(valid_prob, history['valid']['y'].type(torch.int64) - min_label)\n",
    "        metric_output['train_cross-entropy'] = cel_train\n",
    "        metric_output['valid_cross-entropy'] = cel_valid\n",
    "        \n",
    "         \n",
    "        # display metrics\n",
    "        if show_progress:\n",
    "            print(f\"[Epoch: { epoch+1 }]\", end=\" \" )\n",
    "            print(f\"OHPL Loss: {ohpl_loss}\")\n",
    "            for metric_label in metric_labels:\n",
    "                print(f\"{metric_label}: {metric_output[metric_label].item():.3f},\", end=\" \")\n",
    "            print(f\"\")\n",
    "\n",
    "    return (centroid, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_8004/448558442.py\u001b[0m(145)\u001b[0;36mmodel_test_split\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    143 \u001b[0;31m    \u001b[0;31m# Including index as one of the splitting dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    144 \u001b[0;31m    \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 145 \u001b[0;31m    \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ratio\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Number of samples resulting from ratio must be larger than 1 sample!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    146 \u001b[0;31m    \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mselect_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    147 \u001b[0;31m    \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> all(test_ratio*len(data) >= 1 for data in dataset)\n",
      "*** NameError: name 'test_ratio' is not defined\n",
      "ipdb> c\n"
     ]
    }
   ],
   "source": [
    "dataset_model, dataset_test, dataset_train_kfold, dataset_valid_kfold = patitioned_data_object_numeric(x, y, test_split_ratio, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 1] OHPL Loss: 1.6218199452260271\n",
      "train_mae: 0.692, valid_mae: 0.720, train_mze: 0.579, valid_mze: 0.605, train_f1-micro: 0.421, valid_f1-micro: 0.395, train_f1-macro: 0.388, valid_f1-macro: 0.358, train_cross-entropy: 1.648, valid_cross-entropy: 1.665, \n",
      "[Epoch: 2] OHPL Loss: 2.9365480031398343\n",
      "train_mae: 0.654, valid_mae: 0.694, train_mze: 0.556, valid_mze: 0.592, train_f1-micro: 0.444, valid_f1-micro: 0.408, train_f1-macro: 0.408, valid_f1-macro: 0.366, train_cross-entropy: 1.641, valid_cross-entropy: 1.649, \n",
      "[Epoch: 3] OHPL Loss: 3.663002204848225\n",
      "train_mae: 0.660, valid_mae: 0.675, train_mze: 0.558, valid_mze: 0.586, train_f1-micro: 0.442, valid_f1-micro: 0.414, train_f1-macro: 0.403, valid_f1-macro: 0.370, train_cross-entropy: 1.635, valid_cross-entropy: 1.641, \n",
      "[Epoch: 4] OHPL Loss: 6.567320220031508\n",
      "train_mae: 0.652, valid_mae: 0.669, train_mze: 0.545, valid_mze: 0.567, train_f1-micro: 0.455, valid_f1-micro: 0.433, train_f1-macro: 0.411, valid_f1-macro: 0.379, train_cross-entropy: 1.638, valid_cross-entropy: 1.641, \n",
      "[Epoch: 5] OHPL Loss: 2.1202147188528224\n",
      "train_mae: 0.694, valid_mae: 0.694, train_mze: 0.577, valid_mze: 0.586, train_f1-micro: 0.423, valid_f1-micro: 0.414, train_f1-macro: 0.384, valid_f1-macro: 0.370, train_cross-entropy: 1.656, valid_cross-entropy: 1.669, \n",
      "[Epoch: 6] OHPL Loss: 2.4325644347705286\n",
      "train_mae: 0.655, valid_mae: 0.720, train_mze: 0.559, valid_mze: 0.599, train_f1-micro: 0.441, valid_f1-micro: 0.401, train_f1-macro: 0.397, valid_f1-macro: 0.367, train_cross-entropy: 1.646, valid_cross-entropy: 1.668, \n",
      "[Epoch: 7] OHPL Loss: 1.9370452280833317\n",
      "train_mae: 0.598, valid_mae: 0.624, train_mze: 0.524, valid_mze: 0.554, train_f1-micro: 0.476, valid_f1-micro: 0.446, train_f1-macro: 0.430, valid_f1-macro: 0.381, train_cross-entropy: 1.618, valid_cross-entropy: 1.620, \n",
      "[Epoch: 8] OHPL Loss: 1.8983791184520535\n",
      "train_mae: 0.574, valid_mae: 0.580, train_mze: 0.506, valid_mze: 0.535, train_f1-micro: 0.494, valid_f1-micro: 0.465, train_f1-macro: 0.446, valid_f1-macro: 0.397, train_cross-entropy: 1.601, valid_cross-entropy: 1.625, \n",
      "[Epoch: 9] OHPL Loss: 1.1530076446690363\n",
      "train_mae: 0.548, valid_mae: 0.567, train_mze: 0.490, valid_mze: 0.529, train_f1-micro: 0.510, valid_f1-micro: 0.471, train_f1-macro: 0.462, valid_f1-macro: 0.401, train_cross-entropy: 1.597, valid_cross-entropy: 1.627, \n",
      "[Epoch: 10] OHPL Loss: 2.0392039751548414\n",
      "train_mae: 0.562, valid_mae: 0.580, train_mze: 0.503, valid_mze: 0.535, train_f1-micro: 0.497, valid_f1-micro: 0.465, train_f1-macro: 0.461, valid_f1-macro: 0.402, train_cross-entropy: 1.601, valid_cross-entropy: 1.638, \n",
      "[Epoch: 11] OHPL Loss: 0.6793982804231633\n",
      "train_mae: 0.558, valid_mae: 0.573, train_mze: 0.495, valid_mze: 0.529, train_f1-micro: 0.505, valid_f1-micro: 0.471, train_f1-macro: 0.460, valid_f1-macro: 0.403, train_cross-entropy: 1.596, valid_cross-entropy: 1.627, \n",
      "[Epoch: 12] OHPL Loss: 0.4216463354146178\n",
      "train_mae: 0.543, valid_mae: 0.554, train_mze: 0.481, valid_mze: 0.510, train_f1-micro: 0.519, valid_f1-micro: 0.490, train_f1-macro: 0.466, valid_f1-macro: 0.405, train_cross-entropy: 1.590, valid_cross-entropy: 1.619, \n",
      "[Epoch: 13] OHPL Loss: 0.9252179763224281\n",
      "train_mae: 0.522, valid_mae: 0.529, train_mze: 0.466, valid_mze: 0.497, train_f1-micro: 0.534, valid_f1-micro: 0.503, train_f1-macro: 0.486, valid_f1-macro: 0.428, train_cross-entropy: 1.587, valid_cross-entropy: 1.614, \n",
      "[Epoch: 14] OHPL Loss: 0.5067021657372339\n",
      "train_mae: 0.518, valid_mae: 0.522, train_mze: 0.465, valid_mze: 0.490, train_f1-micro: 0.535, valid_f1-micro: 0.510, train_f1-macro: 0.496, valid_f1-macro: 0.436, train_cross-entropy: 1.586, valid_cross-entropy: 1.603, \n",
      "[Epoch: 15] OHPL Loss: 1.4353628405661056\n",
      "train_mae: 0.498, valid_mae: 0.503, train_mze: 0.460, valid_mze: 0.478, train_f1-micro: 0.540, valid_f1-micro: 0.522, train_f1-macro: 0.506, valid_f1-macro: 0.447, train_cross-entropy: 1.584, valid_cross-entropy: 1.600, \n",
      "[Epoch: 16] OHPL Loss: 0.8867959009844615\n",
      "train_mae: 0.511, valid_mae: 0.503, train_mze: 0.470, valid_mze: 0.484, train_f1-micro: 0.530, valid_f1-micro: 0.516, train_f1-macro: 0.491, valid_f1-macro: 0.437, train_cross-entropy: 1.593, valid_cross-entropy: 1.597, \n",
      "[Epoch: 17] OHPL Loss: 0.6173187072339524\n",
      "train_mae: 0.534, valid_mae: 0.548, train_mze: 0.484, valid_mze: 0.497, train_f1-micro: 0.516, valid_f1-micro: 0.503, train_f1-macro: 0.465, valid_f1-macro: 0.424, train_cross-entropy: 1.602, valid_cross-entropy: 1.607, \n",
      "[Epoch: 18] OHPL Loss: 0.4846176828759338\n",
      "train_mae: 0.538, valid_mae: 0.567, train_mze: 0.476, valid_mze: 0.516, train_f1-micro: 0.524, valid_f1-micro: 0.484, train_f1-macro: 0.473, valid_f1-macro: 0.408, train_cross-entropy: 1.594, valid_cross-entropy: 1.616, \n",
      "[Epoch: 19] OHPL Loss: 0.827689107472832\n",
      "train_mae: 0.518, valid_mae: 0.554, train_mze: 0.470, valid_mze: 0.516, train_f1-micro: 0.530, valid_f1-micro: 0.484, train_f1-macro: 0.490, valid_f1-macro: 0.410, train_cross-entropy: 1.588, valid_cross-entropy: 1.610, \n",
      "[Epoch: 20] OHPL Loss: 0.3397318599469426\n",
      "train_mae: 0.508, valid_mae: 0.548, train_mze: 0.463, valid_mze: 0.510, train_f1-micro: 0.537, valid_f1-micro: 0.490, train_f1-macro: 0.491, valid_f1-macro: 0.421, train_cross-entropy: 1.591, valid_cross-entropy: 1.613, \n",
      "[Epoch: 21] OHPL Loss: 0.48885471081015036\n",
      "train_mae: 0.524, valid_mae: 0.522, train_mze: 0.476, valid_mze: 0.484, train_f1-micro: 0.524, valid_f1-micro: 0.516, train_f1-macro: 0.477, valid_f1-macro: 0.435, train_cross-entropy: 1.585, valid_cross-entropy: 1.619, \n",
      "[Epoch: 22] OHPL Loss: 1.1423669770008522\n",
      "train_mae: 0.482, valid_mae: 0.503, train_mze: 0.444, valid_mze: 0.478, train_f1-micro: 0.556, valid_f1-micro: 0.522, train_f1-macro: 0.517, valid_f1-macro: 0.444, train_cross-entropy: 1.574, valid_cross-entropy: 1.600, \n",
      "[Epoch: 23] OHPL Loss: 0.928827055076779\n",
      "train_mae: 0.478, valid_mae: 0.503, train_mze: 0.444, valid_mze: 0.490, train_f1-micro: 0.556, valid_f1-micro: 0.510, train_f1-macro: 0.528, valid_f1-macro: 0.448, train_cross-entropy: 1.573, valid_cross-entropy: 1.597, \n",
      "[Epoch: 24] OHPL Loss: 1.7911702115555055\n",
      "train_mae: 0.474, valid_mae: 0.516, train_mze: 0.433, valid_mze: 0.503, train_f1-micro: 0.567, valid_f1-micro: 0.497, train_f1-macro: 0.551, valid_f1-macro: 0.450, train_cross-entropy: 1.575, valid_cross-entropy: 1.603, \n",
      "[Epoch: 25] OHPL Loss: 0.8655492684958728\n",
      "train_mae: 0.473, valid_mae: 0.510, train_mze: 0.433, valid_mze: 0.497, train_f1-micro: 0.567, valid_f1-micro: 0.503, train_f1-macro: 0.547, valid_f1-macro: 0.456, train_cross-entropy: 1.570, valid_cross-entropy: 1.594, \n",
      "[Epoch: 26] OHPL Loss: 0.3019332589218528\n",
      "train_mae: 0.478, valid_mae: 0.490, train_mze: 0.438, valid_mze: 0.471, train_f1-micro: 0.562, valid_f1-micro: 0.529, train_f1-macro: 0.544, valid_f1-macro: 0.474, train_cross-entropy: 1.569, valid_cross-entropy: 1.591, \n",
      "[Epoch: 27] OHPL Loss: 0.8328318725528321\n",
      "train_mae: 0.487, valid_mae: 0.484, train_mze: 0.447, valid_mze: 0.478, train_f1-micro: 0.553, valid_f1-micro: 0.522, train_f1-macro: 0.530, valid_f1-macro: 0.464, train_cross-entropy: 1.567, valid_cross-entropy: 1.586, \n",
      "[Epoch: 28] OHPL Loss: 1.2983812346291652\n",
      "train_mae: 0.487, valid_mae: 0.490, train_mze: 0.452, valid_mze: 0.471, train_f1-micro: 0.548, valid_f1-micro: 0.529, train_f1-macro: 0.519, valid_f1-macro: 0.460, train_cross-entropy: 1.570, valid_cross-entropy: 1.584, \n",
      "[Epoch: 29] OHPL Loss: 1.225070546058499\n",
      "train_mae: 0.495, valid_mae: 0.554, train_mze: 0.455, valid_mze: 0.535, train_f1-micro: 0.545, valid_f1-micro: 0.465, train_f1-macro: 0.509, valid_f1-macro: 0.400, train_cross-entropy: 1.577, valid_cross-entropy: 1.593, \n",
      "[Epoch: 30] OHPL Loss: 2.0738631746335554\n",
      "train_mae: 0.466, valid_mae: 0.459, train_mze: 0.431, valid_mze: 0.446, train_f1-micro: 0.569, valid_f1-micro: 0.554, train_f1-macro: 0.543, valid_f1-macro: 0.471, train_cross-entropy: 1.569, valid_cross-entropy: 1.575, \n",
      "[Epoch: 31] OHPL Loss: 1.1176363930780706\n",
      "train_mae: 0.481, valid_mae: 0.490, train_mze: 0.449, valid_mze: 0.478, train_f1-micro: 0.551, valid_f1-micro: 0.522, train_f1-macro: 0.525, valid_f1-macro: 0.455, train_cross-entropy: 1.569, valid_cross-entropy: 1.585, \n",
      "[Epoch: 32] OHPL Loss: 1.810539166463987\n",
      "train_mae: 0.500, valid_mae: 0.497, train_mze: 0.466, valid_mze: 0.478, train_f1-micro: 0.534, valid_f1-micro: 0.522, train_f1-macro: 0.501, valid_f1-macro: 0.449, train_cross-entropy: 1.572, valid_cross-entropy: 1.584, \n",
      "[Epoch: 33] OHPL Loss: 0.815614073354052\n",
      "train_mae: 0.490, valid_mae: 0.516, train_mze: 0.457, valid_mze: 0.490, train_f1-micro: 0.543, valid_f1-micro: 0.510, train_f1-macro: 0.506, valid_f1-macro: 0.430, train_cross-entropy: 1.579, valid_cross-entropy: 1.593, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 34] OHPL Loss: 0.2948230253539015\n",
      "train_mae: 0.538, valid_mae: 0.592, train_mze: 0.479, valid_mze: 0.548, train_f1-micro: 0.521, valid_f1-micro: 0.452, train_f1-macro: 0.477, valid_f1-macro: 0.376, train_cross-entropy: 1.616, valid_cross-entropy: 1.640, \n",
      "[Epoch: 35] OHPL Loss: 0.6943485964438424\n",
      "train_mae: 0.534, valid_mae: 0.567, train_mze: 0.484, valid_mze: 0.529, train_f1-micro: 0.516, valid_f1-micro: 0.471, train_f1-macro: 0.475, valid_f1-macro: 0.420, train_cross-entropy: 1.603, valid_cross-entropy: 1.621, \n",
      "[Epoch: 36] OHPL Loss: 0.23945796907232283\n",
      "train_mae: 0.481, valid_mae: 0.529, train_mze: 0.450, valid_mze: 0.503, train_f1-micro: 0.550, valid_f1-micro: 0.497, train_f1-macro: 0.536, valid_f1-macro: 0.439, train_cross-entropy: 1.586, valid_cross-entropy: 1.599, \n",
      "[Epoch: 37] OHPL Loss: 1.9278924146457976\n",
      "train_mae: 0.481, valid_mae: 0.478, train_mze: 0.452, valid_mze: 0.452, train_f1-micro: 0.548, valid_f1-micro: 0.548, train_f1-macro: 0.552, valid_f1-macro: 0.480, train_cross-entropy: 1.575, valid_cross-entropy: 1.593, \n",
      "[Epoch: 38] OHPL Loss: 0.5163105397476169\n",
      "train_mae: 0.489, valid_mae: 0.490, train_mze: 0.447, valid_mze: 0.471, train_f1-micro: 0.553, valid_f1-micro: 0.529, train_f1-macro: 0.557, valid_f1-macro: 0.477, train_cross-entropy: 1.571, valid_cross-entropy: 1.594, \n",
      "[Epoch: 39] OHPL Loss: 0.647516307935113\n",
      "train_mae: 0.474, valid_mae: 0.478, train_mze: 0.433, valid_mze: 0.465, train_f1-micro: 0.567, valid_f1-micro: 0.535, train_f1-macro: 0.564, valid_f1-macro: 0.485, train_cross-entropy: 1.570, valid_cross-entropy: 1.591, \n",
      "[Epoch: 40] OHPL Loss: 1.0373292339198896\n",
      "train_mae: 0.460, valid_mae: 0.490, train_mze: 0.425, valid_mze: 0.478, train_f1-micro: 0.575, valid_f1-micro: 0.522, train_f1-macro: 0.568, valid_f1-macro: 0.471, train_cross-entropy: 1.571, valid_cross-entropy: 1.592, \n",
      "[Epoch: 41] OHPL Loss: 1.4669414330911719\n",
      "train_mae: 0.478, valid_mae: 0.522, train_mze: 0.444, valid_mze: 0.510, train_f1-micro: 0.556, valid_f1-micro: 0.490, train_f1-macro: 0.558, valid_f1-macro: 0.460, train_cross-entropy: 1.572, valid_cross-entropy: 1.591, \n",
      "[Epoch: 42] OHPL Loss: 1.7472041230117128\n",
      "train_mae: 0.473, valid_mae: 0.478, train_mze: 0.444, valid_mze: 0.465, train_f1-micro: 0.556, valid_f1-micro: 0.535, train_f1-macro: 0.557, valid_f1-macro: 0.482, train_cross-entropy: 1.572, valid_cross-entropy: 1.589, \n",
      "[Epoch: 43] OHPL Loss: 0.5247884300003594\n",
      "train_mae: 0.481, valid_mae: 0.503, train_mze: 0.450, valid_mze: 0.484, train_f1-micro: 0.550, valid_f1-micro: 0.516, train_f1-macro: 0.543, valid_f1-macro: 0.465, train_cross-entropy: 1.584, valid_cross-entropy: 1.601, \n",
      "[Epoch: 44] OHPL Loss: 2.065380641958961\n",
      "train_mae: 0.511, valid_mae: 0.510, train_mze: 0.474, valid_mze: 0.484, train_f1-micro: 0.526, valid_f1-micro: 0.516, train_f1-macro: 0.507, valid_f1-macro: 0.436, train_cross-entropy: 1.584, valid_cross-entropy: 1.608, \n",
      "[Epoch: 45] OHPL Loss: 0.40987137466684603\n",
      "train_mae: 0.518, valid_mae: 0.497, train_mze: 0.474, valid_mze: 0.471, train_f1-micro: 0.526, valid_f1-micro: 0.529, train_f1-macro: 0.492, valid_f1-macro: 0.401, train_cross-entropy: 1.583, valid_cross-entropy: 1.593, \n",
      "[Epoch: 46] OHPL Loss: 1.4216785243560421\n",
      "train_mae: 0.463, valid_mae: 0.478, train_mze: 0.436, valid_mze: 0.465, train_f1-micro: 0.564, valid_f1-micro: 0.535, train_f1-macro: 0.548, valid_f1-macro: 0.469, train_cross-entropy: 1.570, valid_cross-entropy: 1.590, \n",
      "[Epoch: 47] OHPL Loss: 1.0467685256272417\n",
      "train_mae: 0.449, valid_mae: 0.439, train_mze: 0.421, valid_mze: 0.433, train_f1-micro: 0.579, valid_f1-micro: 0.567, train_f1-macro: 0.577, valid_f1-macro: 0.513, train_cross-entropy: 1.565, valid_cross-entropy: 1.578, \n",
      "[Epoch: 48] OHPL Loss: 1.0035052056366922\n",
      "train_mae: 0.447, valid_mae: 0.452, train_mze: 0.420, valid_mze: 0.439, train_f1-micro: 0.580, valid_f1-micro: 0.561, train_f1-macro: 0.576, valid_f1-macro: 0.509, train_cross-entropy: 1.568, valid_cross-entropy: 1.584, \n",
      "[Epoch: 49] OHPL Loss: 0.9428585267106162\n",
      "train_mae: 0.449, valid_mae: 0.446, train_mze: 0.426, valid_mze: 0.433, train_f1-micro: 0.574, valid_f1-micro: 0.567, train_f1-macro: 0.563, valid_f1-macro: 0.497, train_cross-entropy: 1.569, valid_cross-entropy: 1.579, \n",
      "[Epoch: 50] OHPL Loss: 0.518607961312692\n",
      "train_mae: 0.471, valid_mae: 0.484, train_mze: 0.444, valid_mze: 0.471, train_f1-micro: 0.556, valid_f1-micro: 0.529, train_f1-macro: 0.545, valid_f1-macro: 0.468, train_cross-entropy: 1.564, valid_cross-entropy: 1.584, \n",
      "[Epoch: 51] OHPL Loss: 0.4243737477794245\n",
      "train_mae: 0.454, valid_mae: 0.478, train_mze: 0.429, valid_mze: 0.471, train_f1-micro: 0.571, valid_f1-micro: 0.529, train_f1-macro: 0.558, valid_f1-macro: 0.470, train_cross-entropy: 1.561, valid_cross-entropy: 1.579, \n",
      "[Epoch: 52] OHPL Loss: 1.7072095277126167\n",
      "train_mae: 0.455, valid_mae: 0.459, train_mze: 0.434, valid_mze: 0.452, train_f1-micro: 0.566, valid_f1-micro: 0.548, train_f1-macro: 0.549, valid_f1-macro: 0.462, train_cross-entropy: 1.558, valid_cross-entropy: 1.562, \n",
      "[Epoch: 53] OHPL Loss: 0.7342311392705225\n",
      "train_mae: 0.465, valid_mae: 0.465, train_mze: 0.441, valid_mze: 0.452, train_f1-micro: 0.559, valid_f1-micro: 0.548, train_f1-macro: 0.549, valid_f1-macro: 0.471, train_cross-entropy: 1.557, valid_cross-entropy: 1.571, \n",
      "[Epoch: 54] OHPL Loss: 0.569815314717692\n",
      "train_mae: 0.458, valid_mae: 0.433, train_mze: 0.433, valid_mze: 0.420, train_f1-micro: 0.567, valid_f1-micro: 0.580, train_f1-macro: 0.561, valid_f1-macro: 0.517, train_cross-entropy: 1.568, valid_cross-entropy: 1.569, \n",
      "[Epoch: 55] OHPL Loss: 1.0316837168456405\n",
      "train_mae: 0.463, valid_mae: 0.471, train_mze: 0.436, valid_mze: 0.459, train_f1-micro: 0.564, valid_f1-micro: 0.541, train_f1-macro: 0.581, valid_f1-macro: 0.519, train_cross-entropy: 1.564, valid_cross-entropy: 1.585, \n",
      "[Epoch: 56] OHPL Loss: 0.7940792910197297\n",
      "train_mae: 0.450, valid_mae: 0.471, train_mze: 0.421, valid_mze: 0.459, train_f1-micro: 0.579, valid_f1-micro: 0.541, train_f1-macro: 0.589, valid_f1-macro: 0.502, train_cross-entropy: 1.562, valid_cross-entropy: 1.590, \n",
      "[Epoch: 57] OHPL Loss: 0.2224053673403914\n",
      "train_mae: 0.462, valid_mae: 0.484, train_mze: 0.431, valid_mze: 0.471, train_f1-micro: 0.569, valid_f1-micro: 0.529, train_f1-macro: 0.571, valid_f1-macro: 0.494, train_cross-entropy: 1.570, valid_cross-entropy: 1.578, \n",
      "[Epoch: 58] OHPL Loss: 1.8329556445768327\n",
      "train_mae: 0.455, valid_mae: 0.452, train_mze: 0.429, valid_mze: 0.446, train_f1-micro: 0.571, valid_f1-micro: 0.554, train_f1-macro: 0.572, valid_f1-macro: 0.492, train_cross-entropy: 1.566, valid_cross-entropy: 1.584, \n",
      "[Epoch: 59] OHPL Loss: 0.40570434049388143\n",
      "train_mae: 0.463, valid_mae: 0.446, train_mze: 0.439, valid_mze: 0.439, train_f1-micro: 0.561, valid_f1-micro: 0.561, train_f1-macro: 0.551, valid_f1-macro: 0.493, train_cross-entropy: 1.564, valid_cross-entropy: 1.581, \n",
      "[Epoch: 60] OHPL Loss: 0.27199015177652414\n",
      "train_mae: 0.457, valid_mae: 0.427, train_mze: 0.433, valid_mze: 0.420, train_f1-micro: 0.567, valid_f1-micro: 0.580, train_f1-macro: 0.558, valid_f1-macro: 0.516, train_cross-entropy: 1.562, valid_cross-entropy: 1.576, \n",
      "[Epoch: 61] OHPL Loss: 1.2503531273646338\n",
      "train_mae: 0.474, valid_mae: 0.478, train_mze: 0.449, valid_mze: 0.465, train_f1-micro: 0.551, valid_f1-micro: 0.535, train_f1-macro: 0.548, valid_f1-macro: 0.467, train_cross-entropy: 1.562, valid_cross-entropy: 1.576, \n",
      "[Epoch: 62] OHPL Loss: 1.3380827948401581\n",
      "train_mae: 0.478, valid_mae: 0.478, train_mze: 0.452, valid_mze: 0.465, train_f1-micro: 0.548, valid_f1-micro: 0.535, train_f1-macro: 0.545, valid_f1-macro: 0.475, train_cross-entropy: 1.570, valid_cross-entropy: 1.588, \n",
      "[Epoch: 63] OHPL Loss: 0.4412531064301172\n",
      "train_mae: 0.447, valid_mae: 0.471, train_mze: 0.425, valid_mze: 0.459, train_f1-micro: 0.575, valid_f1-micro: 0.541, train_f1-macro: 0.579, valid_f1-macro: 0.494, train_cross-entropy: 1.564, valid_cross-entropy: 1.579, \n",
      "[Epoch: 64] OHPL Loss: 0.44716526358165193\n",
      "train_mae: 0.447, valid_mae: 0.471, train_mze: 0.423, valid_mze: 0.465, train_f1-micro: 0.577, valid_f1-micro: 0.535, train_f1-macro: 0.592, valid_f1-macro: 0.493, train_cross-entropy: 1.569, valid_cross-entropy: 1.587, \n",
      "[Epoch: 65] OHPL Loss: 0.46318578321262466\n",
      "train_mae: 0.465, valid_mae: 0.478, train_mze: 0.436, valid_mze: 0.465, train_f1-micro: 0.564, valid_f1-micro: 0.535, train_f1-macro: 0.576, valid_f1-macro: 0.496, train_cross-entropy: 1.571, valid_cross-entropy: 1.585, \n",
      "[Epoch: 66] OHPL Loss: 0.506078269472221\n",
      "train_mae: 0.470, valid_mae: 0.446, train_mze: 0.441, valid_mze: 0.433, train_f1-micro: 0.559, valid_f1-micro: 0.567, train_f1-macro: 0.581, valid_f1-macro: 0.520, train_cross-entropy: 1.571, valid_cross-entropy: 1.581, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 67] OHPL Loss: 0.9295292689216992\n",
      "train_mae: 0.465, valid_mae: 0.420, train_mze: 0.431, valid_mze: 0.414, train_f1-micro: 0.569, valid_f1-micro: 0.586, train_f1-macro: 0.600, valid_f1-macro: 0.521, train_cross-entropy: 1.567, valid_cross-entropy: 1.562, \n",
      "[Epoch: 68] OHPL Loss: 0.6215191874850737\n",
      "train_mae: 0.486, valid_mae: 0.433, train_mze: 0.452, valid_mze: 0.414, train_f1-micro: 0.548, valid_f1-micro: 0.586, train_f1-macro: 0.580, valid_f1-macro: 0.535, train_cross-entropy: 1.573, valid_cross-entropy: 1.577, \n",
      "[Epoch: 69] OHPL Loss: 0.4070356306711942\n",
      "train_mae: 0.463, valid_mae: 0.439, train_mze: 0.434, valid_mze: 0.427, train_f1-micro: 0.566, valid_f1-micro: 0.573, train_f1-macro: 0.597, valid_f1-macro: 0.548, train_cross-entropy: 1.564, valid_cross-entropy: 1.577, \n",
      "[Epoch: 70] OHPL Loss: 0.4600729735837023\n",
      "train_mae: 0.462, valid_mae: 0.452, train_mze: 0.433, valid_mze: 0.439, train_f1-micro: 0.567, valid_f1-micro: 0.561, train_f1-macro: 0.592, valid_f1-macro: 0.522, train_cross-entropy: 1.566, valid_cross-entropy: 1.581, \n",
      "[Epoch: 71] OHPL Loss: 1.0550850433865238\n",
      "train_mae: 0.463, valid_mae: 0.459, train_mze: 0.431, valid_mze: 0.446, train_f1-micro: 0.569, valid_f1-micro: 0.554, train_f1-macro: 0.595, valid_f1-macro: 0.501, train_cross-entropy: 1.563, valid_cross-entropy: 1.575, \n",
      "[Epoch: 72] OHPL Loss: 1.833360772303253\n",
      "train_mae: 0.478, valid_mae: 0.459, train_mze: 0.442, valid_mze: 0.446, train_f1-micro: 0.558, valid_f1-micro: 0.554, train_f1-macro: 0.584, valid_f1-macro: 0.495, train_cross-entropy: 1.562, valid_cross-entropy: 1.574, \n",
      "[Epoch: 73] OHPL Loss: 0.5076558588222194\n",
      "train_mae: 0.462, valid_mae: 0.478, train_mze: 0.429, valid_mze: 0.471, train_f1-micro: 0.571, valid_f1-micro: 0.529, train_f1-macro: 0.594, valid_f1-macro: 0.478, train_cross-entropy: 1.564, valid_cross-entropy: 1.578, \n",
      "[Epoch: 74] OHPL Loss: 0.35648336996412117\n",
      "train_mae: 0.452, valid_mae: 0.439, train_mze: 0.428, valid_mze: 0.433, train_f1-micro: 0.572, valid_f1-micro: 0.567, train_f1-macro: 0.596, valid_f1-macro: 0.498, train_cross-entropy: 1.560, valid_cross-entropy: 1.571, \n",
      "[Epoch: 75] OHPL Loss: 0.39032030008241725\n",
      "train_mae: 0.473, valid_mae: 0.465, train_mze: 0.447, valid_mze: 0.452, train_f1-micro: 0.553, valid_f1-micro: 0.548, train_f1-macro: 0.574, valid_f1-macro: 0.492, train_cross-entropy: 1.572, valid_cross-entropy: 1.589, \n",
      "[Epoch: 76] OHPL Loss: 0.4512801210067084\n",
      "train_mae: 0.470, valid_mae: 0.452, train_mze: 0.438, valid_mze: 0.439, train_f1-micro: 0.562, valid_f1-micro: 0.561, train_f1-macro: 0.581, valid_f1-macro: 0.508, train_cross-entropy: 1.573, valid_cross-entropy: 1.584, \n",
      "[Epoch: 77] OHPL Loss: 4.641351018812218\n",
      "train_mae: 0.471, valid_mae: 0.452, train_mze: 0.444, valid_mze: 0.439, train_f1-micro: 0.556, valid_f1-micro: 0.561, train_f1-macro: 0.567, valid_f1-macro: 0.510, train_cross-entropy: 1.570, valid_cross-entropy: 1.586, \n",
      "[Epoch: 78] OHPL Loss: 2.3316716337006955\n",
      "train_mae: 0.465, valid_mae: 0.439, train_mze: 0.436, valid_mze: 0.427, train_f1-micro: 0.564, valid_f1-micro: 0.573, train_f1-macro: 0.590, valid_f1-macro: 0.515, train_cross-entropy: 1.568, valid_cross-entropy: 1.575, \n",
      "[Epoch: 79] OHPL Loss: 0.31904546877706025\n",
      "train_mae: 0.454, valid_mae: 0.427, train_mze: 0.428, valid_mze: 0.414, train_f1-micro: 0.572, valid_f1-micro: 0.586, train_f1-macro: 0.574, valid_f1-macro: 0.513, train_cross-entropy: 1.560, valid_cross-entropy: 1.566, \n",
      "[Epoch: 80] OHPL Loss: 1.7647577454649468\n",
      "train_mae: 0.447, valid_mae: 0.401, train_mze: 0.421, valid_mze: 0.395, train_f1-micro: 0.579, valid_f1-micro: 0.605, train_f1-macro: 0.578, valid_f1-macro: 0.522, train_cross-entropy: 1.564, valid_cross-entropy: 1.570, \n",
      "[Epoch: 81] OHPL Loss: 2.1947950011534068\n",
      "train_mae: 0.460, valid_mae: 0.459, train_mze: 0.431, valid_mze: 0.446, train_f1-micro: 0.569, valid_f1-micro: 0.554, train_f1-macro: 0.575, valid_f1-macro: 0.493, train_cross-entropy: 1.566, valid_cross-entropy: 1.589, \n",
      "[Epoch: 82] OHPL Loss: 0.6650277079224706\n",
      "train_mae: 0.463, valid_mae: 0.484, train_mze: 0.429, valid_mze: 0.471, train_f1-micro: 0.571, valid_f1-micro: 0.529, train_f1-macro: 0.573, valid_f1-macro: 0.475, train_cross-entropy: 1.570, valid_cross-entropy: 1.591, \n",
      "[Epoch: 83] OHPL Loss: 1.2824020066047586\n",
      "train_mae: 0.476, valid_mae: 0.478, train_mze: 0.446, valid_mze: 0.465, train_f1-micro: 0.554, valid_f1-micro: 0.535, train_f1-macro: 0.564, valid_f1-macro: 0.471, train_cross-entropy: 1.569, valid_cross-entropy: 1.585, \n",
      "[Epoch: 84] OHPL Loss: 0.20065153875307443\n",
      "train_mae: 0.476, valid_mae: 0.459, train_mze: 0.449, valid_mze: 0.446, train_f1-micro: 0.551, valid_f1-micro: 0.554, train_f1-macro: 0.551, valid_f1-macro: 0.481, train_cross-entropy: 1.568, valid_cross-entropy: 1.572, \n",
      "[Epoch: 85] OHPL Loss: 0.3236575387255392\n",
      "train_mae: 0.465, valid_mae: 0.471, train_mze: 0.438, valid_mze: 0.459, train_f1-micro: 0.562, valid_f1-micro: 0.541, train_f1-macro: 0.564, valid_f1-macro: 0.466, train_cross-entropy: 1.561, valid_cross-entropy: 1.576, \n",
      "[Epoch: 86] OHPL Loss: 0.8407598964002949\n",
      "train_mae: 0.441, valid_mae: 0.484, train_mze: 0.418, valid_mze: 0.471, train_f1-micro: 0.582, valid_f1-micro: 0.529, train_f1-macro: 0.585, valid_f1-macro: 0.462, train_cross-entropy: 1.566, valid_cross-entropy: 1.586, \n",
      "[Epoch: 87] OHPL Loss: 0.1786107802813274\n",
      "train_mae: 0.449, valid_mae: 0.471, train_mze: 0.425, valid_mze: 0.452, train_f1-micro: 0.575, valid_f1-micro: 0.548, train_f1-macro: 0.580, valid_f1-macro: 0.489, train_cross-entropy: 1.560, valid_cross-entropy: 1.581, \n",
      "[Epoch: 88] OHPL Loss: 1.2688102607527885\n",
      "train_mae: 0.452, valid_mae: 0.471, train_mze: 0.426, valid_mze: 0.452, train_f1-micro: 0.574, valid_f1-micro: 0.548, train_f1-macro: 0.582, valid_f1-macro: 0.495, train_cross-entropy: 1.561, valid_cross-entropy: 1.585, \n",
      "[Epoch: 89] OHPL Loss: 0.612344991242889\n",
      "train_mae: 0.446, valid_mae: 0.471, train_mze: 0.415, valid_mze: 0.459, train_f1-micro: 0.585, valid_f1-micro: 0.541, train_f1-macro: 0.601, valid_f1-macro: 0.496, train_cross-entropy: 1.565, valid_cross-entropy: 1.585, \n",
      "[Epoch: 90] OHPL Loss: 0.5153169388297605\n",
      "train_mae: 0.460, valid_mae: 0.465, train_mze: 0.425, valid_mze: 0.452, train_f1-micro: 0.575, valid_f1-micro: 0.548, train_f1-macro: 0.581, valid_f1-macro: 0.510, train_cross-entropy: 1.565, valid_cross-entropy: 1.581, \n",
      "[Epoch: 91] OHPL Loss: 0.5298059183947272\n",
      "train_mae: 0.458, valid_mae: 0.478, train_mze: 0.428, valid_mze: 0.465, train_f1-micro: 0.572, valid_f1-micro: 0.535, train_f1-macro: 0.585, valid_f1-macro: 0.505, train_cross-entropy: 1.563, valid_cross-entropy: 1.587, \n",
      "[Epoch: 92] OHPL Loss: 0.6461072888166256\n",
      "train_mae: 0.460, valid_mae: 0.465, train_mze: 0.436, valid_mze: 0.459, train_f1-micro: 0.564, valid_f1-micro: 0.541, train_f1-macro: 0.568, valid_f1-macro: 0.488, train_cross-entropy: 1.565, valid_cross-entropy: 1.591, \n",
      "[Epoch: 93] OHPL Loss: 0.45621052539912754\n",
      "train_mae: 0.471, valid_mae: 0.439, train_mze: 0.447, valid_mze: 0.427, train_f1-micro: 0.553, valid_f1-micro: 0.573, train_f1-macro: 0.568, valid_f1-macro: 0.518, train_cross-entropy: 1.562, valid_cross-entropy: 1.583, \n",
      "[Epoch: 94] OHPL Loss: 1.0427870711471317\n",
      "train_mae: 0.458, valid_mae: 0.433, train_mze: 0.436, valid_mze: 0.414, train_f1-micro: 0.564, valid_f1-micro: 0.586, train_f1-macro: 0.565, valid_f1-macro: 0.522, train_cross-entropy: 1.561, valid_cross-entropy: 1.576, \n",
      "[Epoch: 95] OHPL Loss: 0.34143692480360827\n",
      "train_mae: 0.436, valid_mae: 0.465, train_mze: 0.415, valid_mze: 0.459, train_f1-micro: 0.585, valid_f1-micro: 0.541, train_f1-macro: 0.593, valid_f1-macro: 0.487, train_cross-entropy: 1.564, valid_cross-entropy: 1.584, \n",
      "[Epoch: 96] OHPL Loss: 0.5138618539585417\n",
      "train_mae: 0.446, valid_mae: 0.465, train_mze: 0.417, valid_mze: 0.459, train_f1-micro: 0.583, valid_f1-micro: 0.541, train_f1-macro: 0.593, valid_f1-macro: 0.494, train_cross-entropy: 1.564, valid_cross-entropy: 1.588, \n",
      "[Epoch: 97] OHPL Loss: 0.3651453028950737\n",
      "train_mae: 0.486, valid_mae: 0.471, train_mze: 0.457, valid_mze: 0.459, train_f1-micro: 0.543, valid_f1-micro: 0.541, train_f1-macro: 0.558, valid_f1-macro: 0.494, train_cross-entropy: 1.567, valid_cross-entropy: 1.596, \n",
      "[Epoch: 98] OHPL Loss: 0.2681181462373401\n",
      "train_mae: 0.486, valid_mae: 0.478, train_mze: 0.462, valid_mze: 0.465, train_f1-micro: 0.538, valid_f1-micro: 0.535, train_f1-macro: 0.549, valid_f1-macro: 0.485, train_cross-entropy: 1.569, valid_cross-entropy: 1.594, \n",
      "[Epoch: 99] OHPL Loss: 0.5174750837341955\n",
      "train_mae: 0.447, valid_mae: 0.478, train_mze: 0.426, valid_mze: 0.471, train_f1-micro: 0.574, valid_f1-micro: 0.529, train_f1-macro: 0.588, valid_f1-macro: 0.486, train_cross-entropy: 1.562, valid_cross-entropy: 1.585, \n",
      "[Epoch: 100] OHPL Loss: 0.6580786288786662\n",
      "train_mae: 0.446, valid_mae: 0.452, train_mze: 0.423, valid_mze: 0.439, train_f1-micro: 0.577, valid_f1-micro: 0.561, train_f1-macro: 0.585, valid_f1-macro: 0.513, train_cross-entropy: 1.559, valid_cross-entropy: 1.579, \n",
      "Time elapsed: 4.010805368423462s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1,  0,  0,  0,  0,  0],\n",
       "       [ 4, 11,  4,  0,  0,  0],\n",
       "       [ 0, 18, 22,  7,  0,  0],\n",
       "       [ 0,  0,  8, 33,  9,  1],\n",
       "       [ 0,  0,  1,  6, 12,  8],\n",
       "       [ 0,  0,  0,  0,  3,  9]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "N_FEATURE = x.shape[1]\n",
    "architecture = [25, 30, 6]\n",
    "dropout_p = 0.1\n",
    "learning_rate = 1e-2\n",
    "\n",
    "model = MultiLayerFC(N_FEATURE,architecture,dropout_p=dropout_p).to(device)\n",
    "model.apply(initialize_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "criterion = ohpl\n",
    "metric_dict = {\n",
    "    'mae': lambda y_est,y: torch.mean(torch.abs(y_est-y)), \n",
    "    'mze': lambda y_est,y: torch.mean((torch.abs(y_est-y) > 0).type(torch.double)),\n",
    "    'f1-micro':  lambda y_est,y: f1_score(y,y_est,average='micro'),\n",
    "    'f1-macro':  lambda y_est,y: f1_score(y,y_est,average='macro'),\n",
    "              }\n",
    "centroid, history = train_ohpl(model=model, \n",
    "                               train_dataset=dataset_model, \n",
    "                               valid_dataset=dataset_test, \n",
    "                               min_label=min_label, \n",
    "                               max_label=max_label, \n",
    "                               criterion=criterion,\n",
    "                               optimizer=optimizer,\n",
    "                               metric_dict=metric_dict,\n",
    "                               num_epochs=num_epochs, \n",
    "                               batch_size=64,\n",
    "                               ohpl_norm_order=1)\n",
    "\n",
    "t1 = time.time()\n",
    "print(f\"Time elapsed: {t1-t0}s\")\n",
    "\n",
    "confusion_matrix(history['valid']['y'], history['valid']['y_est'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ray Tune Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_metrics = [\"training_iteration\",\n",
    "                  \"ohpl\",\n",
    "                  \"train_mae\", \n",
    "                  \"valid_mae\",\n",
    "                  \"train_mze\", \n",
    "                  \"valid_mze\",\n",
    "                  \"train_f1-micro\",\n",
    "                  \"valid_f1-micro\",\n",
    "                  \"train_f1-macro\",\n",
    "                  \"valid_f1-marco\",\n",
    "                  \"train_cross-entropy\", \n",
    "                  \"valid_cross-entropy\",\n",
    "                 ]\n",
    "\n",
    "reporter = tune.JupyterNotebookReporter(overwrite=True, max_progress_rows=35, metric_columns= report_metrics)\n",
    "searchopt = BasicVariantGenerator(max_concurrent=15)\n",
    "\n",
    "config = {\"lr\": tune.loguniform(lr_min, lr_max),                       # Learning Rate\n",
    "          \"dropout_p\": tune.uniform(dropout_p_min, dropout_p_max),     # Dropout On/Off\n",
    "          \"k\": tune.grid_search([*range(k)]),                          # K-Fold Index\n",
    "          \"batch_size\": tune.choice(batch_size),                       # 1: SGD; 2+: Zero-Filled BGD\n",
    "          \"margin\": tune.uniform(margin_min, margin_max),              # OHPL margin\n",
    "          \"ordering_loss_weight\": tune.uniform(alpha_min, alpha_max),  # OHPL ordering loss weight\n",
    "          \"h_total\": tune.choice([*range(h_total_min, h_total_max, h_total_step)]),\n",
    "          \"h_fc\": tune.sample_from(lambda spec: split_sampling(num_ele = spec.config.h_total, \n",
    "                                                               n_min = h_fc_min_neuron_per_layer,\n",
    "                                                               n_max = h_fc_max_neuron_per_layer,\n",
    "                                                               out_dim = h_fc_max_layer,\n",
    "                                                               single_sample = True)),\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.6/59.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/32.37 GiB heap, 0.0/16.18 GiB objects (0.0/1.0 CPU_group_44c0a7d52eb21b3d3eabe6ead34b9d1e, 0.0/1.0 CPU_group_0_44c0a7d52eb21b3d3eabe6ead34b9d1e, 0.0/1.0 CPU_group_93e1d2cdc58d280540228776df1f02d3, 0.0/1.0 CPU_group_0_e1f3248a80527d05ab27c188bd79e5c3, 0.0/1.0 CPU_group_bcfbc6887f16b56cf99fd02b2f738e41, 0.0/1.0 CPU_group_0_bcfbc6887f16b56cf99fd02b2f738e41, 0.0/1.0 CPU_group_0_93e1d2cdc58d280540228776df1f02d3, 0.0/1.0 CPU_group_c0c90b86aad9881b0c1d55c55160a613, 0.0/1.0 accelerator_type:V100, 0.0/1.0 CPU_group_0_c0c90b86aad9881b0c1d55c55160a613, 0.0/1.0 CPU_group_e1f3248a80527d05ab27c188bd79e5c3)<br>Result logdir: /home/calvin_chan/data/output/testing/train_ohpl_raytune_2021-08-26_23-52-51<br>Number of trials: 5/5 (5 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  dropout_p</th><th>h_fc     </th><th style=\"text-align: right;\">  h_total</th><th style=\"text-align: right;\">  k</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  margin</th><th style=\"text-align: right;\">  ordering_loss_weight</th><th style=\"text-align: right;\">  training_iteration</th><th style=\"text-align: right;\">   ohpl</th><th style=\"text-align: right;\">  train_mae</th><th style=\"text-align: right;\">  valid_mae</th><th style=\"text-align: right;\">  train_mze</th><th style=\"text-align: right;\">  valid_mze</th><th style=\"text-align: right;\">  train_f1-micro</th><th style=\"text-align: right;\">  valid_f1-micro</th><th style=\"text-align: right;\">  train_f1-macro</th><th style=\"text-align: right;\">  train_cross-entropy</th><th style=\"text-align: right;\">  valid_cross-entropy</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_ohpl_raytune_b89e7_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">           5</td><td style=\"text-align: right;\">  0.0693328</td><td>[3, 2, 3]</td><td style=\"text-align: right;\">        8</td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">0.000128489</td><td style=\"text-align: right;\">0.850497</td><td style=\"text-align: right;\">              0.509621</td><td style=\"text-align: right;\">                 100</td><td style=\"text-align: right;\">1.55205</td><td style=\"text-align: right;\">   0.741483</td><td style=\"text-align: right;\">   0.816   </td><td style=\"text-align: right;\">   0.609218</td><td style=\"text-align: right;\">   0.68    </td><td style=\"text-align: right;\">        0.390782</td><td style=\"text-align: right;\">        0.32    </td><td style=\"text-align: right;\">       0.379818 </td><td style=\"text-align: right;\">              1.66549</td><td style=\"text-align: right;\">              1.69199</td></tr>\n",
       "<tr><td>train_ohpl_raytune_b89e7_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">           5</td><td style=\"text-align: right;\">  0.221446 </td><td>[4, 2, 2]</td><td style=\"text-align: right;\">        8</td><td style=\"text-align: right;\">  1</td><td style=\"text-align: right;\">0.0415138  </td><td style=\"text-align: right;\">0.872935</td><td style=\"text-align: right;\">              0.92912 </td><td style=\"text-align: right;\">                 100</td><td style=\"text-align: right;\">5.57472</td><td style=\"text-align: right;\">   2.89579 </td><td style=\"text-align: right;\">   2.952   </td><td style=\"text-align: right;\">   0.987976</td><td style=\"text-align: right;\">   1       </td><td style=\"text-align: right;\">        0.012024</td><td style=\"text-align: right;\">        0       </td><td style=\"text-align: right;\">       0.0039604</td><td style=\"text-align: right;\">              1.79176</td><td style=\"text-align: right;\">              1.79176</td></tr>\n",
       "<tr><td>train_ohpl_raytune_b89e7_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">           5</td><td style=\"text-align: right;\">  0.586613 </td><td>[3, 3]   </td><td style=\"text-align: right;\">        6</td><td style=\"text-align: right;\">  2</td><td style=\"text-align: right;\">0.013008   </td><td style=\"text-align: right;\">0.939486</td><td style=\"text-align: right;\">              0.978904</td><td style=\"text-align: right;\">                 100</td><td style=\"text-align: right;\">2.93671</td><td style=\"text-align: right;\">   0.963928</td><td style=\"text-align: right;\">   0.864   </td><td style=\"text-align: right;\">   0.669339</td><td style=\"text-align: right;\">   0.632   </td><td style=\"text-align: right;\">        0.330661</td><td style=\"text-align: right;\">        0.368   </td><td style=\"text-align: right;\">       0.284331 </td><td style=\"text-align: right;\">              1.74916</td><td style=\"text-align: right;\">              1.72539</td></tr>\n",
       "<tr><td>train_ohpl_raytune_b89e7_00003</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">           5</td><td style=\"text-align: right;\">  0.453275 </td><td>[2, 2, 2]</td><td style=\"text-align: right;\">        6</td><td style=\"text-align: right;\">  3</td><td style=\"text-align: right;\">0.00250131 </td><td style=\"text-align: right;\">0.833   </td><td style=\"text-align: right;\">              0.926098</td><td style=\"text-align: right;\">                 100</td><td style=\"text-align: right;\">2.53323</td><td style=\"text-align: right;\">   1.07014 </td><td style=\"text-align: right;\">   1.088   </td><td style=\"text-align: right;\">   0.725451</td><td style=\"text-align: right;\">   0.744   </td><td style=\"text-align: right;\">        0.274549</td><td style=\"text-align: right;\">        0.256   </td><td style=\"text-align: right;\">       0.265007 </td><td style=\"text-align: right;\">              1.78118</td><td style=\"text-align: right;\">              1.75732</td></tr>\n",
       "<tr><td>train_ohpl_raytune_b89e7_00004</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">           5</td><td style=\"text-align: right;\">  0.145067 </td><td>[2, 2]   </td><td style=\"text-align: right;\">        4</td><td style=\"text-align: right;\">  4</td><td style=\"text-align: right;\">0.000199224</td><td style=\"text-align: right;\">0.930827</td><td style=\"text-align: right;\">              0.844911</td><td style=\"text-align: right;\">                 100</td><td style=\"text-align: right;\">2.35893</td><td style=\"text-align: right;\">   0.654   </td><td style=\"text-align: right;\">   0.629032</td><td style=\"text-align: right;\">   0.532   </td><td style=\"text-align: right;\">   0.516129</td><td style=\"text-align: right;\">        0.468   </td><td style=\"text-align: right;\">        0.483871</td><td style=\"text-align: right;\">       0.404811 </td><td style=\"text-align: right;\">              1.64608</td><td style=\"text-align: right;\">              1.59583</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-26 23:53:25,199\tINFO tune.py:550 -- Total run time: 33.76 seconds (33.56 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 34.264498710632324s\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "result = tune.run(\n",
    "        tune.with_parameters(train_ohpl_raytune, \n",
    "                             num_in_feat   = N_FEATURE,\n",
    "                             num_epochs    = num_epochs, \n",
    "                             train_dataset = dataset_train_kfold, \n",
    "                             valid_dataset = dataset_valid_kfold,\n",
    "                             ),\n",
    "        config = config,\n",
    "        resources_per_trial={\"cpu\": 1, \"gpu\": 0.1},\n",
    "        num_samples = num_hp_search_samples,\n",
    "        local_dir = chkpt_dir,\n",
    "        progress_reporter = reporter,\n",
    "#         scheduler = scheduler,\n",
    "        search_alg = searchopt,\n",
    ")\n",
    "\n",
    "t1 = time.time()\n",
    "print(f\"Time elapsed: {t1-t0}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['train_mae', 'valid_mae', 'train_mze', 'valid_mze', 'train_f1-micro',\n",
       "       'valid_f1-micro', 'train_f1-macro', 'valid_f1-macro', 'ohpl',\n",
       "       'train_cross-entropy', 'valid_cross-entropy', 'time_this_iter_s',\n",
       "       'should_checkpoint', 'done', 'timesteps_total', 'episodes_total',\n",
       "       'training_iteration', 'experiment_id', 'date', 'timestamp',\n",
       "       'time_total_s', 'pid', 'hostname', 'node_ip', 'time_since_restore',\n",
       "       'timesteps_since_restore', 'iterations_since_restore', 'experiment_tag',\n",
       "       'config.lr', 'config.dropout_p', 'config.k', 'config.batch_size',\n",
       "       'config.margin', 'config.ordering_loss_weight', 'config.h_total',\n",
       "       'config.h_fc'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.results_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trial_id\n",
       "b89e7_00000    0.816000\n",
       "b89e7_00001    2.952000\n",
       "b89e7_00002    0.864000\n",
       "b89e7_00003    1.088000\n",
       "b89e7_00004    0.629032\n",
       "Name: valid_mae, dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.results_df.valid_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true = torch.tensor([4,1,2,0,4,2,1])\n",
    "# y_pred = torch.tensor([6.0,3.1,5.2,1.0,4.0,2.2,3.7], dtype=torch.float32)\n",
    "# minlabel = 0\n",
    "# maxlabel = 4\n",
    "# margin = 0.3\n",
    "# ordering_loss_weight = 0.1   # Alpha\n",
    "# loss_bound = 1e9\n",
    "\n",
    "# # === HCL: Hyperplane Centroid Loss ===\n",
    "# # (To ensure hyperplane are ordered by rank)\n",
    "\n",
    "# min_label = torch.tensor(minlabel, dtype=torch.float32, requires_grad=False)\n",
    "# max_label = torch.tensor(maxlabel, dtype=torch.float32, requires_grad=False)\n",
    "# margin = torch.tensor(margin, dtype=torch.float32, requires_grad=False)\n",
    "# ordering_loss_weight = torch.tensor(ordering_loss_weight, dtype=torch.float32, requires_grad=False)\n",
    "\n",
    "# y_true = y_true.type(y_pred.dtype)\n",
    "# ords, idx = torch.unique(y_true, return_inverse=True)\n",
    "# num_label = ords.shape[0]\n",
    "# y_true_ohe = F.one_hot(idx,num_classes=num_label)\n",
    "\n",
    "# # hyperplane intercept term\n",
    "# yO = y_pred.type(torch.float32) @ y_true_ohe.type(torch.float32)\n",
    "# yc = torch.sum(y_true_ohe, dim=0)\n",
    "# class_mean = torch.div(yO,yc)\n",
    "\n",
    "# # relative rank distance between centroids\n",
    "# min_distance = torch.reshape(ords,(-1,1)) - torch.reshape(ords,(1,-1))\n",
    "# min_distance = torch.relu(min_distance)\n",
    "\n",
    "# # keeps min. distance (???)\n",
    "# keep = torch.minimum(min_distance,torch.ones(min_distance.shape))\n",
    "\n",
    "# # positive mean sample distance between centroids\n",
    "# centroid_distance = torch.reshape(class_mean,(-1,1)) - torch.reshape(class_mean,(1,-1))\n",
    "# centroid_distance = torch.relu(centroid_distance)   # zero loss for correct ordering\n",
    "# centroid_distance = torch.multiply(keep, centroid_distance)\n",
    "\n",
    "# hp_ordering_loss = torch.sum(torch.relu(min_distance - centroid_distance))\n",
    "\n",
    "# # === HPL/HPPL: Hyperplane Point Loss ===\n",
    "# # (To ensure transformation place the point near the correct centroid)\n",
    "# mean_centroid_of_sample = y_true_ohe.type(torch.float32) @ torch.reshape(class_mean,(-1,1))\n",
    "\n",
    "\n",
    "# # --- Limit Edge Case Loss ---\n",
    "# # No reason to limit distance from edge cases:\n",
    "# # 1. Positive edge case (max_label) for upper loss\n",
    "# # 2. Negative edge case (min_label) for lower loss\n",
    "# upper_bound = (y_true - max_label + 1) * loss_bound   # Select edge case and give a large loss_bound (we want to pull it back in case if it gets too big)\n",
    "# upper_bound = torch.relu(upper_bound) + margin        # Add margin to non-edge cases\n",
    "# lower_bound = (-(y_true - min_label) + 1) * loss_bound\n",
    "# lower_bound = torch.relu(lower_bound) + margin   \n",
    "\n",
    "# # -- Compute Loss ---\n",
    "# upper_loss = y_pred[:,None] - mean_centroid_of_sample\n",
    "# upper_loss_bounded = torch.relu(upper_loss - upper_bound[:,None])\n",
    "# lower_loss = -(y_pred[:,None] - mean_centroid_of_sample)\n",
    "# lower_loss_bounded = torch.relu(lower_loss - lower_bound[:,None])\n",
    "\n",
    "# hp_point_loss = torch.mean(upper_loss_bounded + lower_loss_bounded)\n",
    "\n",
    "# # === OHPL ===\n",
    "# loss = hp_point_loss + ordering_loss_weight * hp_ordering_loss\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_p39]",
   "language": "python",
   "name": "conda-env-pytorch_p39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
